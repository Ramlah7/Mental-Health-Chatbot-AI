{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 28755,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020865936358894104,
      "grad_norm": 59.85308074951172,
      "learning_rate": 9.5e-06,
      "loss": 7.4808,
      "step": 20
    },
    {
      "epoch": 0.004173187271778821,
      "grad_norm": 52.49814987182617,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 7.369,
      "step": 40
    },
    {
      "epoch": 0.006259780907668232,
      "grad_norm": 44.03772735595703,
      "learning_rate": 2.95e-05,
      "loss": 6.2366,
      "step": 60
    },
    {
      "epoch": 0.008346374543557642,
      "grad_norm": 28.28744125366211,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 5.075,
      "step": 80
    },
    {
      "epoch": 0.010432968179447054,
      "grad_norm": 21.7711181640625,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 4.5649,
      "step": 100
    },
    {
      "epoch": 0.012519561815336464,
      "grad_norm": 14.814327239990234,
      "learning_rate": 4.996684697260513e-05,
      "loss": 4.0856,
      "step": 120
    },
    {
      "epoch": 0.014606155451225874,
      "grad_norm": 11.441915512084961,
      "learning_rate": 4.993194904903158e-05,
      "loss": 3.9,
      "step": 140
    },
    {
      "epoch": 0.016692749087115284,
      "grad_norm": 12.330286026000977,
      "learning_rate": 4.989705112545804e-05,
      "loss": 3.7195,
      "step": 160
    },
    {
      "epoch": 0.018779342723004695,
      "grad_norm": 11.40850830078125,
      "learning_rate": 4.986215320188449e-05,
      "loss": 3.702,
      "step": 180
    },
    {
      "epoch": 0.020865936358894107,
      "grad_norm": 11.272089958190918,
      "learning_rate": 4.9827255278310944e-05,
      "loss": 3.6899,
      "step": 200
    },
    {
      "epoch": 0.022952529994783515,
      "grad_norm": 10.282651901245117,
      "learning_rate": 4.9792357354737395e-05,
      "loss": 3.6351,
      "step": 220
    },
    {
      "epoch": 0.025039123630672927,
      "grad_norm": 8.204571723937988,
      "learning_rate": 4.975745943116385e-05,
      "loss": 3.4626,
      "step": 240
    },
    {
      "epoch": 0.027125717266562335,
      "grad_norm": 10.152949333190918,
      "learning_rate": 4.97225615075903e-05,
      "loss": 3.4501,
      "step": 260
    },
    {
      "epoch": 0.029212310902451747,
      "grad_norm": 8.80673885345459,
      "learning_rate": 4.968766358401675e-05,
      "loss": 3.4961,
      "step": 280
    },
    {
      "epoch": 0.03129890453834116,
      "grad_norm": 8.1156644821167,
      "learning_rate": 4.965276566044321e-05,
      "loss": 3.5148,
      "step": 300
    },
    {
      "epoch": 0.03338549817423057,
      "grad_norm": 9.16470718383789,
      "learning_rate": 4.961786773686966e-05,
      "loss": 3.4216,
      "step": 320
    },
    {
      "epoch": 0.03547209181011998,
      "grad_norm": 8.020098686218262,
      "learning_rate": 4.958296981329611e-05,
      "loss": 3.4407,
      "step": 340
    },
    {
      "epoch": 0.03755868544600939,
      "grad_norm": 8.156919479370117,
      "learning_rate": 4.954807188972256e-05,
      "loss": 3.4533,
      "step": 360
    },
    {
      "epoch": 0.0396452790818988,
      "grad_norm": 8.454769134521484,
      "learning_rate": 4.951317396614902e-05,
      "loss": 3.4088,
      "step": 380
    },
    {
      "epoch": 0.041731872717788214,
      "grad_norm": 7.752870082855225,
      "learning_rate": 4.9478276042575465e-05,
      "loss": 3.3929,
      "step": 400
    },
    {
      "epoch": 0.04381846635367762,
      "grad_norm": 7.210986137390137,
      "learning_rate": 4.9443378119001924e-05,
      "loss": 3.3865,
      "step": 420
    },
    {
      "epoch": 0.04590505998956703,
      "grad_norm": 6.919450759887695,
      "learning_rate": 4.9408480195428375e-05,
      "loss": 3.467,
      "step": 440
    },
    {
      "epoch": 0.047991653625456446,
      "grad_norm": 6.831155776977539,
      "learning_rate": 4.9373582271854826e-05,
      "loss": 3.3191,
      "step": 460
    },
    {
      "epoch": 0.050078247261345854,
      "grad_norm": 7.6104960441589355,
      "learning_rate": 4.933868434828128e-05,
      "loss": 3.433,
      "step": 480
    },
    {
      "epoch": 0.05216484089723526,
      "grad_norm": 7.507455825805664,
      "learning_rate": 4.930378642470773e-05,
      "loss": 3.3054,
      "step": 500
    },
    {
      "epoch": 0.05425143453312467,
      "grad_norm": 8.363861083984375,
      "learning_rate": 4.926888850113419e-05,
      "loss": 3.2376,
      "step": 520
    },
    {
      "epoch": 0.056338028169014086,
      "grad_norm": 7.225987911224365,
      "learning_rate": 4.923399057756064e-05,
      "loss": 3.2399,
      "step": 540
    },
    {
      "epoch": 0.058424621804903494,
      "grad_norm": 8.62004566192627,
      "learning_rate": 4.919909265398709e-05,
      "loss": 3.3121,
      "step": 560
    },
    {
      "epoch": 0.0605112154407929,
      "grad_norm": 5.870245933532715,
      "learning_rate": 4.916419473041354e-05,
      "loss": 3.2002,
      "step": 580
    },
    {
      "epoch": 0.06259780907668232,
      "grad_norm": 6.014284133911133,
      "learning_rate": 4.912929680684e-05,
      "loss": 3.3616,
      "step": 600
    },
    {
      "epoch": 0.06468440271257173,
      "grad_norm": 7.40529203414917,
      "learning_rate": 4.9094398883266445e-05,
      "loss": 3.2469,
      "step": 620
    },
    {
      "epoch": 0.06677099634846113,
      "grad_norm": 6.3890581130981445,
      "learning_rate": 4.90595009596929e-05,
      "loss": 3.2923,
      "step": 640
    },
    {
      "epoch": 0.06885758998435054,
      "grad_norm": 7.43268346786499,
      "learning_rate": 4.9024603036119355e-05,
      "loss": 3.2309,
      "step": 660
    },
    {
      "epoch": 0.07094418362023996,
      "grad_norm": 6.195108890533447,
      "learning_rate": 4.8989705112545806e-05,
      "loss": 3.3155,
      "step": 680
    },
    {
      "epoch": 0.07303077725612937,
      "grad_norm": 7.498032569885254,
      "learning_rate": 4.895480718897226e-05,
      "loss": 3.305,
      "step": 700
    },
    {
      "epoch": 0.07511737089201878,
      "grad_norm": 5.930196762084961,
      "learning_rate": 4.891990926539871e-05,
      "loss": 3.3047,
      "step": 720
    },
    {
      "epoch": 0.07720396452790819,
      "grad_norm": 6.173227310180664,
      "learning_rate": 4.888501134182517e-05,
      "loss": 3.2164,
      "step": 740
    },
    {
      "epoch": 0.0792905581637976,
      "grad_norm": 5.447803974151611,
      "learning_rate": 4.885011341825161e-05,
      "loss": 3.3184,
      "step": 760
    },
    {
      "epoch": 0.081377151799687,
      "grad_norm": 5.071650505065918,
      "learning_rate": 4.881521549467807e-05,
      "loss": 3.2999,
      "step": 780
    },
    {
      "epoch": 0.08346374543557643,
      "grad_norm": 5.710894584655762,
      "learning_rate": 4.878031757110452e-05,
      "loss": 3.2844,
      "step": 800
    },
    {
      "epoch": 0.08555033907146584,
      "grad_norm": 6.499618053436279,
      "learning_rate": 4.8745419647530973e-05,
      "loss": 3.1665,
      "step": 820
    },
    {
      "epoch": 0.08763693270735524,
      "grad_norm": 5.2452192306518555,
      "learning_rate": 4.8710521723957425e-05,
      "loss": 3.2722,
      "step": 840
    },
    {
      "epoch": 0.08972352634324465,
      "grad_norm": 6.873688220977783,
      "learning_rate": 4.8675623800383876e-05,
      "loss": 3.1764,
      "step": 860
    },
    {
      "epoch": 0.09181011997913406,
      "grad_norm": 7.274019718170166,
      "learning_rate": 4.8640725876810335e-05,
      "loss": 3.4247,
      "step": 880
    },
    {
      "epoch": 0.09389671361502347,
      "grad_norm": 7.711844444274902,
      "learning_rate": 4.860582795323678e-05,
      "loss": 3.265,
      "step": 900
    },
    {
      "epoch": 0.09598330725091289,
      "grad_norm": 6.683176040649414,
      "learning_rate": 4.857093002966324e-05,
      "loss": 3.2062,
      "step": 920
    },
    {
      "epoch": 0.0980699008868023,
      "grad_norm": 5.0998759269714355,
      "learning_rate": 4.853603210608969e-05,
      "loss": 3.2812,
      "step": 940
    },
    {
      "epoch": 0.10015649452269171,
      "grad_norm": 6.2645392417907715,
      "learning_rate": 4.850113418251615e-05,
      "loss": 3.2456,
      "step": 960
    },
    {
      "epoch": 0.10224308815858112,
      "grad_norm": 4.747495651245117,
      "learning_rate": 4.846623625894259e-05,
      "loss": 3.3157,
      "step": 980
    },
    {
      "epoch": 0.10432968179447052,
      "grad_norm": 6.651544094085693,
      "learning_rate": 4.843133833536905e-05,
      "loss": 3.3672,
      "step": 1000
    },
    {
      "epoch": 0.10641627543035993,
      "grad_norm": 5.23291015625,
      "learning_rate": 4.83964404117955e-05,
      "loss": 3.0881,
      "step": 1020
    },
    {
      "epoch": 0.10850286906624934,
      "grad_norm": 5.684906959533691,
      "learning_rate": 4.836154248822195e-05,
      "loss": 3.2043,
      "step": 1040
    },
    {
      "epoch": 0.11058946270213876,
      "grad_norm": 6.1135945320129395,
      "learning_rate": 4.8326644564648405e-05,
      "loss": 3.2489,
      "step": 1060
    },
    {
      "epoch": 0.11267605633802817,
      "grad_norm": 4.802430152893066,
      "learning_rate": 4.8291746641074856e-05,
      "loss": 3.1383,
      "step": 1080
    },
    {
      "epoch": 0.11476264997391758,
      "grad_norm": 4.8509087562561035,
      "learning_rate": 4.8256848717501315e-05,
      "loss": 3.2137,
      "step": 1100
    },
    {
      "epoch": 0.11684924360980699,
      "grad_norm": 4.623457431793213,
      "learning_rate": 4.822195079392776e-05,
      "loss": 3.1668,
      "step": 1120
    },
    {
      "epoch": 0.1189358372456964,
      "grad_norm": 5.512795925140381,
      "learning_rate": 4.818705287035422e-05,
      "loss": 3.2137,
      "step": 1140
    },
    {
      "epoch": 0.1210224308815858,
      "grad_norm": 5.0648393630981445,
      "learning_rate": 4.815215494678067e-05,
      "loss": 3.3303,
      "step": 1160
    },
    {
      "epoch": 0.12310902451747523,
      "grad_norm": 5.118282318115234,
      "learning_rate": 4.811725702320713e-05,
      "loss": 3.2117,
      "step": 1180
    },
    {
      "epoch": 0.12519561815336464,
      "grad_norm": 4.937882423400879,
      "learning_rate": 4.808235909963357e-05,
      "loss": 3.1639,
      "step": 1200
    },
    {
      "epoch": 0.12728221178925403,
      "grad_norm": 5.2977190017700195,
      "learning_rate": 4.804746117606003e-05,
      "loss": 3.1739,
      "step": 1220
    },
    {
      "epoch": 0.12936880542514345,
      "grad_norm": 4.901418209075928,
      "learning_rate": 4.801256325248648e-05,
      "loss": 3.2026,
      "step": 1240
    },
    {
      "epoch": 0.13145539906103287,
      "grad_norm": 6.679388523101807,
      "learning_rate": 4.7977665328912926e-05,
      "loss": 3.2468,
      "step": 1260
    },
    {
      "epoch": 0.13354199269692227,
      "grad_norm": 4.555056095123291,
      "learning_rate": 4.7942767405339385e-05,
      "loss": 3.1696,
      "step": 1280
    },
    {
      "epoch": 0.1356285863328117,
      "grad_norm": 6.380293846130371,
      "learning_rate": 4.7907869481765836e-05,
      "loss": 3.2156,
      "step": 1300
    },
    {
      "epoch": 0.13771517996870108,
      "grad_norm": 4.840030670166016,
      "learning_rate": 4.7872971558192294e-05,
      "loss": 3.2221,
      "step": 1320
    },
    {
      "epoch": 0.1398017736045905,
      "grad_norm": 5.853987693786621,
      "learning_rate": 4.783807363461874e-05,
      "loss": 3.3622,
      "step": 1340
    },
    {
      "epoch": 0.14188836724047993,
      "grad_norm": 6.84716796875,
      "learning_rate": 4.78031757110452e-05,
      "loss": 3.0986,
      "step": 1360
    },
    {
      "epoch": 0.14397496087636932,
      "grad_norm": 4.999035835266113,
      "learning_rate": 4.776827778747165e-05,
      "loss": 3.2123,
      "step": 1380
    },
    {
      "epoch": 0.14606155451225875,
      "grad_norm": 5.219056129455566,
      "learning_rate": 4.77333798638981e-05,
      "loss": 3.1331,
      "step": 1400
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 5.242255210876465,
      "learning_rate": 4.769848194032455e-05,
      "loss": 3.2652,
      "step": 1420
    },
    {
      "epoch": 0.15023474178403756,
      "grad_norm": 5.382508277893066,
      "learning_rate": 4.7663584016751e-05,
      "loss": 3.0853,
      "step": 1440
    },
    {
      "epoch": 0.15232133541992696,
      "grad_norm": 4.553712368011475,
      "learning_rate": 4.762868609317746e-05,
      "loss": 3.2535,
      "step": 1460
    },
    {
      "epoch": 0.15440792905581638,
      "grad_norm": 5.311634540557861,
      "learning_rate": 4.7593788169603906e-05,
      "loss": 3.2756,
      "step": 1480
    },
    {
      "epoch": 0.1564945226917058,
      "grad_norm": 5.5282416343688965,
      "learning_rate": 4.7558890246030364e-05,
      "loss": 3.2041,
      "step": 1500
    },
    {
      "epoch": 0.1585811163275952,
      "grad_norm": 5.015432357788086,
      "learning_rate": 4.7523992322456816e-05,
      "loss": 3.072,
      "step": 1520
    },
    {
      "epoch": 0.16066770996348462,
      "grad_norm": 4.37262487411499,
      "learning_rate": 4.748909439888327e-05,
      "loss": 3.1103,
      "step": 1540
    },
    {
      "epoch": 0.162754303599374,
      "grad_norm": 3.7587945461273193,
      "learning_rate": 4.745419647530972e-05,
      "loss": 3.2073,
      "step": 1560
    },
    {
      "epoch": 0.16484089723526343,
      "grad_norm": 4.9616899490356445,
      "learning_rate": 4.741929855173618e-05,
      "loss": 3.1268,
      "step": 1580
    },
    {
      "epoch": 0.16692749087115286,
      "grad_norm": 4.82269811630249,
      "learning_rate": 4.738440062816263e-05,
      "loss": 3.071,
      "step": 1600
    },
    {
      "epoch": 0.16901408450704225,
      "grad_norm": 4.841825485229492,
      "learning_rate": 4.734950270458908e-05,
      "loss": 3.0842,
      "step": 1620
    },
    {
      "epoch": 0.17110067814293167,
      "grad_norm": 4.831801891326904,
      "learning_rate": 4.731460478101553e-05,
      "loss": 3.0226,
      "step": 1640
    },
    {
      "epoch": 0.17318727177882107,
      "grad_norm": 4.938666343688965,
      "learning_rate": 4.727970685744198e-05,
      "loss": 3.2287,
      "step": 1660
    },
    {
      "epoch": 0.1752738654147105,
      "grad_norm": 5.5708794593811035,
      "learning_rate": 4.724480893386844e-05,
      "loss": 3.263,
      "step": 1680
    },
    {
      "epoch": 0.17736045905059988,
      "grad_norm": 4.615349769592285,
      "learning_rate": 4.7209911010294886e-05,
      "loss": 3.1172,
      "step": 1700
    },
    {
      "epoch": 0.1794470526864893,
      "grad_norm": 3.5827927589416504,
      "learning_rate": 4.7175013086721344e-05,
      "loss": 3.1519,
      "step": 1720
    },
    {
      "epoch": 0.18153364632237873,
      "grad_norm": 7.185900688171387,
      "learning_rate": 4.7140115163147796e-05,
      "loss": 3.1575,
      "step": 1740
    },
    {
      "epoch": 0.18362023995826812,
      "grad_norm": 4.2270331382751465,
      "learning_rate": 4.710521723957425e-05,
      "loss": 3.0586,
      "step": 1760
    },
    {
      "epoch": 0.18570683359415754,
      "grad_norm": 5.219381809234619,
      "learning_rate": 4.70703193160007e-05,
      "loss": 3.1674,
      "step": 1780
    },
    {
      "epoch": 0.18779342723004694,
      "grad_norm": 6.1258625984191895,
      "learning_rate": 4.703542139242715e-05,
      "loss": 3.1921,
      "step": 1800
    },
    {
      "epoch": 0.18988002086593636,
      "grad_norm": 4.079378128051758,
      "learning_rate": 4.700052346885361e-05,
      "loss": 3.1164,
      "step": 1820
    },
    {
      "epoch": 0.19196661450182578,
      "grad_norm": 4.5688934326171875,
      "learning_rate": 4.696562554528005e-05,
      "loss": 3.1529,
      "step": 1840
    },
    {
      "epoch": 0.19405320813771518,
      "grad_norm": 4.827645778656006,
      "learning_rate": 4.693072762170651e-05,
      "loss": 3.145,
      "step": 1860
    },
    {
      "epoch": 0.1961398017736046,
      "grad_norm": 3.902463436126709,
      "learning_rate": 4.689582969813296e-05,
      "loss": 3.06,
      "step": 1880
    },
    {
      "epoch": 0.198226395409494,
      "grad_norm": 3.332993268966675,
      "learning_rate": 4.6860931774559414e-05,
      "loss": 3.1124,
      "step": 1900
    },
    {
      "epoch": 0.20031298904538342,
      "grad_norm": 4.431176662445068,
      "learning_rate": 4.6826033850985866e-05,
      "loss": 3.1095,
      "step": 1920
    },
    {
      "epoch": 0.2023995826812728,
      "grad_norm": 4.972181797027588,
      "learning_rate": 4.6791135927412324e-05,
      "loss": 3.1099,
      "step": 1940
    },
    {
      "epoch": 0.20448617631716223,
      "grad_norm": 4.495469570159912,
      "learning_rate": 4.6756238003838776e-05,
      "loss": 3.0138,
      "step": 1960
    },
    {
      "epoch": 0.20657276995305165,
      "grad_norm": 3.843961477279663,
      "learning_rate": 4.672134008026523e-05,
      "loss": 3.0879,
      "step": 1980
    },
    {
      "epoch": 0.20865936358894105,
      "grad_norm": 4.390803337097168,
      "learning_rate": 4.668644215669168e-05,
      "loss": 3.2601,
      "step": 2000
    },
    {
      "epoch": 0.21074595722483047,
      "grad_norm": 5.105547904968262,
      "learning_rate": 4.665154423311813e-05,
      "loss": 3.2192,
      "step": 2020
    },
    {
      "epoch": 0.21283255086071987,
      "grad_norm": 4.243202209472656,
      "learning_rate": 4.661664630954458e-05,
      "loss": 3.146,
      "step": 2040
    },
    {
      "epoch": 0.2149191444966093,
      "grad_norm": 4.895033359527588,
      "learning_rate": 4.658174838597103e-05,
      "loss": 3.0837,
      "step": 2060
    },
    {
      "epoch": 0.21700573813249868,
      "grad_norm": 3.718737840652466,
      "learning_rate": 4.654685046239749e-05,
      "loss": 3.2413,
      "step": 2080
    },
    {
      "epoch": 0.2190923317683881,
      "grad_norm": 4.262011528015137,
      "learning_rate": 4.651195253882394e-05,
      "loss": 3.1358,
      "step": 2100
    },
    {
      "epoch": 0.22117892540427753,
      "grad_norm": 4.140048503875732,
      "learning_rate": 4.6477054615250394e-05,
      "loss": 3.1296,
      "step": 2120
    },
    {
      "epoch": 0.22326551904016692,
      "grad_norm": 5.6192450523376465,
      "learning_rate": 4.6442156691676846e-05,
      "loss": 3.1484,
      "step": 2140
    },
    {
      "epoch": 0.22535211267605634,
      "grad_norm": 4.639669418334961,
      "learning_rate": 4.6407258768103304e-05,
      "loss": 3.1299,
      "step": 2160
    },
    {
      "epoch": 0.22743870631194574,
      "grad_norm": 3.923673152923584,
      "learning_rate": 4.6372360844529755e-05,
      "loss": 3.0901,
      "step": 2180
    },
    {
      "epoch": 0.22952529994783516,
      "grad_norm": 4.799685001373291,
      "learning_rate": 4.633746292095621e-05,
      "loss": 3.1255,
      "step": 2200
    },
    {
      "epoch": 0.23161189358372458,
      "grad_norm": 4.7286529541015625,
      "learning_rate": 4.630256499738266e-05,
      "loss": 3.0743,
      "step": 2220
    },
    {
      "epoch": 0.23369848721961398,
      "grad_norm": 4.7300519943237305,
      "learning_rate": 4.626766707380911e-05,
      "loss": 3.2325,
      "step": 2240
    },
    {
      "epoch": 0.2357850808555034,
      "grad_norm": 3.534453868865967,
      "learning_rate": 4.623276915023556e-05,
      "loss": 3.0891,
      "step": 2260
    },
    {
      "epoch": 0.2378716744913928,
      "grad_norm": 4.701668739318848,
      "learning_rate": 4.619787122666201e-05,
      "loss": 3.1732,
      "step": 2280
    },
    {
      "epoch": 0.23995826812728221,
      "grad_norm": 4.207893371582031,
      "learning_rate": 4.616297330308847e-05,
      "loss": 3.154,
      "step": 2300
    },
    {
      "epoch": 0.2420448617631716,
      "grad_norm": 4.564828872680664,
      "learning_rate": 4.612807537951492e-05,
      "loss": 3.0861,
      "step": 2320
    },
    {
      "epoch": 0.24413145539906103,
      "grad_norm": 6.206182956695557,
      "learning_rate": 4.6093177455941374e-05,
      "loss": 3.2017,
      "step": 2340
    },
    {
      "epoch": 0.24621804903495045,
      "grad_norm": 5.528647422790527,
      "learning_rate": 4.6058279532367826e-05,
      "loss": 2.9777,
      "step": 2360
    },
    {
      "epoch": 0.24830464267083985,
      "grad_norm": 4.595165729522705,
      "learning_rate": 4.602338160879428e-05,
      "loss": 3.196,
      "step": 2380
    },
    {
      "epoch": 0.25039123630672927,
      "grad_norm": 4.470137119293213,
      "learning_rate": 4.598848368522073e-05,
      "loss": 3.1011,
      "step": 2400
    },
    {
      "epoch": 0.2524778299426187,
      "grad_norm": 3.6092429161071777,
      "learning_rate": 4.595358576164718e-05,
      "loss": 3.1179,
      "step": 2420
    },
    {
      "epoch": 0.25456442357850806,
      "grad_norm": 3.9862513542175293,
      "learning_rate": 4.591868783807364e-05,
      "loss": 3.161,
      "step": 2440
    },
    {
      "epoch": 0.2566510172143975,
      "grad_norm": 4.707757949829102,
      "learning_rate": 4.588378991450009e-05,
      "loss": 3.0386,
      "step": 2460
    },
    {
      "epoch": 0.2587376108502869,
      "grad_norm": 5.173054218292236,
      "learning_rate": 4.584889199092654e-05,
      "loss": 3.1102,
      "step": 2480
    },
    {
      "epoch": 0.2608242044861763,
      "grad_norm": 5.846890926361084,
      "learning_rate": 4.581399406735299e-05,
      "loss": 3.1239,
      "step": 2500
    },
    {
      "epoch": 0.26291079812206575,
      "grad_norm": 3.8958334922790527,
      "learning_rate": 4.577909614377945e-05,
      "loss": 3.0134,
      "step": 2520
    },
    {
      "epoch": 0.2649973917579551,
      "grad_norm": 4.169942378997803,
      "learning_rate": 4.5744198220205896e-05,
      "loss": 3.0428,
      "step": 2540
    },
    {
      "epoch": 0.26708398539384454,
      "grad_norm": 5.019186973571777,
      "learning_rate": 4.5709300296632354e-05,
      "loss": 3.1816,
      "step": 2560
    },
    {
      "epoch": 0.26917057902973396,
      "grad_norm": 4.442682266235352,
      "learning_rate": 4.5674402373058805e-05,
      "loss": 3.1002,
      "step": 2580
    },
    {
      "epoch": 0.2712571726656234,
      "grad_norm": 3.949509859085083,
      "learning_rate": 4.563950444948526e-05,
      "loss": 3.0196,
      "step": 2600
    },
    {
      "epoch": 0.2733437663015128,
      "grad_norm": 4.823343276977539,
      "learning_rate": 4.560460652591171e-05,
      "loss": 3.1364,
      "step": 2620
    },
    {
      "epoch": 0.27543035993740217,
      "grad_norm": 4.547382831573486,
      "learning_rate": 4.556970860233816e-05,
      "loss": 3.1214,
      "step": 2640
    },
    {
      "epoch": 0.2775169535732916,
      "grad_norm": 4.781062602996826,
      "learning_rate": 4.553481067876462e-05,
      "loss": 3.0475,
      "step": 2660
    },
    {
      "epoch": 0.279603547209181,
      "grad_norm": 4.325123310089111,
      "learning_rate": 4.549991275519107e-05,
      "loss": 3.1223,
      "step": 2680
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 4.388126373291016,
      "learning_rate": 4.546501483161752e-05,
      "loss": 3.1086,
      "step": 2700
    },
    {
      "epoch": 0.28377673448095986,
      "grad_norm": 4.129994869232178,
      "learning_rate": 4.543011690804397e-05,
      "loss": 3.0308,
      "step": 2720
    },
    {
      "epoch": 0.2858633281168492,
      "grad_norm": 4.335352897644043,
      "learning_rate": 4.539521898447043e-05,
      "loss": 3.0336,
      "step": 2740
    },
    {
      "epoch": 0.28794992175273865,
      "grad_norm": 5.634723663330078,
      "learning_rate": 4.5360321060896876e-05,
      "loss": 2.9924,
      "step": 2760
    },
    {
      "epoch": 0.29003651538862807,
      "grad_norm": 4.26605749130249,
      "learning_rate": 4.5325423137323334e-05,
      "loss": 3.1001,
      "step": 2780
    },
    {
      "epoch": 0.2921231090245175,
      "grad_norm": 5.158514976501465,
      "learning_rate": 4.5290525213749785e-05,
      "loss": 3.1164,
      "step": 2800
    },
    {
      "epoch": 0.2942097026604069,
      "grad_norm": 4.633584976196289,
      "learning_rate": 4.525562729017624e-05,
      "loss": 3.0531,
      "step": 2820
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 4.679189682006836,
      "learning_rate": 4.522072936660269e-05,
      "loss": 3.1531,
      "step": 2840
    },
    {
      "epoch": 0.2983828899321857,
      "grad_norm": 3.9938855171203613,
      "learning_rate": 4.518583144302914e-05,
      "loss": 3.093,
      "step": 2860
    },
    {
      "epoch": 0.3004694835680751,
      "grad_norm": 4.6060099601745605,
      "learning_rate": 4.51509335194556e-05,
      "loss": 3.0236,
      "step": 2880
    },
    {
      "epoch": 0.30255607720396455,
      "grad_norm": 3.9438393115997314,
      "learning_rate": 4.511603559588204e-05,
      "loss": 3.0791,
      "step": 2900
    },
    {
      "epoch": 0.3046426708398539,
      "grad_norm": 5.046111106872559,
      "learning_rate": 4.50811376723085e-05,
      "loss": 3.1802,
      "step": 2920
    },
    {
      "epoch": 0.30672926447574334,
      "grad_norm": 4.5174174308776855,
      "learning_rate": 4.504623974873495e-05,
      "loss": 3.08,
      "step": 2940
    },
    {
      "epoch": 0.30881585811163276,
      "grad_norm": 6.069310188293457,
      "learning_rate": 4.5011341825161404e-05,
      "loss": 3.0218,
      "step": 2960
    },
    {
      "epoch": 0.3109024517475222,
      "grad_norm": 4.568482398986816,
      "learning_rate": 4.4976443901587855e-05,
      "loss": 3.1327,
      "step": 2980
    },
    {
      "epoch": 0.3129890453834116,
      "grad_norm": 4.63304328918457,
      "learning_rate": 4.494154597801431e-05,
      "loss": 3.0283,
      "step": 3000
    },
    {
      "epoch": 0.31507563901930097,
      "grad_norm": 3.685300588607788,
      "learning_rate": 4.4906648054440765e-05,
      "loss": 3.0828,
      "step": 3020
    },
    {
      "epoch": 0.3171622326551904,
      "grad_norm": 7.58882999420166,
      "learning_rate": 4.487175013086721e-05,
      "loss": 3.1282,
      "step": 3040
    },
    {
      "epoch": 0.3192488262910798,
      "grad_norm": 4.250005722045898,
      "learning_rate": 4.483685220729367e-05,
      "loss": 3.1233,
      "step": 3060
    },
    {
      "epoch": 0.32133541992696923,
      "grad_norm": 4.89195442199707,
      "learning_rate": 4.480195428372012e-05,
      "loss": 3.067,
      "step": 3080
    },
    {
      "epoch": 0.32342201356285866,
      "grad_norm": 3.8936619758605957,
      "learning_rate": 4.476705636014658e-05,
      "loss": 2.9934,
      "step": 3100
    },
    {
      "epoch": 0.325508607198748,
      "grad_norm": 4.719967365264893,
      "learning_rate": 4.473215843657302e-05,
      "loss": 3.0584,
      "step": 3120
    },
    {
      "epoch": 0.32759520083463745,
      "grad_norm": 4.011388301849365,
      "learning_rate": 4.469726051299948e-05,
      "loss": 3.215,
      "step": 3140
    },
    {
      "epoch": 0.32968179447052687,
      "grad_norm": 4.220953464508057,
      "learning_rate": 4.466236258942593e-05,
      "loss": 2.8953,
      "step": 3160
    },
    {
      "epoch": 0.3317683881064163,
      "grad_norm": 5.525933265686035,
      "learning_rate": 4.4627464665852384e-05,
      "loss": 3.0171,
      "step": 3180
    },
    {
      "epoch": 0.3338549817423057,
      "grad_norm": 3.996816635131836,
      "learning_rate": 4.4592566742278835e-05,
      "loss": 3.0935,
      "step": 3200
    },
    {
      "epoch": 0.3359415753781951,
      "grad_norm": 4.196084499359131,
      "learning_rate": 4.455766881870529e-05,
      "loss": 3.0198,
      "step": 3220
    },
    {
      "epoch": 0.3380281690140845,
      "grad_norm": 4.375573635101318,
      "learning_rate": 4.4522770895131745e-05,
      "loss": 3.0721,
      "step": 3240
    },
    {
      "epoch": 0.3401147626499739,
      "grad_norm": 4.368087291717529,
      "learning_rate": 4.448787297155819e-05,
      "loss": 3.0082,
      "step": 3260
    },
    {
      "epoch": 0.34220135628586335,
      "grad_norm": 4.287816047668457,
      "learning_rate": 4.445297504798465e-05,
      "loss": 3.0696,
      "step": 3280
    },
    {
      "epoch": 0.3442879499217527,
      "grad_norm": 4.814499378204346,
      "learning_rate": 4.44180771244111e-05,
      "loss": 3.0289,
      "step": 3300
    },
    {
      "epoch": 0.34637454355764213,
      "grad_norm": 4.124007225036621,
      "learning_rate": 4.438317920083756e-05,
      "loss": 2.9442,
      "step": 3320
    },
    {
      "epoch": 0.34846113719353156,
      "grad_norm": 4.4382219314575195,
      "learning_rate": 4.4348281277264e-05,
      "loss": 3.0591,
      "step": 3340
    },
    {
      "epoch": 0.350547730829421,
      "grad_norm": 4.173131465911865,
      "learning_rate": 4.431338335369046e-05,
      "loss": 2.918,
      "step": 3360
    },
    {
      "epoch": 0.3526343244653104,
      "grad_norm": 4.489366054534912,
      "learning_rate": 4.427848543011691e-05,
      "loss": 2.9431,
      "step": 3380
    },
    {
      "epoch": 0.35472091810119977,
      "grad_norm": 4.135776519775391,
      "learning_rate": 4.4243587506543364e-05,
      "loss": 3.042,
      "step": 3400
    },
    {
      "epoch": 0.3568075117370892,
      "grad_norm": 5.762712478637695,
      "learning_rate": 4.4208689582969815e-05,
      "loss": 3.0905,
      "step": 3420
    },
    {
      "epoch": 0.3588941053729786,
      "grad_norm": 4.56895637512207,
      "learning_rate": 4.4173791659396267e-05,
      "loss": 2.9784,
      "step": 3440
    },
    {
      "epoch": 0.36098069900886803,
      "grad_norm": 3.8606607913970947,
      "learning_rate": 4.4138893735822725e-05,
      "loss": 2.9523,
      "step": 3460
    },
    {
      "epoch": 0.36306729264475746,
      "grad_norm": 4.257267475128174,
      "learning_rate": 4.410399581224917e-05,
      "loss": 3.1679,
      "step": 3480
    },
    {
      "epoch": 0.3651538862806468,
      "grad_norm": 3.9343411922454834,
      "learning_rate": 4.406909788867563e-05,
      "loss": 3.041,
      "step": 3500
    },
    {
      "epoch": 0.36724047991653624,
      "grad_norm": 5.1827592849731445,
      "learning_rate": 4.403419996510208e-05,
      "loss": 2.9866,
      "step": 3520
    },
    {
      "epoch": 0.36932707355242567,
      "grad_norm": 4.040452003479004,
      "learning_rate": 4.399930204152853e-05,
      "loss": 3.026,
      "step": 3540
    },
    {
      "epoch": 0.3714136671883151,
      "grad_norm": 4.99531888961792,
      "learning_rate": 4.396440411795498e-05,
      "loss": 3.0404,
      "step": 3560
    },
    {
      "epoch": 0.3735002608242045,
      "grad_norm": 4.086441516876221,
      "learning_rate": 4.3929506194381434e-05,
      "loss": 2.9823,
      "step": 3580
    },
    {
      "epoch": 0.3755868544600939,
      "grad_norm": 4.018301963806152,
      "learning_rate": 4.389460827080789e-05,
      "loss": 3.0453,
      "step": 3600
    },
    {
      "epoch": 0.3776734480959833,
      "grad_norm": 3.2475578784942627,
      "learning_rate": 4.385971034723434e-05,
      "loss": 3.0101,
      "step": 3620
    },
    {
      "epoch": 0.3797600417318727,
      "grad_norm": 3.8191421031951904,
      "learning_rate": 4.3824812423660795e-05,
      "loss": 3.1059,
      "step": 3640
    },
    {
      "epoch": 0.38184663536776214,
      "grad_norm": 4.01564884185791,
      "learning_rate": 4.3789914500087246e-05,
      "loss": 3.0677,
      "step": 3660
    },
    {
      "epoch": 0.38393322900365157,
      "grad_norm": 3.7053349018096924,
      "learning_rate": 4.3755016576513705e-05,
      "loss": 2.9772,
      "step": 3680
    },
    {
      "epoch": 0.38601982263954093,
      "grad_norm": 4.546380996704102,
      "learning_rate": 4.372011865294015e-05,
      "loss": 2.9929,
      "step": 3700
    },
    {
      "epoch": 0.38810641627543035,
      "grad_norm": 4.126625061035156,
      "learning_rate": 4.368522072936661e-05,
      "loss": 3.0654,
      "step": 3720
    },
    {
      "epoch": 0.3901930099113198,
      "grad_norm": 5.400767803192139,
      "learning_rate": 4.365032280579306e-05,
      "loss": 2.9218,
      "step": 3740
    },
    {
      "epoch": 0.3922796035472092,
      "grad_norm": 5.654619216918945,
      "learning_rate": 4.361542488221951e-05,
      "loss": 3.0443,
      "step": 3760
    },
    {
      "epoch": 0.39436619718309857,
      "grad_norm": 3.9149630069732666,
      "learning_rate": 4.358052695864596e-05,
      "loss": 3.0217,
      "step": 3780
    },
    {
      "epoch": 0.396452790818988,
      "grad_norm": 4.434333324432373,
      "learning_rate": 4.3545629035072414e-05,
      "loss": 2.9685,
      "step": 3800
    },
    {
      "epoch": 0.3985393844548774,
      "grad_norm": 4.719333648681641,
      "learning_rate": 4.351073111149887e-05,
      "loss": 3.0047,
      "step": 3820
    },
    {
      "epoch": 0.40062597809076683,
      "grad_norm": 4.428431034088135,
      "learning_rate": 4.3475833187925316e-05,
      "loss": 2.9305,
      "step": 3840
    },
    {
      "epoch": 0.40271257172665625,
      "grad_norm": 4.186082363128662,
      "learning_rate": 4.3440935264351775e-05,
      "loss": 3.0395,
      "step": 3860
    },
    {
      "epoch": 0.4047991653625456,
      "grad_norm": 3.747725486755371,
      "learning_rate": 4.3406037340778226e-05,
      "loss": 3.0151,
      "step": 3880
    },
    {
      "epoch": 0.40688575899843504,
      "grad_norm": 4.2763895988464355,
      "learning_rate": 4.337113941720468e-05,
      "loss": 3.0673,
      "step": 3900
    },
    {
      "epoch": 0.40897235263432447,
      "grad_norm": 3.393681764602661,
      "learning_rate": 4.333624149363113e-05,
      "loss": 3.0658,
      "step": 3920
    },
    {
      "epoch": 0.4110589462702139,
      "grad_norm": 4.655083656311035,
      "learning_rate": 4.330134357005759e-05,
      "loss": 2.973,
      "step": 3940
    },
    {
      "epoch": 0.4131455399061033,
      "grad_norm": 4.667159557342529,
      "learning_rate": 4.326644564648404e-05,
      "loss": 2.9727,
      "step": 3960
    },
    {
      "epoch": 0.4152321335419927,
      "grad_norm": 3.87538480758667,
      "learning_rate": 4.323154772291049e-05,
      "loss": 2.9804,
      "step": 3980
    },
    {
      "epoch": 0.4173187271778821,
      "grad_norm": 3.97283935546875,
      "learning_rate": 4.319664979933694e-05,
      "loss": 2.9011,
      "step": 4000
    },
    {
      "epoch": 0.4194053208137715,
      "grad_norm": 4.390599727630615,
      "learning_rate": 4.316175187576339e-05,
      "loss": 3.0264,
      "step": 4020
    },
    {
      "epoch": 0.42149191444966094,
      "grad_norm": 3.8767127990722656,
      "learning_rate": 4.3126853952189845e-05,
      "loss": 3.0708,
      "step": 4040
    },
    {
      "epoch": 0.42357850808555036,
      "grad_norm": 3.879826784133911,
      "learning_rate": 4.3091956028616296e-05,
      "loss": 3.1004,
      "step": 4060
    },
    {
      "epoch": 0.42566510172143973,
      "grad_norm": 4.616877555847168,
      "learning_rate": 4.3057058105042755e-05,
      "loss": 2.9755,
      "step": 4080
    },
    {
      "epoch": 0.42775169535732915,
      "grad_norm": 4.9031524658203125,
      "learning_rate": 4.3022160181469206e-05,
      "loss": 3.0581,
      "step": 4100
    },
    {
      "epoch": 0.4298382889932186,
      "grad_norm": 3.6483168601989746,
      "learning_rate": 4.298726225789566e-05,
      "loss": 2.9528,
      "step": 4120
    },
    {
      "epoch": 0.431924882629108,
      "grad_norm": 4.878682613372803,
      "learning_rate": 4.295236433432211e-05,
      "loss": 3.0653,
      "step": 4140
    },
    {
      "epoch": 0.43401147626499736,
      "grad_norm": 5.435789585113525,
      "learning_rate": 4.291746641074856e-05,
      "loss": 3.055,
      "step": 4160
    },
    {
      "epoch": 0.4360980699008868,
      "grad_norm": 4.027890205383301,
      "learning_rate": 4.288256848717502e-05,
      "loss": 3.0212,
      "step": 4180
    },
    {
      "epoch": 0.4381846635367762,
      "grad_norm": 4.376358509063721,
      "learning_rate": 4.2847670563601463e-05,
      "loss": 3.0221,
      "step": 4200
    },
    {
      "epoch": 0.44027125717266563,
      "grad_norm": 3.9088480472564697,
      "learning_rate": 4.281277264002792e-05,
      "loss": 2.9879,
      "step": 4220
    },
    {
      "epoch": 0.44235785080855505,
      "grad_norm": 3.341165065765381,
      "learning_rate": 4.277787471645437e-05,
      "loss": 2.998,
      "step": 4240
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 4.181614875793457,
      "learning_rate": 4.2742976792880825e-05,
      "loss": 2.9672,
      "step": 4260
    },
    {
      "epoch": 0.44653103808033384,
      "grad_norm": 4.638290882110596,
      "learning_rate": 4.2708078869307276e-05,
      "loss": 2.9533,
      "step": 4280
    },
    {
      "epoch": 0.44861763171622326,
      "grad_norm": 4.5822529792785645,
      "learning_rate": 4.2673180945733734e-05,
      "loss": 3.0354,
      "step": 4300
    },
    {
      "epoch": 0.4507042253521127,
      "grad_norm": 4.305933952331543,
      "learning_rate": 4.2638283022160186e-05,
      "loss": 3.0115,
      "step": 4320
    },
    {
      "epoch": 0.4527908189880021,
      "grad_norm": 4.0466203689575195,
      "learning_rate": 4.260338509858664e-05,
      "loss": 3.0553,
      "step": 4340
    },
    {
      "epoch": 0.4548774126238915,
      "grad_norm": 4.5540289878845215,
      "learning_rate": 4.256848717501309e-05,
      "loss": 2.9736,
      "step": 4360
    },
    {
      "epoch": 0.4569640062597809,
      "grad_norm": 4.099608421325684,
      "learning_rate": 4.253358925143954e-05,
      "loss": 3.0329,
      "step": 4380
    },
    {
      "epoch": 0.4590505998956703,
      "grad_norm": 5.406778335571289,
      "learning_rate": 4.249869132786599e-05,
      "loss": 2.9959,
      "step": 4400
    },
    {
      "epoch": 0.46113719353155974,
      "grad_norm": 3.9117729663848877,
      "learning_rate": 4.246379340429244e-05,
      "loss": 2.8733,
      "step": 4420
    },
    {
      "epoch": 0.46322378716744916,
      "grad_norm": 3.830939769744873,
      "learning_rate": 4.24288954807189e-05,
      "loss": 3.0142,
      "step": 4440
    },
    {
      "epoch": 0.46531038080333853,
      "grad_norm": 3.6433560848236084,
      "learning_rate": 4.239399755714535e-05,
      "loss": 2.9818,
      "step": 4460
    },
    {
      "epoch": 0.46739697443922795,
      "grad_norm": 3.0473105907440186,
      "learning_rate": 4.2359099633571805e-05,
      "loss": 2.9284,
      "step": 4480
    },
    {
      "epoch": 0.4694835680751174,
      "grad_norm": 3.7079508304595947,
      "learning_rate": 4.2324201709998256e-05,
      "loss": 2.9511,
      "step": 4500
    },
    {
      "epoch": 0.4715701617110068,
      "grad_norm": 4.121558666229248,
      "learning_rate": 4.2289303786424714e-05,
      "loss": 2.8588,
      "step": 4520
    },
    {
      "epoch": 0.4736567553468962,
      "grad_norm": 4.4139018058776855,
      "learning_rate": 4.225440586285116e-05,
      "loss": 2.9413,
      "step": 4540
    },
    {
      "epoch": 0.4757433489827856,
      "grad_norm": 4.9527764320373535,
      "learning_rate": 4.221950793927761e-05,
      "loss": 3.0977,
      "step": 4560
    },
    {
      "epoch": 0.477829942618675,
      "grad_norm": 4.145630836486816,
      "learning_rate": 4.218461001570407e-05,
      "loss": 3.0738,
      "step": 4580
    },
    {
      "epoch": 0.47991653625456443,
      "grad_norm": 3.9452247619628906,
      "learning_rate": 4.214971209213052e-05,
      "loss": 2.998,
      "step": 4600
    },
    {
      "epoch": 0.48200312989045385,
      "grad_norm": 4.781635761260986,
      "learning_rate": 4.211481416855697e-05,
      "loss": 2.9158,
      "step": 4620
    },
    {
      "epoch": 0.4840897235263432,
      "grad_norm": 3.8203341960906982,
      "learning_rate": 4.207991624498342e-05,
      "loss": 3.0892,
      "step": 4640
    },
    {
      "epoch": 0.48617631716223264,
      "grad_norm": 3.7263739109039307,
      "learning_rate": 4.204501832140988e-05,
      "loss": 3.0087,
      "step": 4660
    },
    {
      "epoch": 0.48826291079812206,
      "grad_norm": 4.970513343811035,
      "learning_rate": 4.201012039783633e-05,
      "loss": 2.9027,
      "step": 4680
    },
    {
      "epoch": 0.4903495044340115,
      "grad_norm": 4.6976823806762695,
      "learning_rate": 4.1975222474262784e-05,
      "loss": 2.996,
      "step": 4700
    },
    {
      "epoch": 0.4924360980699009,
      "grad_norm": 4.146852016448975,
      "learning_rate": 4.1940324550689236e-05,
      "loss": 2.9733,
      "step": 4720
    },
    {
      "epoch": 0.4945226917057903,
      "grad_norm": 3.9613828659057617,
      "learning_rate": 4.190542662711569e-05,
      "loss": 2.9248,
      "step": 4740
    },
    {
      "epoch": 0.4966092853416797,
      "grad_norm": 3.833705186843872,
      "learning_rate": 4.187052870354214e-05,
      "loss": 3.0029,
      "step": 4760
    },
    {
      "epoch": 0.4986958789775691,
      "grad_norm": 4.743813991546631,
      "learning_rate": 4.183563077996859e-05,
      "loss": 2.9368,
      "step": 4780
    },
    {
      "epoch": 0.5007824726134585,
      "grad_norm": 4.282831192016602,
      "learning_rate": 4.180073285639505e-05,
      "loss": 2.9231,
      "step": 4800
    },
    {
      "epoch": 0.5028690662493479,
      "grad_norm": 4.750066757202148,
      "learning_rate": 4.17658349328215e-05,
      "loss": 3.0151,
      "step": 4820
    },
    {
      "epoch": 0.5049556598852374,
      "grad_norm": 4.1804304122924805,
      "learning_rate": 4.173093700924795e-05,
      "loss": 2.9919,
      "step": 4840
    },
    {
      "epoch": 0.5070422535211268,
      "grad_norm": 4.071496963500977,
      "learning_rate": 4.16960390856744e-05,
      "loss": 2.9001,
      "step": 4860
    },
    {
      "epoch": 0.5091288471570161,
      "grad_norm": 4.91242790222168,
      "learning_rate": 4.166114116210086e-05,
      "loss": 2.9643,
      "step": 4880
    },
    {
      "epoch": 0.5112154407929056,
      "grad_norm": 3.5319650173187256,
      "learning_rate": 4.1626243238527306e-05,
      "loss": 3.0142,
      "step": 4900
    },
    {
      "epoch": 0.513302034428795,
      "grad_norm": 3.3401644229888916,
      "learning_rate": 4.1591345314953764e-05,
      "loss": 2.9845,
      "step": 4920
    },
    {
      "epoch": 0.5153886280646844,
      "grad_norm": 4.913140296936035,
      "learning_rate": 4.1556447391380216e-05,
      "loss": 3.021,
      "step": 4940
    },
    {
      "epoch": 0.5174752217005738,
      "grad_norm": 3.8938777446746826,
      "learning_rate": 4.152154946780667e-05,
      "loss": 2.9646,
      "step": 4960
    },
    {
      "epoch": 0.5195618153364632,
      "grad_norm": 4.636960983276367,
      "learning_rate": 4.148665154423312e-05,
      "loss": 2.9284,
      "step": 4980
    },
    {
      "epoch": 0.5216484089723527,
      "grad_norm": 4.145318508148193,
      "learning_rate": 4.145175362065957e-05,
      "loss": 3.0048,
      "step": 5000
    },
    {
      "epoch": 0.523735002608242,
      "grad_norm": 3.5514466762542725,
      "learning_rate": 4.141685569708603e-05,
      "loss": 2.9733,
      "step": 5020
    },
    {
      "epoch": 0.5258215962441315,
      "grad_norm": 3.7886850833892822,
      "learning_rate": 4.138195777351247e-05,
      "loss": 2.9932,
      "step": 5040
    },
    {
      "epoch": 0.5279081898800209,
      "grad_norm": 4.412848949432373,
      "learning_rate": 4.134705984993893e-05,
      "loss": 2.9299,
      "step": 5060
    },
    {
      "epoch": 0.5299947835159102,
      "grad_norm": 3.724886417388916,
      "learning_rate": 4.131216192636538e-05,
      "loss": 2.9041,
      "step": 5080
    },
    {
      "epoch": 0.5320813771517997,
      "grad_norm": 4.02967643737793,
      "learning_rate": 4.127726400279184e-05,
      "loss": 3.0674,
      "step": 5100
    },
    {
      "epoch": 0.5341679707876891,
      "grad_norm": 3.687180995941162,
      "learning_rate": 4.1242366079218286e-05,
      "loss": 2.9386,
      "step": 5120
    },
    {
      "epoch": 0.5362545644235786,
      "grad_norm": 3.8059167861938477,
      "learning_rate": 4.120746815564474e-05,
      "loss": 2.9082,
      "step": 5140
    },
    {
      "epoch": 0.5383411580594679,
      "grad_norm": 4.054007053375244,
      "learning_rate": 4.1172570232071196e-05,
      "loss": 3.0288,
      "step": 5160
    },
    {
      "epoch": 0.5404277516953573,
      "grad_norm": 3.7624340057373047,
      "learning_rate": 4.113767230849765e-05,
      "loss": 2.8995,
      "step": 5180
    },
    {
      "epoch": 0.5425143453312468,
      "grad_norm": 4.486269474029541,
      "learning_rate": 4.11027743849241e-05,
      "loss": 2.9444,
      "step": 5200
    },
    {
      "epoch": 0.5446009389671361,
      "grad_norm": 4.5055389404296875,
      "learning_rate": 4.106787646135055e-05,
      "loss": 3.0303,
      "step": 5220
    },
    {
      "epoch": 0.5466875326030256,
      "grad_norm": 4.148828983306885,
      "learning_rate": 4.103297853777701e-05,
      "loss": 3.0209,
      "step": 5240
    },
    {
      "epoch": 0.548774126238915,
      "grad_norm": 3.8370161056518555,
      "learning_rate": 4.099808061420345e-05,
      "loss": 3.096,
      "step": 5260
    },
    {
      "epoch": 0.5508607198748043,
      "grad_norm": 5.087380409240723,
      "learning_rate": 4.096318269062991e-05,
      "loss": 2.99,
      "step": 5280
    },
    {
      "epoch": 0.5529473135106938,
      "grad_norm": 5.040654182434082,
      "learning_rate": 4.092828476705636e-05,
      "loss": 2.9428,
      "step": 5300
    },
    {
      "epoch": 0.5550339071465832,
      "grad_norm": 3.3651723861694336,
      "learning_rate": 4.0893386843482814e-05,
      "loss": 2.9509,
      "step": 5320
    },
    {
      "epoch": 0.5571205007824727,
      "grad_norm": 4.727957248687744,
      "learning_rate": 4.0858488919909266e-05,
      "loss": 2.8432,
      "step": 5340
    },
    {
      "epoch": 0.559207094418362,
      "grad_norm": 4.0087890625,
      "learning_rate": 4.082359099633572e-05,
      "loss": 2.9926,
      "step": 5360
    },
    {
      "epoch": 0.5612936880542514,
      "grad_norm": 2.974956750869751,
      "learning_rate": 4.0788693072762175e-05,
      "loss": 3.016,
      "step": 5380
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 4.6574201583862305,
      "learning_rate": 4.075379514918862e-05,
      "loss": 2.903,
      "step": 5400
    },
    {
      "epoch": 0.5654668753260302,
      "grad_norm": 3.8160595893859863,
      "learning_rate": 4.071889722561508e-05,
      "loss": 2.8471,
      "step": 5420
    },
    {
      "epoch": 0.5675534689619197,
      "grad_norm": 4.53587532043457,
      "learning_rate": 4.068399930204153e-05,
      "loss": 3.0504,
      "step": 5440
    },
    {
      "epoch": 0.5696400625978091,
      "grad_norm": 3.2927298545837402,
      "learning_rate": 4.064910137846799e-05,
      "loss": 3.0598,
      "step": 5460
    },
    {
      "epoch": 0.5717266562336984,
      "grad_norm": 3.4690065383911133,
      "learning_rate": 4.061420345489443e-05,
      "loss": 3.0455,
      "step": 5480
    },
    {
      "epoch": 0.5738132498695879,
      "grad_norm": 4.322870254516602,
      "learning_rate": 4.057930553132089e-05,
      "loss": 2.9196,
      "step": 5500
    },
    {
      "epoch": 0.5758998435054773,
      "grad_norm": 4.957510948181152,
      "learning_rate": 4.054440760774734e-05,
      "loss": 3.0344,
      "step": 5520
    },
    {
      "epoch": 0.5779864371413668,
      "grad_norm": 4.267566680908203,
      "learning_rate": 4.0509509684173794e-05,
      "loss": 2.9973,
      "step": 5540
    },
    {
      "epoch": 0.5800730307772561,
      "grad_norm": 4.124598979949951,
      "learning_rate": 4.0474611760600246e-05,
      "loss": 3.0049,
      "step": 5560
    },
    {
      "epoch": 0.5821596244131455,
      "grad_norm": 4.529849529266357,
      "learning_rate": 4.04397138370267e-05,
      "loss": 2.9371,
      "step": 5580
    },
    {
      "epoch": 0.584246218049035,
      "grad_norm": 3.503619432449341,
      "learning_rate": 4.0404815913453155e-05,
      "loss": 2.8801,
      "step": 5600
    },
    {
      "epoch": 0.5863328116849243,
      "grad_norm": 3.7584526538848877,
      "learning_rate": 4.03699179898796e-05,
      "loss": 3.0536,
      "step": 5620
    },
    {
      "epoch": 0.5884194053208138,
      "grad_norm": 3.6659302711486816,
      "learning_rate": 4.033502006630606e-05,
      "loss": 2.9175,
      "step": 5640
    },
    {
      "epoch": 0.5905059989567032,
      "grad_norm": 4.137058734893799,
      "learning_rate": 4.030012214273251e-05,
      "loss": 2.8431,
      "step": 5660
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 4.479515552520752,
      "learning_rate": 4.026522421915897e-05,
      "loss": 2.9859,
      "step": 5680
    },
    {
      "epoch": 0.594679186228482,
      "grad_norm": 4.856977939605713,
      "learning_rate": 4.023032629558541e-05,
      "loss": 2.9133,
      "step": 5700
    },
    {
      "epoch": 0.5967657798643714,
      "grad_norm": 3.791919231414795,
      "learning_rate": 4.0195428372011864e-05,
      "loss": 2.9276,
      "step": 5720
    },
    {
      "epoch": 0.5988523735002608,
      "grad_norm": 3.886505126953125,
      "learning_rate": 4.016053044843832e-05,
      "loss": 2.9949,
      "step": 5740
    },
    {
      "epoch": 0.6009389671361502,
      "grad_norm": 4.100819110870361,
      "learning_rate": 4.012563252486477e-05,
      "loss": 2.9313,
      "step": 5760
    },
    {
      "epoch": 0.6030255607720396,
      "grad_norm": 3.6618387699127197,
      "learning_rate": 4.0090734601291225e-05,
      "loss": 2.9477,
      "step": 5780
    },
    {
      "epoch": 0.6051121544079291,
      "grad_norm": 4.523789882659912,
      "learning_rate": 4.005583667771768e-05,
      "loss": 2.9099,
      "step": 5800
    },
    {
      "epoch": 0.6071987480438185,
      "grad_norm": 4.075380325317383,
      "learning_rate": 4.0020938754144135e-05,
      "loss": 2.9473,
      "step": 5820
    },
    {
      "epoch": 0.6092853416797078,
      "grad_norm": 3.757523536682129,
      "learning_rate": 3.998604083057058e-05,
      "loss": 2.9383,
      "step": 5840
    },
    {
      "epoch": 0.6113719353155973,
      "grad_norm": 3.848855972290039,
      "learning_rate": 3.995114290699704e-05,
      "loss": 2.9633,
      "step": 5860
    },
    {
      "epoch": 0.6134585289514867,
      "grad_norm": 4.544028282165527,
      "learning_rate": 3.991624498342349e-05,
      "loss": 2.879,
      "step": 5880
    },
    {
      "epoch": 0.6155451225873761,
      "grad_norm": 4.059609413146973,
      "learning_rate": 3.988134705984994e-05,
      "loss": 2.9239,
      "step": 5900
    },
    {
      "epoch": 0.6176317162232655,
      "grad_norm": 4.548040866851807,
      "learning_rate": 3.984644913627639e-05,
      "loss": 2.927,
      "step": 5920
    },
    {
      "epoch": 0.6197183098591549,
      "grad_norm": 3.834904193878174,
      "learning_rate": 3.9811551212702844e-05,
      "loss": 2.9627,
      "step": 5940
    },
    {
      "epoch": 0.6218049034950444,
      "grad_norm": 3.7331607341766357,
      "learning_rate": 3.97766532891293e-05,
      "loss": 3.0439,
      "step": 5960
    },
    {
      "epoch": 0.6238914971309337,
      "grad_norm": 4.583003520965576,
      "learning_rate": 3.974175536555575e-05,
      "loss": 2.9044,
      "step": 5980
    },
    {
      "epoch": 0.6259780907668232,
      "grad_norm": 4.813904762268066,
      "learning_rate": 3.9706857441982205e-05,
      "loss": 3.0537,
      "step": 6000
    },
    {
      "epoch": 0.6280646844027126,
      "grad_norm": 4.717869281768799,
      "learning_rate": 3.967195951840866e-05,
      "loss": 3.0101,
      "step": 6020
    },
    {
      "epoch": 0.6301512780386019,
      "grad_norm": 4.129504680633545,
      "learning_rate": 3.963706159483511e-05,
      "loss": 2.8417,
      "step": 6040
    },
    {
      "epoch": 0.6322378716744914,
      "grad_norm": 4.234941482543945,
      "learning_rate": 3.960216367126156e-05,
      "loss": 2.882,
      "step": 6060
    },
    {
      "epoch": 0.6343244653103808,
      "grad_norm": 3.8040058612823486,
      "learning_rate": 3.956726574768802e-05,
      "loss": 2.9078,
      "step": 6080
    },
    {
      "epoch": 0.6364110589462703,
      "grad_norm": 3.7576422691345215,
      "learning_rate": 3.953236782411447e-05,
      "loss": 2.9263,
      "step": 6100
    },
    {
      "epoch": 0.6384976525821596,
      "grad_norm": 4.087821006774902,
      "learning_rate": 3.949746990054092e-05,
      "loss": 3.0437,
      "step": 6120
    },
    {
      "epoch": 0.640584246218049,
      "grad_norm": 4.0199294090271,
      "learning_rate": 3.946257197696737e-05,
      "loss": 2.8812,
      "step": 6140
    },
    {
      "epoch": 0.6426708398539385,
      "grad_norm": 3.919131278991699,
      "learning_rate": 3.9427674053393824e-05,
      "loss": 2.9987,
      "step": 6160
    },
    {
      "epoch": 0.6447574334898278,
      "grad_norm": 4.138503074645996,
      "learning_rate": 3.939277612982028e-05,
      "loss": 2.9306,
      "step": 6180
    },
    {
      "epoch": 0.6468440271257173,
      "grad_norm": 3.9050252437591553,
      "learning_rate": 3.935787820624673e-05,
      "loss": 2.9505,
      "step": 6200
    },
    {
      "epoch": 0.6489306207616067,
      "grad_norm": 3.907466173171997,
      "learning_rate": 3.9322980282673185e-05,
      "loss": 3.0227,
      "step": 6220
    },
    {
      "epoch": 0.651017214397496,
      "grad_norm": 3.442305326461792,
      "learning_rate": 3.9288082359099637e-05,
      "loss": 3.0116,
      "step": 6240
    },
    {
      "epoch": 0.6531038080333855,
      "grad_norm": 4.073860168457031,
      "learning_rate": 3.925318443552609e-05,
      "loss": 3.063,
      "step": 6260
    },
    {
      "epoch": 0.6551904016692749,
      "grad_norm": 3.5174055099487305,
      "learning_rate": 3.921828651195254e-05,
      "loss": 2.8561,
      "step": 6280
    },
    {
      "epoch": 0.6572769953051644,
      "grad_norm": 3.6198387145996094,
      "learning_rate": 3.918338858837899e-05,
      "loss": 2.9656,
      "step": 6300
    },
    {
      "epoch": 0.6593635889410537,
      "grad_norm": 3.7041213512420654,
      "learning_rate": 3.914849066480545e-05,
      "loss": 2.901,
      "step": 6320
    },
    {
      "epoch": 0.6614501825769431,
      "grad_norm": 5.655436038970947,
      "learning_rate": 3.9113592741231894e-05,
      "loss": 2.8972,
      "step": 6340
    },
    {
      "epoch": 0.6635367762128326,
      "grad_norm": 4.263230800628662,
      "learning_rate": 3.907869481765835e-05,
      "loss": 3.0125,
      "step": 6360
    },
    {
      "epoch": 0.665623369848722,
      "grad_norm": 4.176274299621582,
      "learning_rate": 3.9043796894084804e-05,
      "loss": 2.8571,
      "step": 6380
    },
    {
      "epoch": 0.6677099634846114,
      "grad_norm": 3.9094741344451904,
      "learning_rate": 3.9008898970511255e-05,
      "loss": 3.0368,
      "step": 6400
    },
    {
      "epoch": 0.6697965571205008,
      "grad_norm": 4.2764153480529785,
      "learning_rate": 3.897400104693771e-05,
      "loss": 2.981,
      "step": 6420
    },
    {
      "epoch": 0.6718831507563902,
      "grad_norm": 4.034788131713867,
      "learning_rate": 3.8939103123364165e-05,
      "loss": 2.8099,
      "step": 6440
    },
    {
      "epoch": 0.6739697443922796,
      "grad_norm": 4.121728420257568,
      "learning_rate": 3.8904205199790616e-05,
      "loss": 2.9616,
      "step": 6460
    },
    {
      "epoch": 0.676056338028169,
      "grad_norm": 3.93747878074646,
      "learning_rate": 3.886930727621707e-05,
      "loss": 2.9532,
      "step": 6480
    },
    {
      "epoch": 0.6781429316640585,
      "grad_norm": 4.30867862701416,
      "learning_rate": 3.883440935264352e-05,
      "loss": 2.9832,
      "step": 6500
    },
    {
      "epoch": 0.6802295252999478,
      "grad_norm": 3.9892969131469727,
      "learning_rate": 3.879951142906997e-05,
      "loss": 2.9702,
      "step": 6520
    },
    {
      "epoch": 0.6823161189358372,
      "grad_norm": 3.8531248569488525,
      "learning_rate": 3.876461350549642e-05,
      "loss": 3.0325,
      "step": 6540
    },
    {
      "epoch": 0.6844027125717267,
      "grad_norm": 3.847982168197632,
      "learning_rate": 3.8729715581922874e-05,
      "loss": 2.9411,
      "step": 6560
    },
    {
      "epoch": 0.6864893062076161,
      "grad_norm": 3.5702383518218994,
      "learning_rate": 3.869481765834933e-05,
      "loss": 2.939,
      "step": 6580
    },
    {
      "epoch": 0.6885758998435054,
      "grad_norm": 3.6264569759368896,
      "learning_rate": 3.8659919734775784e-05,
      "loss": 3.0479,
      "step": 6600
    },
    {
      "epoch": 0.6906624934793949,
      "grad_norm": 4.748064041137695,
      "learning_rate": 3.8625021811202235e-05,
      "loss": 2.9607,
      "step": 6620
    },
    {
      "epoch": 0.6927490871152843,
      "grad_norm": 3.9882781505584717,
      "learning_rate": 3.8590123887628686e-05,
      "loss": 2.9857,
      "step": 6640
    },
    {
      "epoch": 0.6948356807511737,
      "grad_norm": 3.5104541778564453,
      "learning_rate": 3.8555225964055145e-05,
      "loss": 2.8404,
      "step": 6660
    },
    {
      "epoch": 0.6969222743870631,
      "grad_norm": 3.668736457824707,
      "learning_rate": 3.8520328040481596e-05,
      "loss": 2.9309,
      "step": 6680
    },
    {
      "epoch": 0.6990088680229525,
      "grad_norm": 4.193482398986816,
      "learning_rate": 3.848543011690805e-05,
      "loss": 3.0617,
      "step": 6700
    },
    {
      "epoch": 0.701095461658842,
      "grad_norm": 3.807800531387329,
      "learning_rate": 3.84505321933345e-05,
      "loss": 2.9011,
      "step": 6720
    },
    {
      "epoch": 0.7031820552947313,
      "grad_norm": 4.092607498168945,
      "learning_rate": 3.841563426976095e-05,
      "loss": 2.866,
      "step": 6740
    },
    {
      "epoch": 0.7052686489306208,
      "grad_norm": 5.215275287628174,
      "learning_rate": 3.83807363461874e-05,
      "loss": 2.9526,
      "step": 6760
    },
    {
      "epoch": 0.7073552425665102,
      "grad_norm": 3.740527391433716,
      "learning_rate": 3.8345838422613854e-05,
      "loss": 3.0185,
      "step": 6780
    },
    {
      "epoch": 0.7094418362023995,
      "grad_norm": 4.0383124351501465,
      "learning_rate": 3.831094049904031e-05,
      "loss": 2.838,
      "step": 6800
    },
    {
      "epoch": 0.711528429838289,
      "grad_norm": 3.7398324012756348,
      "learning_rate": 3.827604257546676e-05,
      "loss": 2.946,
      "step": 6820
    },
    {
      "epoch": 0.7136150234741784,
      "grad_norm": 3.809539318084717,
      "learning_rate": 3.8241144651893215e-05,
      "loss": 2.979,
      "step": 6840
    },
    {
      "epoch": 0.7157016171100679,
      "grad_norm": 3.7438976764678955,
      "learning_rate": 3.8206246728319666e-05,
      "loss": 2.9546,
      "step": 6860
    },
    {
      "epoch": 0.7177882107459572,
      "grad_norm": 4.592898845672607,
      "learning_rate": 3.817134880474612e-05,
      "loss": 2.9671,
      "step": 6880
    },
    {
      "epoch": 0.7198748043818466,
      "grad_norm": 3.4910027980804443,
      "learning_rate": 3.813645088117257e-05,
      "loss": 2.9418,
      "step": 6900
    },
    {
      "epoch": 0.7219613980177361,
      "grad_norm": 3.627943277359009,
      "learning_rate": 3.810155295759902e-05,
      "loss": 2.8645,
      "step": 6920
    },
    {
      "epoch": 0.7240479916536254,
      "grad_norm": 4.020124912261963,
      "learning_rate": 3.806665503402548e-05,
      "loss": 2.8881,
      "step": 6940
    },
    {
      "epoch": 0.7261345852895149,
      "grad_norm": 3.574249267578125,
      "learning_rate": 3.803175711045193e-05,
      "loss": 2.8874,
      "step": 6960
    },
    {
      "epoch": 0.7282211789254043,
      "grad_norm": 3.43660306930542,
      "learning_rate": 3.799685918687838e-05,
      "loss": 2.9517,
      "step": 6980
    },
    {
      "epoch": 0.7303077725612936,
      "grad_norm": 3.706155300140381,
      "learning_rate": 3.7961961263304833e-05,
      "loss": 2.949,
      "step": 7000
    },
    {
      "epoch": 0.7323943661971831,
      "grad_norm": 4.383492946624756,
      "learning_rate": 3.792706333973129e-05,
      "loss": 3.0555,
      "step": 7020
    },
    {
      "epoch": 0.7344809598330725,
      "grad_norm": 3.914883852005005,
      "learning_rate": 3.7892165416157736e-05,
      "loss": 2.9898,
      "step": 7040
    },
    {
      "epoch": 0.736567553468962,
      "grad_norm": 4.468227863311768,
      "learning_rate": 3.7857267492584195e-05,
      "loss": 3.0504,
      "step": 7060
    },
    {
      "epoch": 0.7386541471048513,
      "grad_norm": 3.956529378890991,
      "learning_rate": 3.7822369569010646e-05,
      "loss": 2.964,
      "step": 7080
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 3.5277786254882812,
      "learning_rate": 3.77874716454371e-05,
      "loss": 2.9863,
      "step": 7100
    },
    {
      "epoch": 0.7428273343766302,
      "grad_norm": 4.329864978790283,
      "learning_rate": 3.775257372186355e-05,
      "loss": 2.9445,
      "step": 7120
    },
    {
      "epoch": 0.7449139280125195,
      "grad_norm": 3.2807469367980957,
      "learning_rate": 3.771767579829e-05,
      "loss": 2.9956,
      "step": 7140
    },
    {
      "epoch": 0.747000521648409,
      "grad_norm": 3.483419179916382,
      "learning_rate": 3.768277787471646e-05,
      "loss": 3.0768,
      "step": 7160
    },
    {
      "epoch": 0.7490871152842984,
      "grad_norm": 4.193737030029297,
      "learning_rate": 3.764787995114291e-05,
      "loss": 2.9392,
      "step": 7180
    },
    {
      "epoch": 0.7511737089201878,
      "grad_norm": 3.9355690479278564,
      "learning_rate": 3.761298202756936e-05,
      "loss": 2.9006,
      "step": 7200
    },
    {
      "epoch": 0.7532603025560772,
      "grad_norm": 3.9579355716705322,
      "learning_rate": 3.757808410399581e-05,
      "loss": 2.8322,
      "step": 7220
    },
    {
      "epoch": 0.7553468961919666,
      "grad_norm": 3.31601881980896,
      "learning_rate": 3.754318618042227e-05,
      "loss": 2.9655,
      "step": 7240
    },
    {
      "epoch": 0.7574334898278561,
      "grad_norm": 3.1202406883239746,
      "learning_rate": 3.7508288256848716e-05,
      "loss": 2.9378,
      "step": 7260
    },
    {
      "epoch": 0.7595200834637454,
      "grad_norm": 3.566075563430786,
      "learning_rate": 3.7473390333275175e-05,
      "loss": 2.9293,
      "step": 7280
    },
    {
      "epoch": 0.7616066770996348,
      "grad_norm": 4.486855506896973,
      "learning_rate": 3.7438492409701626e-05,
      "loss": 3.006,
      "step": 7300
    },
    {
      "epoch": 0.7636932707355243,
      "grad_norm": 3.3320834636688232,
      "learning_rate": 3.740359448612808e-05,
      "loss": 2.9459,
      "step": 7320
    },
    {
      "epoch": 0.7657798643714137,
      "grad_norm": 3.270076036453247,
      "learning_rate": 3.736869656255453e-05,
      "loss": 2.742,
      "step": 7340
    },
    {
      "epoch": 0.7678664580073031,
      "grad_norm": 3.6852970123291016,
      "learning_rate": 3.733379863898098e-05,
      "loss": 2.848,
      "step": 7360
    },
    {
      "epoch": 0.7699530516431925,
      "grad_norm": 4.000364303588867,
      "learning_rate": 3.729890071540744e-05,
      "loss": 2.9241,
      "step": 7380
    },
    {
      "epoch": 0.7720396452790819,
      "grad_norm": 4.4645586013793945,
      "learning_rate": 3.7264002791833883e-05,
      "loss": 2.9566,
      "step": 7400
    },
    {
      "epoch": 0.7741262389149713,
      "grad_norm": 3.962106943130493,
      "learning_rate": 3.722910486826034e-05,
      "loss": 2.8917,
      "step": 7420
    },
    {
      "epoch": 0.7762128325508607,
      "grad_norm": 3.3551273345947266,
      "learning_rate": 3.719420694468679e-05,
      "loss": 2.9519,
      "step": 7440
    },
    {
      "epoch": 0.7782994261867501,
      "grad_norm": 3.1076412200927734,
      "learning_rate": 3.7159309021113245e-05,
      "loss": 2.9518,
      "step": 7460
    },
    {
      "epoch": 0.7803860198226396,
      "grad_norm": 6.181719779968262,
      "learning_rate": 3.7124411097539696e-05,
      "loss": 2.9616,
      "step": 7480
    },
    {
      "epoch": 0.7824726134585289,
      "grad_norm": 3.2874507904052734,
      "learning_rate": 3.708951317396615e-05,
      "loss": 2.9512,
      "step": 7500
    },
    {
      "epoch": 0.7845592070944184,
      "grad_norm": 4.085745334625244,
      "learning_rate": 3.7054615250392606e-05,
      "loss": 3.0345,
      "step": 7520
    },
    {
      "epoch": 0.7866458007303078,
      "grad_norm": 5.328747749328613,
      "learning_rate": 3.701971732681905e-05,
      "loss": 2.8839,
      "step": 7540
    },
    {
      "epoch": 0.7887323943661971,
      "grad_norm": 3.965914726257324,
      "learning_rate": 3.698481940324551e-05,
      "loss": 2.9265,
      "step": 7560
    },
    {
      "epoch": 0.7908189880020866,
      "grad_norm": 4.889834880828857,
      "learning_rate": 3.694992147967196e-05,
      "loss": 2.9317,
      "step": 7580
    },
    {
      "epoch": 0.792905581637976,
      "grad_norm": 3.0184457302093506,
      "learning_rate": 3.691502355609842e-05,
      "loss": 2.9481,
      "step": 7600
    },
    {
      "epoch": 0.7949921752738655,
      "grad_norm": 3.0898971557617188,
      "learning_rate": 3.688012563252486e-05,
      "loss": 2.9604,
      "step": 7620
    },
    {
      "epoch": 0.7970787689097548,
      "grad_norm": 3.5677802562713623,
      "learning_rate": 3.684522770895132e-05,
      "loss": 2.9259,
      "step": 7640
    },
    {
      "epoch": 0.7991653625456442,
      "grad_norm": 3.4166696071624756,
      "learning_rate": 3.681032978537777e-05,
      "loss": 2.9268,
      "step": 7660
    },
    {
      "epoch": 0.8012519561815337,
      "grad_norm": 3.6142072677612305,
      "learning_rate": 3.6775431861804224e-05,
      "loss": 2.9143,
      "step": 7680
    },
    {
      "epoch": 0.803338549817423,
      "grad_norm": 3.229372978210449,
      "learning_rate": 3.6740533938230676e-05,
      "loss": 2.8705,
      "step": 7700
    },
    {
      "epoch": 0.8054251434533125,
      "grad_norm": 5.0168585777282715,
      "learning_rate": 3.670563601465713e-05,
      "loss": 2.9545,
      "step": 7720
    },
    {
      "epoch": 0.8075117370892019,
      "grad_norm": 3.5384490489959717,
      "learning_rate": 3.6670738091083586e-05,
      "loss": 2.923,
      "step": 7740
    },
    {
      "epoch": 0.8095983307250912,
      "grad_norm": 3.965475082397461,
      "learning_rate": 3.663584016751003e-05,
      "loss": 2.9697,
      "step": 7760
    },
    {
      "epoch": 0.8116849243609807,
      "grad_norm": 4.076536655426025,
      "learning_rate": 3.660094224393649e-05,
      "loss": 2.8352,
      "step": 7780
    },
    {
      "epoch": 0.8137715179968701,
      "grad_norm": 3.375971794128418,
      "learning_rate": 3.656604432036294e-05,
      "loss": 2.8716,
      "step": 7800
    },
    {
      "epoch": 0.8158581116327596,
      "grad_norm": 3.1463472843170166,
      "learning_rate": 3.65311463967894e-05,
      "loss": 2.8038,
      "step": 7820
    },
    {
      "epoch": 0.8179447052686489,
      "grad_norm": 3.9101316928863525,
      "learning_rate": 3.649624847321584e-05,
      "loss": 2.9315,
      "step": 7840
    },
    {
      "epoch": 0.8200312989045383,
      "grad_norm": 4.967012882232666,
      "learning_rate": 3.64613505496423e-05,
      "loss": 2.9512,
      "step": 7860
    },
    {
      "epoch": 0.8221178925404278,
      "grad_norm": 3.7744545936584473,
      "learning_rate": 3.642645262606875e-05,
      "loss": 2.8283,
      "step": 7880
    },
    {
      "epoch": 0.8242044861763171,
      "grad_norm": 4.260847091674805,
      "learning_rate": 3.63915547024952e-05,
      "loss": 2.9555,
      "step": 7900
    },
    {
      "epoch": 0.8262910798122066,
      "grad_norm": 4.393072128295898,
      "learning_rate": 3.6356656778921656e-05,
      "loss": 3.0496,
      "step": 7920
    },
    {
      "epoch": 0.828377673448096,
      "grad_norm": 4.188318252563477,
      "learning_rate": 3.632175885534811e-05,
      "loss": 2.8911,
      "step": 7940
    },
    {
      "epoch": 0.8304642670839854,
      "grad_norm": 4.7077155113220215,
      "learning_rate": 3.6286860931774566e-05,
      "loss": 2.9043,
      "step": 7960
    },
    {
      "epoch": 0.8325508607198748,
      "grad_norm": 2.660100221633911,
      "learning_rate": 3.625196300820101e-05,
      "loss": 2.8789,
      "step": 7980
    },
    {
      "epoch": 0.8346374543557642,
      "grad_norm": 3.89865779876709,
      "learning_rate": 3.621706508462747e-05,
      "loss": 3.0254,
      "step": 8000
    },
    {
      "epoch": 0.8367240479916537,
      "grad_norm": 3.492547035217285,
      "learning_rate": 3.618216716105392e-05,
      "loss": 2.9522,
      "step": 8020
    },
    {
      "epoch": 0.838810641627543,
      "grad_norm": 3.6862423419952393,
      "learning_rate": 3.614726923748037e-05,
      "loss": 2.8971,
      "step": 8040
    },
    {
      "epoch": 0.8408972352634324,
      "grad_norm": 3.3728220462799072,
      "learning_rate": 3.611237131390682e-05,
      "loss": 2.8512,
      "step": 8060
    },
    {
      "epoch": 0.8429838288993219,
      "grad_norm": 6.08913516998291,
      "learning_rate": 3.6077473390333274e-05,
      "loss": 2.7706,
      "step": 8080
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 4.413311958312988,
      "learning_rate": 3.604257546675973e-05,
      "loss": 2.9992,
      "step": 8100
    },
    {
      "epoch": 0.8471570161711007,
      "grad_norm": 4.631320476531982,
      "learning_rate": 3.600767754318618e-05,
      "loss": 2.7955,
      "step": 8120
    },
    {
      "epoch": 0.8492436098069901,
      "grad_norm": 3.7051455974578857,
      "learning_rate": 3.5972779619612636e-05,
      "loss": 2.8947,
      "step": 8140
    },
    {
      "epoch": 0.8513302034428795,
      "grad_norm": 4.155196666717529,
      "learning_rate": 3.593788169603909e-05,
      "loss": 3.0522,
      "step": 8160
    },
    {
      "epoch": 0.8534167970787689,
      "grad_norm": 3.812983751296997,
      "learning_rate": 3.5902983772465545e-05,
      "loss": 2.8786,
      "step": 8180
    },
    {
      "epoch": 0.8555033907146583,
      "grad_norm": 3.465038776397705,
      "learning_rate": 3.586808584889199e-05,
      "loss": 2.8749,
      "step": 8200
    },
    {
      "epoch": 0.8575899843505478,
      "grad_norm": 3.4187161922454834,
      "learning_rate": 3.583318792531845e-05,
      "loss": 2.9289,
      "step": 8220
    },
    {
      "epoch": 0.8596765779864372,
      "grad_norm": 4.185065269470215,
      "learning_rate": 3.57982900017449e-05,
      "loss": 2.8583,
      "step": 8240
    },
    {
      "epoch": 0.8617631716223265,
      "grad_norm": 3.514477252960205,
      "learning_rate": 3.576339207817135e-05,
      "loss": 2.9327,
      "step": 8260
    },
    {
      "epoch": 0.863849765258216,
      "grad_norm": 3.611069679260254,
      "learning_rate": 3.57284941545978e-05,
      "loss": 2.8519,
      "step": 8280
    },
    {
      "epoch": 0.8659363588941054,
      "grad_norm": 4.109785079956055,
      "learning_rate": 3.5693596231024254e-05,
      "loss": 2.7565,
      "step": 8300
    },
    {
      "epoch": 0.8680229525299947,
      "grad_norm": 3.9188849925994873,
      "learning_rate": 3.565869830745071e-05,
      "loss": 2.8918,
      "step": 8320
    },
    {
      "epoch": 0.8701095461658842,
      "grad_norm": 4.1152424812316895,
      "learning_rate": 3.562380038387716e-05,
      "loss": 2.8957,
      "step": 8340
    },
    {
      "epoch": 0.8721961398017736,
      "grad_norm": 3.859478235244751,
      "learning_rate": 3.5588902460303615e-05,
      "loss": 3.0652,
      "step": 8360
    },
    {
      "epoch": 0.874282733437663,
      "grad_norm": 3.6629436016082764,
      "learning_rate": 3.555400453673007e-05,
      "loss": 2.847,
      "step": 8380
    },
    {
      "epoch": 0.8763693270735524,
      "grad_norm": 4.441497802734375,
      "learning_rate": 3.551910661315652e-05,
      "loss": 2.8128,
      "step": 8400
    },
    {
      "epoch": 0.8784559207094418,
      "grad_norm": 4.676402568817139,
      "learning_rate": 3.548420868958297e-05,
      "loss": 2.9158,
      "step": 8420
    },
    {
      "epoch": 0.8805425143453313,
      "grad_norm": 3.3343613147735596,
      "learning_rate": 3.544931076600943e-05,
      "loss": 2.9837,
      "step": 8440
    },
    {
      "epoch": 0.8826291079812206,
      "grad_norm": 3.5341320037841797,
      "learning_rate": 3.541441284243588e-05,
      "loss": 2.8163,
      "step": 8460
    },
    {
      "epoch": 0.8847157016171101,
      "grad_norm": 3.2601237297058105,
      "learning_rate": 3.5379514918862324e-05,
      "loss": 2.9046,
      "step": 8480
    },
    {
      "epoch": 0.8868022952529995,
      "grad_norm": 3.7824511528015137,
      "learning_rate": 3.534461699528878e-05,
      "loss": 2.9331,
      "step": 8500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 3.9469211101531982,
      "learning_rate": 3.5309719071715234e-05,
      "loss": 2.9907,
      "step": 8520
    },
    {
      "epoch": 0.8909754825247783,
      "grad_norm": 3.863117218017578,
      "learning_rate": 3.5274821148141686e-05,
      "loss": 3.0186,
      "step": 8540
    },
    {
      "epoch": 0.8930620761606677,
      "grad_norm": 3.7609057426452637,
      "learning_rate": 3.523992322456814e-05,
      "loss": 2.9516,
      "step": 8560
    },
    {
      "epoch": 0.8951486697965572,
      "grad_norm": 3.2855751514434814,
      "learning_rate": 3.5205025300994595e-05,
      "loss": 2.8729,
      "step": 8580
    },
    {
      "epoch": 0.8972352634324465,
      "grad_norm": 3.849299430847168,
      "learning_rate": 3.517012737742105e-05,
      "loss": 2.9364,
      "step": 8600
    },
    {
      "epoch": 0.8993218570683359,
      "grad_norm": 3.484638214111328,
      "learning_rate": 3.51352294538475e-05,
      "loss": 3.0254,
      "step": 8620
    },
    {
      "epoch": 0.9014084507042254,
      "grad_norm": 3.829460382461548,
      "learning_rate": 3.510033153027395e-05,
      "loss": 2.912,
      "step": 8640
    },
    {
      "epoch": 0.9034950443401147,
      "grad_norm": 3.9363081455230713,
      "learning_rate": 3.50654336067004e-05,
      "loss": 2.9033,
      "step": 8660
    },
    {
      "epoch": 0.9055816379760042,
      "grad_norm": 3.1808629035949707,
      "learning_rate": 3.503053568312686e-05,
      "loss": 2.8381,
      "step": 8680
    },
    {
      "epoch": 0.9076682316118936,
      "grad_norm": 3.9095420837402344,
      "learning_rate": 3.4995637759553304e-05,
      "loss": 2.9685,
      "step": 8700
    },
    {
      "epoch": 0.909754825247783,
      "grad_norm": 3.418480396270752,
      "learning_rate": 3.496073983597976e-05,
      "loss": 2.8639,
      "step": 8720
    },
    {
      "epoch": 0.9118414188836724,
      "grad_norm": 4.826135635375977,
      "learning_rate": 3.4925841912406214e-05,
      "loss": 2.8959,
      "step": 8740
    },
    {
      "epoch": 0.9139280125195618,
      "grad_norm": 3.279996395111084,
      "learning_rate": 3.4890943988832665e-05,
      "loss": 2.846,
      "step": 8760
    },
    {
      "epoch": 0.9160146061554513,
      "grad_norm": 3.782012939453125,
      "learning_rate": 3.485604606525912e-05,
      "loss": 2.9083,
      "step": 8780
    },
    {
      "epoch": 0.9181011997913406,
      "grad_norm": 3.870023727416992,
      "learning_rate": 3.4821148141685575e-05,
      "loss": 2.927,
      "step": 8800
    },
    {
      "epoch": 0.92018779342723,
      "grad_norm": 5.309847354888916,
      "learning_rate": 3.478625021811203e-05,
      "loss": 2.8833,
      "step": 8820
    },
    {
      "epoch": 0.9222743870631195,
      "grad_norm": 3.9117822647094727,
      "learning_rate": 3.475135229453848e-05,
      "loss": 2.8693,
      "step": 8840
    },
    {
      "epoch": 0.9243609806990088,
      "grad_norm": 4.546703338623047,
      "learning_rate": 3.471645437096493e-05,
      "loss": 2.9364,
      "step": 8860
    },
    {
      "epoch": 0.9264475743348983,
      "grad_norm": 3.375088691711426,
      "learning_rate": 3.468155644739138e-05,
      "loss": 2.8628,
      "step": 8880
    },
    {
      "epoch": 0.9285341679707877,
      "grad_norm": 3.3022053241729736,
      "learning_rate": 3.464665852381783e-05,
      "loss": 2.933,
      "step": 8900
    },
    {
      "epoch": 0.9306207616066771,
      "grad_norm": 3.6661524772644043,
      "learning_rate": 3.4611760600244284e-05,
      "loss": 2.8539,
      "step": 8920
    },
    {
      "epoch": 0.9327073552425665,
      "grad_norm": 3.8049263954162598,
      "learning_rate": 3.457686267667074e-05,
      "loss": 2.8532,
      "step": 8940
    },
    {
      "epoch": 0.9347939488784559,
      "grad_norm": 3.4946699142456055,
      "learning_rate": 3.4541964753097194e-05,
      "loss": 2.9907,
      "step": 8960
    },
    {
      "epoch": 0.9368805425143454,
      "grad_norm": 3.367466926574707,
      "learning_rate": 3.4507066829523645e-05,
      "loss": 2.7853,
      "step": 8980
    },
    {
      "epoch": 0.9389671361502347,
      "grad_norm": 5.244054794311523,
      "learning_rate": 3.44721689059501e-05,
      "loss": 3.0029,
      "step": 9000
    },
    {
      "epoch": 0.9410537297861241,
      "grad_norm": 2.7614946365356445,
      "learning_rate": 3.443727098237655e-05,
      "loss": 2.8877,
      "step": 9020
    },
    {
      "epoch": 0.9431403234220136,
      "grad_norm": 4.88873815536499,
      "learning_rate": 3.4402373058803e-05,
      "loss": 2.8809,
      "step": 9040
    },
    {
      "epoch": 0.945226917057903,
      "grad_norm": 3.2019824981689453,
      "learning_rate": 3.436747513522945e-05,
      "loss": 2.8957,
      "step": 9060
    },
    {
      "epoch": 0.9473135106937924,
      "grad_norm": 4.308298110961914,
      "learning_rate": 3.433257721165591e-05,
      "loss": 2.887,
      "step": 9080
    },
    {
      "epoch": 0.9494001043296818,
      "grad_norm": 3.3344337940216064,
      "learning_rate": 3.429767928808236e-05,
      "loss": 2.963,
      "step": 9100
    },
    {
      "epoch": 0.9514866979655712,
      "grad_norm": 3.7773489952087402,
      "learning_rate": 3.426278136450881e-05,
      "loss": 2.8442,
      "step": 9120
    },
    {
      "epoch": 0.9535732916014606,
      "grad_norm": 3.4455089569091797,
      "learning_rate": 3.4227883440935264e-05,
      "loss": 2.9186,
      "step": 9140
    },
    {
      "epoch": 0.95565988523735,
      "grad_norm": 3.172262668609619,
      "learning_rate": 3.419298551736172e-05,
      "loss": 2.9417,
      "step": 9160
    },
    {
      "epoch": 0.9577464788732394,
      "grad_norm": 3.425028085708618,
      "learning_rate": 3.4158087593788174e-05,
      "loss": 2.8796,
      "step": 9180
    },
    {
      "epoch": 0.9598330725091289,
      "grad_norm": 3.0820236206054688,
      "learning_rate": 3.4123189670214625e-05,
      "loss": 2.876,
      "step": 9200
    },
    {
      "epoch": 0.9619196661450182,
      "grad_norm": 3.678516149520874,
      "learning_rate": 3.4088291746641077e-05,
      "loss": 2.9111,
      "step": 9220
    },
    {
      "epoch": 0.9640062597809077,
      "grad_norm": 4.131619930267334,
      "learning_rate": 3.405339382306753e-05,
      "loss": 2.9696,
      "step": 9240
    },
    {
      "epoch": 0.9660928534167971,
      "grad_norm": 3.2540228366851807,
      "learning_rate": 3.401849589949398e-05,
      "loss": 2.9229,
      "step": 9260
    },
    {
      "epoch": 0.9681794470526864,
      "grad_norm": 4.188549995422363,
      "learning_rate": 3.398359797592043e-05,
      "loss": 2.922,
      "step": 9280
    },
    {
      "epoch": 0.9702660406885759,
      "grad_norm": 3.510122776031494,
      "learning_rate": 3.394870005234689e-05,
      "loss": 2.8355,
      "step": 9300
    },
    {
      "epoch": 0.9723526343244653,
      "grad_norm": 2.978097677230835,
      "learning_rate": 3.391380212877334e-05,
      "loss": 2.8005,
      "step": 9320
    },
    {
      "epoch": 0.9744392279603548,
      "grad_norm": 3.8303353786468506,
      "learning_rate": 3.387890420519979e-05,
      "loss": 2.9023,
      "step": 9340
    },
    {
      "epoch": 0.9765258215962441,
      "grad_norm": 3.758751153945923,
      "learning_rate": 3.3844006281626244e-05,
      "loss": 2.9199,
      "step": 9360
    },
    {
      "epoch": 0.9786124152321335,
      "grad_norm": 4.744060516357422,
      "learning_rate": 3.38091083580527e-05,
      "loss": 2.8415,
      "step": 9380
    },
    {
      "epoch": 0.980699008868023,
      "grad_norm": 3.7515437602996826,
      "learning_rate": 3.377421043447915e-05,
      "loss": 2.9303,
      "step": 9400
    },
    {
      "epoch": 0.9827856025039123,
      "grad_norm": 4.879307270050049,
      "learning_rate": 3.3739312510905605e-05,
      "loss": 2.8504,
      "step": 9420
    },
    {
      "epoch": 0.9848721961398018,
      "grad_norm": 3.8156509399414062,
      "learning_rate": 3.3704414587332056e-05,
      "loss": 2.9799,
      "step": 9440
    },
    {
      "epoch": 0.9869587897756912,
      "grad_norm": 3.5587527751922607,
      "learning_rate": 3.366951666375851e-05,
      "loss": 2.9639,
      "step": 9460
    },
    {
      "epoch": 0.9890453834115805,
      "grad_norm": 3.75887131690979,
      "learning_rate": 3.363461874018496e-05,
      "loss": 2.8805,
      "step": 9480
    },
    {
      "epoch": 0.99113197704747,
      "grad_norm": 4.102512359619141,
      "learning_rate": 3.359972081661141e-05,
      "loss": 2.9161,
      "step": 9500
    },
    {
      "epoch": 0.9932185706833594,
      "grad_norm": 3.341273784637451,
      "learning_rate": 3.356482289303787e-05,
      "loss": 2.8776,
      "step": 9520
    },
    {
      "epoch": 0.9953051643192489,
      "grad_norm": 3.854715585708618,
      "learning_rate": 3.3529924969464314e-05,
      "loss": 2.862,
      "step": 9540
    },
    {
      "epoch": 0.9973917579551382,
      "grad_norm": 3.499377489089966,
      "learning_rate": 3.349502704589077e-05,
      "loss": 2.8667,
      "step": 9560
    },
    {
      "epoch": 0.9994783515910276,
      "grad_norm": 4.344675540924072,
      "learning_rate": 3.3460129122317224e-05,
      "loss": 2.973,
      "step": 9580
    },
    {
      "epoch": 1.001564945226917,
      "grad_norm": 3.846278667449951,
      "learning_rate": 3.3425231198743675e-05,
      "loss": 2.8362,
      "step": 9600
    },
    {
      "epoch": 1.0036515388628064,
      "grad_norm": 4.1056623458862305,
      "learning_rate": 3.3390333275170127e-05,
      "loss": 2.6482,
      "step": 9620
    },
    {
      "epoch": 1.0057381324986958,
      "grad_norm": 4.201272487640381,
      "learning_rate": 3.335543535159658e-05,
      "loss": 2.7131,
      "step": 9640
    },
    {
      "epoch": 1.0078247261345852,
      "grad_norm": 3.909557580947876,
      "learning_rate": 3.3320537428023036e-05,
      "loss": 2.7303,
      "step": 9660
    },
    {
      "epoch": 1.0099113197704748,
      "grad_norm": 3.6446409225463867,
      "learning_rate": 3.328563950444949e-05,
      "loss": 2.7125,
      "step": 9680
    },
    {
      "epoch": 1.0119979134063641,
      "grad_norm": 5.613490104675293,
      "learning_rate": 3.325074158087594e-05,
      "loss": 2.7764,
      "step": 9700
    },
    {
      "epoch": 1.0140845070422535,
      "grad_norm": 4.837247371673584,
      "learning_rate": 3.321584365730239e-05,
      "loss": 2.6378,
      "step": 9720
    },
    {
      "epoch": 1.0161711006781429,
      "grad_norm": 4.61496114730835,
      "learning_rate": 3.318094573372885e-05,
      "loss": 2.6025,
      "step": 9740
    },
    {
      "epoch": 1.0182576943140322,
      "grad_norm": 4.9781880378723145,
      "learning_rate": 3.3146047810155294e-05,
      "loss": 2.5893,
      "step": 9760
    },
    {
      "epoch": 1.0203442879499218,
      "grad_norm": 4.075106143951416,
      "learning_rate": 3.311114988658175e-05,
      "loss": 2.6474,
      "step": 9780
    },
    {
      "epoch": 1.0224308815858112,
      "grad_norm": 4.0398268699646,
      "learning_rate": 3.3076251963008203e-05,
      "loss": 2.6201,
      "step": 9800
    },
    {
      "epoch": 1.0245174752217006,
      "grad_norm": 4.625763416290283,
      "learning_rate": 3.3041354039434655e-05,
      "loss": 2.6294,
      "step": 9820
    },
    {
      "epoch": 1.02660406885759,
      "grad_norm": 3.6352286338806152,
      "learning_rate": 3.3006456115861106e-05,
      "loss": 2.6778,
      "step": 9840
    },
    {
      "epoch": 1.0286906624934793,
      "grad_norm": 4.191526889801025,
      "learning_rate": 3.297155819228756e-05,
      "loss": 2.7722,
      "step": 9860
    },
    {
      "epoch": 1.0307772561293689,
      "grad_norm": 3.8096814155578613,
      "learning_rate": 3.2936660268714016e-05,
      "loss": 2.6732,
      "step": 9880
    },
    {
      "epoch": 1.0328638497652582,
      "grad_norm": 3.405536651611328,
      "learning_rate": 3.290176234514046e-05,
      "loss": 2.7112,
      "step": 9900
    },
    {
      "epoch": 1.0349504434011476,
      "grad_norm": 3.723200559616089,
      "learning_rate": 3.286686442156692e-05,
      "loss": 2.7557,
      "step": 9920
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 3.2797420024871826,
      "learning_rate": 3.283196649799337e-05,
      "loss": 2.671,
      "step": 9940
    },
    {
      "epoch": 1.0391236306729263,
      "grad_norm": 4.10203218460083,
      "learning_rate": 3.279706857441983e-05,
      "loss": 2.5935,
      "step": 9960
    },
    {
      "epoch": 1.041210224308816,
      "grad_norm": 3.525493621826172,
      "learning_rate": 3.2762170650846274e-05,
      "loss": 2.6397,
      "step": 9980
    },
    {
      "epoch": 1.0432968179447053,
      "grad_norm": 4.867917537689209,
      "learning_rate": 3.272727272727273e-05,
      "loss": 2.6914,
      "step": 10000
    },
    {
      "epoch": 1.0453834115805947,
      "grad_norm": 4.560960292816162,
      "learning_rate": 3.269237480369918e-05,
      "loss": 2.8013,
      "step": 10020
    },
    {
      "epoch": 1.047470005216484,
      "grad_norm": 3.134465456008911,
      "learning_rate": 3.2657476880125635e-05,
      "loss": 2.7104,
      "step": 10040
    },
    {
      "epoch": 1.0495565988523734,
      "grad_norm": 4.012558937072754,
      "learning_rate": 3.2622578956552086e-05,
      "loss": 2.57,
      "step": 10060
    },
    {
      "epoch": 1.051643192488263,
      "grad_norm": 3.658604145050049,
      "learning_rate": 3.258768103297854e-05,
      "loss": 2.7603,
      "step": 10080
    },
    {
      "epoch": 1.0537297861241524,
      "grad_norm": 3.3090927600860596,
      "learning_rate": 3.2552783109404996e-05,
      "loss": 2.6524,
      "step": 10100
    },
    {
      "epoch": 1.0558163797600417,
      "grad_norm": 4.009222030639648,
      "learning_rate": 3.251788518583144e-05,
      "loss": 2.6892,
      "step": 10120
    },
    {
      "epoch": 1.057902973395931,
      "grad_norm": 3.7321937084198,
      "learning_rate": 3.24829872622579e-05,
      "loss": 2.679,
      "step": 10140
    },
    {
      "epoch": 1.0599895670318205,
      "grad_norm": 4.200656890869141,
      "learning_rate": 3.244808933868435e-05,
      "loss": 2.7007,
      "step": 10160
    },
    {
      "epoch": 1.06207616066771,
      "grad_norm": 3.38657546043396,
      "learning_rate": 3.24131914151108e-05,
      "loss": 2.8117,
      "step": 10180
    },
    {
      "epoch": 1.0641627543035994,
      "grad_norm": 4.267452239990234,
      "learning_rate": 3.237829349153725e-05,
      "loss": 2.6838,
      "step": 10200
    },
    {
      "epoch": 1.0662493479394888,
      "grad_norm": 3.860189199447632,
      "learning_rate": 3.2343395567963705e-05,
      "loss": 2.7187,
      "step": 10220
    },
    {
      "epoch": 1.0683359415753781,
      "grad_norm": 4.183380603790283,
      "learning_rate": 3.230849764439016e-05,
      "loss": 2.7929,
      "step": 10240
    },
    {
      "epoch": 1.0704225352112675,
      "grad_norm": 4.038989067077637,
      "learning_rate": 3.227359972081661e-05,
      "loss": 2.7347,
      "step": 10260
    },
    {
      "epoch": 1.072509128847157,
      "grad_norm": 4.093756198883057,
      "learning_rate": 3.2238701797243066e-05,
      "loss": 2.6986,
      "step": 10280
    },
    {
      "epoch": 1.0745957224830465,
      "grad_norm": 4.5770487785339355,
      "learning_rate": 3.220380387366952e-05,
      "loss": 2.7612,
      "step": 10300
    },
    {
      "epoch": 1.0766823161189358,
      "grad_norm": 3.597172498703003,
      "learning_rate": 3.2168905950095976e-05,
      "loss": 2.7736,
      "step": 10320
    },
    {
      "epoch": 1.0787689097548252,
      "grad_norm": 4.29976224899292,
      "learning_rate": 3.213400802652242e-05,
      "loss": 2.7696,
      "step": 10340
    },
    {
      "epoch": 1.0808555033907146,
      "grad_norm": 3.4976351261138916,
      "learning_rate": 3.209911010294888e-05,
      "loss": 2.7678,
      "step": 10360
    },
    {
      "epoch": 1.0829420970266042,
      "grad_norm": 3.4992434978485107,
      "learning_rate": 3.206421217937533e-05,
      "loss": 2.7467,
      "step": 10380
    },
    {
      "epoch": 1.0850286906624935,
      "grad_norm": 3.284327745437622,
      "learning_rate": 3.202931425580178e-05,
      "loss": 2.7481,
      "step": 10400
    },
    {
      "epoch": 1.087115284298383,
      "grad_norm": 4.546664714813232,
      "learning_rate": 3.199441633222823e-05,
      "loss": 2.7428,
      "step": 10420
    },
    {
      "epoch": 1.0892018779342723,
      "grad_norm": 3.899777889251709,
      "learning_rate": 3.1959518408654685e-05,
      "loss": 2.6619,
      "step": 10440
    },
    {
      "epoch": 1.0912884715701616,
      "grad_norm": 4.013366222381592,
      "learning_rate": 3.192462048508114e-05,
      "loss": 2.631,
      "step": 10460
    },
    {
      "epoch": 1.0933750652060512,
      "grad_norm": 6.378779888153076,
      "learning_rate": 3.188972256150759e-05,
      "loss": 2.6962,
      "step": 10480
    },
    {
      "epoch": 1.0954616588419406,
      "grad_norm": 3.260874032974243,
      "learning_rate": 3.1854824637934046e-05,
      "loss": 2.6304,
      "step": 10500
    },
    {
      "epoch": 1.09754825247783,
      "grad_norm": 3.988076686859131,
      "learning_rate": 3.18199267143605e-05,
      "loss": 2.7247,
      "step": 10520
    },
    {
      "epoch": 1.0996348461137193,
      "grad_norm": 4.936222553253174,
      "learning_rate": 3.178502879078695e-05,
      "loss": 2.7005,
      "step": 10540
    },
    {
      "epoch": 1.1017214397496087,
      "grad_norm": 4.173657417297363,
      "learning_rate": 3.17501308672134e-05,
      "loss": 2.7414,
      "step": 10560
    },
    {
      "epoch": 1.1038080333854983,
      "grad_norm": 3.5288896560668945,
      "learning_rate": 3.171523294363986e-05,
      "loss": 2.6039,
      "step": 10580
    },
    {
      "epoch": 1.1058946270213876,
      "grad_norm": 3.7278387546539307,
      "learning_rate": 3.168033502006631e-05,
      "loss": 2.6771,
      "step": 10600
    },
    {
      "epoch": 1.107981220657277,
      "grad_norm": 3.8993031978607178,
      "learning_rate": 3.164543709649276e-05,
      "loss": 2.6937,
      "step": 10620
    },
    {
      "epoch": 1.1100678142931664,
      "grad_norm": 3.943596363067627,
      "learning_rate": 3.161053917291921e-05,
      "loss": 2.6398,
      "step": 10640
    },
    {
      "epoch": 1.1121544079290557,
      "grad_norm": 4.348452568054199,
      "learning_rate": 3.1575641249345665e-05,
      "loss": 2.7674,
      "step": 10660
    },
    {
      "epoch": 1.1142410015649453,
      "grad_norm": 3.5500426292419434,
      "learning_rate": 3.154074332577212e-05,
      "loss": 2.6672,
      "step": 10680
    },
    {
      "epoch": 1.1163275952008347,
      "grad_norm": 4.195494651794434,
      "learning_rate": 3.150584540219857e-05,
      "loss": 2.6494,
      "step": 10700
    },
    {
      "epoch": 1.118414188836724,
      "grad_norm": 3.971268653869629,
      "learning_rate": 3.1470947478625026e-05,
      "loss": 2.7337,
      "step": 10720
    },
    {
      "epoch": 1.1205007824726134,
      "grad_norm": 3.5216574668884277,
      "learning_rate": 3.143604955505148e-05,
      "loss": 2.7182,
      "step": 10740
    },
    {
      "epoch": 1.1225873761085028,
      "grad_norm": 3.9079062938690186,
      "learning_rate": 3.140115163147793e-05,
      "loss": 2.7387,
      "step": 10760
    },
    {
      "epoch": 1.1246739697443924,
      "grad_norm": 3.922990083694458,
      "learning_rate": 3.136625370790438e-05,
      "loss": 2.7357,
      "step": 10780
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 3.6436848640441895,
      "learning_rate": 3.133135578433083e-05,
      "loss": 2.6517,
      "step": 10800
    },
    {
      "epoch": 1.128847157016171,
      "grad_norm": 4.373398303985596,
      "learning_rate": 3.129645786075729e-05,
      "loss": 2.7836,
      "step": 10820
    },
    {
      "epoch": 1.1309337506520605,
      "grad_norm": 3.8419229984283447,
      "learning_rate": 3.1261559937183735e-05,
      "loss": 2.6706,
      "step": 10840
    },
    {
      "epoch": 1.1330203442879498,
      "grad_norm": 4.713501453399658,
      "learning_rate": 3.122666201361019e-05,
      "loss": 2.7031,
      "step": 10860
    },
    {
      "epoch": 1.1351069379238394,
      "grad_norm": 4.077991485595703,
      "learning_rate": 3.1191764090036644e-05,
      "loss": 2.7842,
      "step": 10880
    },
    {
      "epoch": 1.1371935315597288,
      "grad_norm": 3.6738626956939697,
      "learning_rate": 3.1156866166463096e-05,
      "loss": 2.6131,
      "step": 10900
    },
    {
      "epoch": 1.1392801251956182,
      "grad_norm": 4.115248203277588,
      "learning_rate": 3.112196824288955e-05,
      "loss": 2.7714,
      "step": 10920
    },
    {
      "epoch": 1.1413667188315075,
      "grad_norm": 4.643983840942383,
      "learning_rate": 3.1087070319316006e-05,
      "loss": 2.7202,
      "step": 10940
    },
    {
      "epoch": 1.143453312467397,
      "grad_norm": 3.6622314453125,
      "learning_rate": 3.105217239574246e-05,
      "loss": 2.5885,
      "step": 10960
    },
    {
      "epoch": 1.1455399061032865,
      "grad_norm": 4.376994609832764,
      "learning_rate": 3.101727447216891e-05,
      "loss": 2.6017,
      "step": 10980
    },
    {
      "epoch": 1.1476264997391759,
      "grad_norm": 3.6719202995300293,
      "learning_rate": 3.098237654859536e-05,
      "loss": 2.6813,
      "step": 11000
    },
    {
      "epoch": 1.1497130933750652,
      "grad_norm": 3.723626136779785,
      "learning_rate": 3.094747862502181e-05,
      "loss": 2.7521,
      "step": 11020
    },
    {
      "epoch": 1.1517996870109546,
      "grad_norm": 4.542405605316162,
      "learning_rate": 3.091258070144826e-05,
      "loss": 2.6845,
      "step": 11040
    },
    {
      "epoch": 1.153886280646844,
      "grad_norm": 2.973999500274658,
      "learning_rate": 3.0877682777874715e-05,
      "loss": 2.6308,
      "step": 11060
    },
    {
      "epoch": 1.1559728742827335,
      "grad_norm": 3.2834770679473877,
      "learning_rate": 3.084278485430117e-05,
      "loss": 2.723,
      "step": 11080
    },
    {
      "epoch": 1.158059467918623,
      "grad_norm": 3.749089241027832,
      "learning_rate": 3.0807886930727624e-05,
      "loss": 2.7009,
      "step": 11100
    },
    {
      "epoch": 1.1601460615545123,
      "grad_norm": 4.589754104614258,
      "learning_rate": 3.0772989007154076e-05,
      "loss": 2.7292,
      "step": 11120
    },
    {
      "epoch": 1.1622326551904016,
      "grad_norm": 3.720353126525879,
      "learning_rate": 3.073809108358053e-05,
      "loss": 2.6735,
      "step": 11140
    },
    {
      "epoch": 1.164319248826291,
      "grad_norm": 4.371529579162598,
      "learning_rate": 3.0703193160006985e-05,
      "loss": 2.6746,
      "step": 11160
    },
    {
      "epoch": 1.1664058424621806,
      "grad_norm": 4.757245063781738,
      "learning_rate": 3.066829523643344e-05,
      "loss": 2.7056,
      "step": 11180
    },
    {
      "epoch": 1.16849243609807,
      "grad_norm": 4.099810600280762,
      "learning_rate": 3.063339731285988e-05,
      "loss": 2.6635,
      "step": 11200
    },
    {
      "epoch": 1.1705790297339593,
      "grad_norm": 4.122176170349121,
      "learning_rate": 3.059849938928634e-05,
      "loss": 2.6092,
      "step": 11220
    },
    {
      "epoch": 1.1726656233698487,
      "grad_norm": 3.5091686248779297,
      "learning_rate": 3.056360146571279e-05,
      "loss": 2.7258,
      "step": 11240
    },
    {
      "epoch": 1.174752217005738,
      "grad_norm": 3.7068893909454346,
      "learning_rate": 3.052870354213924e-05,
      "loss": 2.7404,
      "step": 11260
    },
    {
      "epoch": 1.1768388106416277,
      "grad_norm": 3.709368944168091,
      "learning_rate": 3.0493805618565698e-05,
      "loss": 2.6475,
      "step": 11280
    },
    {
      "epoch": 1.178925404277517,
      "grad_norm": 3.6754539012908936,
      "learning_rate": 3.045890769499215e-05,
      "loss": 2.6607,
      "step": 11300
    },
    {
      "epoch": 1.1810119979134064,
      "grad_norm": 4.429028034210205,
      "learning_rate": 3.0424009771418604e-05,
      "loss": 2.7556,
      "step": 11320
    },
    {
      "epoch": 1.1830985915492958,
      "grad_norm": 3.6540143489837646,
      "learning_rate": 3.0389111847845052e-05,
      "loss": 2.6401,
      "step": 11340
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 4.493878364562988,
      "learning_rate": 3.0354213924271507e-05,
      "loss": 2.7954,
      "step": 11360
    },
    {
      "epoch": 1.1872717788210747,
      "grad_norm": 4.120182514190674,
      "learning_rate": 3.0319316000697962e-05,
      "loss": 2.7247,
      "step": 11380
    },
    {
      "epoch": 1.189358372456964,
      "grad_norm": 3.4787747859954834,
      "learning_rate": 3.028441807712441e-05,
      "loss": 2.7162,
      "step": 11400
    },
    {
      "epoch": 1.1914449660928534,
      "grad_norm": 3.5961382389068604,
      "learning_rate": 3.0249520153550865e-05,
      "loss": 2.744,
      "step": 11420
    },
    {
      "epoch": 1.1935315597287428,
      "grad_norm": 4.172773838043213,
      "learning_rate": 3.021462222997732e-05,
      "loss": 2.7482,
      "step": 11440
    },
    {
      "epoch": 1.1956181533646322,
      "grad_norm": 4.202794075012207,
      "learning_rate": 3.017972430640377e-05,
      "loss": 2.586,
      "step": 11460
    },
    {
      "epoch": 1.1977047470005218,
      "grad_norm": 3.6240837574005127,
      "learning_rate": 3.0144826382830223e-05,
      "loss": 2.7082,
      "step": 11480
    },
    {
      "epoch": 1.1997913406364111,
      "grad_norm": 4.866048336029053,
      "learning_rate": 3.0109928459256674e-05,
      "loss": 2.7434,
      "step": 11500
    },
    {
      "epoch": 1.2018779342723005,
      "grad_norm": 3.2117791175842285,
      "learning_rate": 3.007503053568313e-05,
      "loss": 2.6936,
      "step": 11520
    },
    {
      "epoch": 1.2039645279081899,
      "grad_norm": 3.839735746383667,
      "learning_rate": 3.0040132612109577e-05,
      "loss": 2.6051,
      "step": 11540
    },
    {
      "epoch": 1.2060511215440792,
      "grad_norm": 4.619904518127441,
      "learning_rate": 3.0005234688536032e-05,
      "loss": 2.745,
      "step": 11560
    },
    {
      "epoch": 1.2081377151799688,
      "grad_norm": 4.590149402618408,
      "learning_rate": 2.9970336764962487e-05,
      "loss": 2.7971,
      "step": 11580
    },
    {
      "epoch": 1.2102243088158582,
      "grad_norm": 3.9390885829925537,
      "learning_rate": 2.9935438841388942e-05,
      "loss": 2.7262,
      "step": 11600
    },
    {
      "epoch": 1.2123109024517476,
      "grad_norm": 3.3303096294403076,
      "learning_rate": 2.990054091781539e-05,
      "loss": 2.6681,
      "step": 11620
    },
    {
      "epoch": 1.214397496087637,
      "grad_norm": 3.7412757873535156,
      "learning_rate": 2.9865642994241845e-05,
      "loss": 2.7546,
      "step": 11640
    },
    {
      "epoch": 1.2164840897235263,
      "grad_norm": 4.1915435791015625,
      "learning_rate": 2.9830745070668296e-05,
      "loss": 2.8078,
      "step": 11660
    },
    {
      "epoch": 1.2185706833594159,
      "grad_norm": 3.4193413257598877,
      "learning_rate": 2.979584714709475e-05,
      "loss": 2.7157,
      "step": 11680
    },
    {
      "epoch": 1.2206572769953052,
      "grad_norm": 4.212924957275391,
      "learning_rate": 2.97609492235212e-05,
      "loss": 2.775,
      "step": 11700
    },
    {
      "epoch": 1.2227438706311946,
      "grad_norm": 3.3587214946746826,
      "learning_rate": 2.9726051299947654e-05,
      "loss": 2.7237,
      "step": 11720
    },
    {
      "epoch": 1.224830464267084,
      "grad_norm": 4.1487884521484375,
      "learning_rate": 2.969115337637411e-05,
      "loss": 2.7439,
      "step": 11740
    },
    {
      "epoch": 1.2269170579029733,
      "grad_norm": 4.20940637588501,
      "learning_rate": 2.9656255452800557e-05,
      "loss": 2.7379,
      "step": 11760
    },
    {
      "epoch": 1.229003651538863,
      "grad_norm": 3.3031609058380127,
      "learning_rate": 2.9621357529227012e-05,
      "loss": 2.7242,
      "step": 11780
    },
    {
      "epoch": 1.2310902451747523,
      "grad_norm": 3.863361120223999,
      "learning_rate": 2.9586459605653467e-05,
      "loss": 2.6965,
      "step": 11800
    },
    {
      "epoch": 1.2331768388106417,
      "grad_norm": 3.839603900909424,
      "learning_rate": 2.955156168207992e-05,
      "loss": 2.7202,
      "step": 11820
    },
    {
      "epoch": 1.235263432446531,
      "grad_norm": 4.516960620880127,
      "learning_rate": 2.951666375850637e-05,
      "loss": 2.769,
      "step": 11840
    },
    {
      "epoch": 1.2373500260824204,
      "grad_norm": 3.86894154548645,
      "learning_rate": 2.9481765834932825e-05,
      "loss": 2.6758,
      "step": 11860
    },
    {
      "epoch": 1.2394366197183098,
      "grad_norm": 3.5355889797210693,
      "learning_rate": 2.9446867911359276e-05,
      "loss": 2.794,
      "step": 11880
    },
    {
      "epoch": 1.2415232133541994,
      "grad_norm": 3.9652059078216553,
      "learning_rate": 2.9411969987785724e-05,
      "loss": 2.8138,
      "step": 11900
    },
    {
      "epoch": 1.2436098069900887,
      "grad_norm": 3.9044506549835205,
      "learning_rate": 2.937707206421218e-05,
      "loss": 2.6134,
      "step": 11920
    },
    {
      "epoch": 1.245696400625978,
      "grad_norm": 5.090015411376953,
      "learning_rate": 2.9342174140638634e-05,
      "loss": 2.818,
      "step": 11940
    },
    {
      "epoch": 1.2477829942618675,
      "grad_norm": 4.347915172576904,
      "learning_rate": 2.930727621706509e-05,
      "loss": 2.6186,
      "step": 11960
    },
    {
      "epoch": 1.2498695878977568,
      "grad_norm": 3.585151433944702,
      "learning_rate": 2.9272378293491537e-05,
      "loss": 2.725,
      "step": 11980
    },
    {
      "epoch": 1.2519561815336462,
      "grad_norm": 3.556467294692993,
      "learning_rate": 2.9237480369917992e-05,
      "loss": 2.6833,
      "step": 12000
    },
    {
      "epoch": 1.2540427751695358,
      "grad_norm": 4.041532039642334,
      "learning_rate": 2.9202582446344447e-05,
      "loss": 2.7463,
      "step": 12020
    },
    {
      "epoch": 1.2561293688054251,
      "grad_norm": 3.7671546936035156,
      "learning_rate": 2.9167684522770895e-05,
      "loss": 2.7397,
      "step": 12040
    },
    {
      "epoch": 1.2582159624413145,
      "grad_norm": 3.819619655609131,
      "learning_rate": 2.913278659919735e-05,
      "loss": 2.8139,
      "step": 12060
    },
    {
      "epoch": 1.260302556077204,
      "grad_norm": 4.491519927978516,
      "learning_rate": 2.90978886756238e-05,
      "loss": 2.5875,
      "step": 12080
    },
    {
      "epoch": 1.2623891497130932,
      "grad_norm": 4.045101642608643,
      "learning_rate": 2.9062990752050256e-05,
      "loss": 2.8064,
      "step": 12100
    },
    {
      "epoch": 1.2644757433489828,
      "grad_norm": 3.803126096725464,
      "learning_rate": 2.9028092828476704e-05,
      "loss": 2.6003,
      "step": 12120
    },
    {
      "epoch": 1.2665623369848722,
      "grad_norm": 4.960179328918457,
      "learning_rate": 2.899319490490316e-05,
      "loss": 2.7678,
      "step": 12140
    },
    {
      "epoch": 1.2686489306207616,
      "grad_norm": 4.533692359924316,
      "learning_rate": 2.8958296981329614e-05,
      "loss": 2.6422,
      "step": 12160
    },
    {
      "epoch": 1.2707355242566511,
      "grad_norm": 3.6454834938049316,
      "learning_rate": 2.892339905775607e-05,
      "loss": 2.7164,
      "step": 12180
    },
    {
      "epoch": 1.2728221178925403,
      "grad_norm": 4.31441593170166,
      "learning_rate": 2.8888501134182517e-05,
      "loss": 2.6535,
      "step": 12200
    },
    {
      "epoch": 1.2749087115284299,
      "grad_norm": 4.7957563400268555,
      "learning_rate": 2.885360321060897e-05,
      "loss": 2.6762,
      "step": 12220
    },
    {
      "epoch": 1.2769953051643192,
      "grad_norm": 3.5925543308258057,
      "learning_rate": 2.8818705287035423e-05,
      "loss": 2.6887,
      "step": 12240
    },
    {
      "epoch": 1.2790818988002086,
      "grad_norm": 4.644979000091553,
      "learning_rate": 2.8783807363461875e-05,
      "loss": 2.7292,
      "step": 12260
    },
    {
      "epoch": 1.2811684924360982,
      "grad_norm": 3.306992292404175,
      "learning_rate": 2.8748909439888326e-05,
      "loss": 2.701,
      "step": 12280
    },
    {
      "epoch": 1.2832550860719873,
      "grad_norm": 4.492326736450195,
      "learning_rate": 2.871401151631478e-05,
      "loss": 2.7617,
      "step": 12300
    },
    {
      "epoch": 1.285341679707877,
      "grad_norm": 5.406615257263184,
      "learning_rate": 2.8679113592741236e-05,
      "loss": 2.7765,
      "step": 12320
    },
    {
      "epoch": 1.2874282733437663,
      "grad_norm": 3.879159688949585,
      "learning_rate": 2.8644215669167684e-05,
      "loss": 2.7085,
      "step": 12340
    },
    {
      "epoch": 1.2895148669796557,
      "grad_norm": 4.185833930969238,
      "learning_rate": 2.860931774559414e-05,
      "loss": 2.6261,
      "step": 12360
    },
    {
      "epoch": 1.2916014606155453,
      "grad_norm": 4.038340091705322,
      "learning_rate": 2.8574419822020594e-05,
      "loss": 2.7516,
      "step": 12380
    },
    {
      "epoch": 1.2936880542514344,
      "grad_norm": 3.8879499435424805,
      "learning_rate": 2.853952189844704e-05,
      "loss": 2.6935,
      "step": 12400
    },
    {
      "epoch": 1.295774647887324,
      "grad_norm": 4.909348487854004,
      "learning_rate": 2.8504623974873497e-05,
      "loss": 2.7333,
      "step": 12420
    },
    {
      "epoch": 1.2978612415232134,
      "grad_norm": 3.647310972213745,
      "learning_rate": 2.846972605129995e-05,
      "loss": 2.7324,
      "step": 12440
    },
    {
      "epoch": 1.2999478351591027,
      "grad_norm": 4.106322765350342,
      "learning_rate": 2.8434828127726403e-05,
      "loss": 2.5572,
      "step": 12460
    },
    {
      "epoch": 1.302034428794992,
      "grad_norm": 3.633585214614868,
      "learning_rate": 2.839993020415285e-05,
      "loss": 2.71,
      "step": 12480
    },
    {
      "epoch": 1.3041210224308815,
      "grad_norm": 4.162114143371582,
      "learning_rate": 2.8365032280579306e-05,
      "loss": 2.7175,
      "step": 12500
    },
    {
      "epoch": 1.306207616066771,
      "grad_norm": 3.163818836212158,
      "learning_rate": 2.833013435700576e-05,
      "loss": 2.6659,
      "step": 12520
    },
    {
      "epoch": 1.3082942097026604,
      "grad_norm": 4.473788738250732,
      "learning_rate": 2.829523643343221e-05,
      "loss": 2.7286,
      "step": 12540
    },
    {
      "epoch": 1.3103808033385498,
      "grad_norm": 4.200985908508301,
      "learning_rate": 2.8260338509858664e-05,
      "loss": 2.7314,
      "step": 12560
    },
    {
      "epoch": 1.3124673969744391,
      "grad_norm": 4.604043006896973,
      "learning_rate": 2.822544058628512e-05,
      "loss": 2.7415,
      "step": 12580
    },
    {
      "epoch": 1.3145539906103285,
      "grad_norm": 4.114278793334961,
      "learning_rate": 2.8190542662711573e-05,
      "loss": 2.624,
      "step": 12600
    },
    {
      "epoch": 1.316640584246218,
      "grad_norm": 5.033934593200684,
      "learning_rate": 2.815564473913802e-05,
      "loss": 2.6283,
      "step": 12620
    },
    {
      "epoch": 1.3187271778821075,
      "grad_norm": 4.001364231109619,
      "learning_rate": 2.8120746815564476e-05,
      "loss": 2.6153,
      "step": 12640
    },
    {
      "epoch": 1.3208137715179968,
      "grad_norm": 4.882514476776123,
      "learning_rate": 2.8085848891990928e-05,
      "loss": 2.684,
      "step": 12660
    },
    {
      "epoch": 1.3229003651538862,
      "grad_norm": 6.323293685913086,
      "learning_rate": 2.8050950968417383e-05,
      "loss": 2.6438,
      "step": 12680
    },
    {
      "epoch": 1.3249869587897756,
      "grad_norm": 3.364931583404541,
      "learning_rate": 2.801605304484383e-05,
      "loss": 2.6491,
      "step": 12700
    },
    {
      "epoch": 1.3270735524256652,
      "grad_norm": 3.522508382797241,
      "learning_rate": 2.7981155121270286e-05,
      "loss": 2.6575,
      "step": 12720
    },
    {
      "epoch": 1.3291601460615545,
      "grad_norm": 3.9070820808410645,
      "learning_rate": 2.794625719769674e-05,
      "loss": 2.6199,
      "step": 12740
    },
    {
      "epoch": 1.331246739697444,
      "grad_norm": 4.132755279541016,
      "learning_rate": 2.791135927412319e-05,
      "loss": 2.7458,
      "step": 12760
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 4.059818744659424,
      "learning_rate": 2.7876461350549644e-05,
      "loss": 2.7198,
      "step": 12780
    },
    {
      "epoch": 1.3354199269692226,
      "grad_norm": 3.633251190185547,
      "learning_rate": 2.78415634269761e-05,
      "loss": 2.6654,
      "step": 12800
    },
    {
      "epoch": 1.3375065206051122,
      "grad_norm": 4.507572174072266,
      "learning_rate": 2.780666550340255e-05,
      "loss": 2.7141,
      "step": 12820
    },
    {
      "epoch": 1.3395931142410016,
      "grad_norm": 4.172826290130615,
      "learning_rate": 2.7771767579829e-05,
      "loss": 2.6791,
      "step": 12840
    },
    {
      "epoch": 1.341679707876891,
      "grad_norm": 4.692668437957764,
      "learning_rate": 2.7736869656255453e-05,
      "loss": 2.6702,
      "step": 12860
    },
    {
      "epoch": 1.3437663015127803,
      "grad_norm": 4.432159900665283,
      "learning_rate": 2.7701971732681908e-05,
      "loss": 2.7247,
      "step": 12880
    },
    {
      "epoch": 1.3458528951486697,
      "grad_norm": 3.485520362854004,
      "learning_rate": 2.7667073809108356e-05,
      "loss": 2.6842,
      "step": 12900
    },
    {
      "epoch": 1.3479394887845593,
      "grad_norm": 3.8233726024627686,
      "learning_rate": 2.763217588553481e-05,
      "loss": 2.6053,
      "step": 12920
    },
    {
      "epoch": 1.3500260824204486,
      "grad_norm": 4.481874465942383,
      "learning_rate": 2.7597277961961266e-05,
      "loss": 2.7315,
      "step": 12940
    },
    {
      "epoch": 1.352112676056338,
      "grad_norm": 4.502357006072998,
      "learning_rate": 2.756238003838772e-05,
      "loss": 2.6404,
      "step": 12960
    },
    {
      "epoch": 1.3541992696922274,
      "grad_norm": 3.979825973510742,
      "learning_rate": 2.752748211481417e-05,
      "loss": 2.6023,
      "step": 12980
    },
    {
      "epoch": 1.3562858633281167,
      "grad_norm": 3.6806089878082275,
      "learning_rate": 2.7492584191240623e-05,
      "loss": 2.6709,
      "step": 13000
    },
    {
      "epoch": 1.3583724569640063,
      "grad_norm": 3.86993408203125,
      "learning_rate": 2.7457686267667075e-05,
      "loss": 2.7673,
      "step": 13020
    },
    {
      "epoch": 1.3604590505998957,
      "grad_norm": 3.805842399597168,
      "learning_rate": 2.7422788344093526e-05,
      "loss": 2.7365,
      "step": 13040
    },
    {
      "epoch": 1.362545644235785,
      "grad_norm": 3.7016191482543945,
      "learning_rate": 2.7387890420519978e-05,
      "loss": 2.7301,
      "step": 13060
    },
    {
      "epoch": 1.3646322378716744,
      "grad_norm": 3.726118803024292,
      "learning_rate": 2.7352992496946433e-05,
      "loss": 2.6658,
      "step": 13080
    },
    {
      "epoch": 1.3667188315075638,
      "grad_norm": 4.087122440338135,
      "learning_rate": 2.7318094573372888e-05,
      "loss": 2.7621,
      "step": 13100
    },
    {
      "epoch": 1.3688054251434534,
      "grad_norm": 3.9749298095703125,
      "learning_rate": 2.7283196649799336e-05,
      "loss": 2.6007,
      "step": 13120
    },
    {
      "epoch": 1.3708920187793427,
      "grad_norm": 4.108143329620361,
      "learning_rate": 2.724829872622579e-05,
      "loss": 2.6319,
      "step": 13140
    },
    {
      "epoch": 1.3729786124152321,
      "grad_norm": 3.555729389190674,
      "learning_rate": 2.7213400802652245e-05,
      "loss": 2.7116,
      "step": 13160
    },
    {
      "epoch": 1.3750652060511215,
      "grad_norm": 5.76149845123291,
      "learning_rate": 2.71785028790787e-05,
      "loss": 2.7009,
      "step": 13180
    },
    {
      "epoch": 1.3771517996870108,
      "grad_norm": 3.8467857837677,
      "learning_rate": 2.714360495550515e-05,
      "loss": 2.7311,
      "step": 13200
    },
    {
      "epoch": 1.3792383933229004,
      "grad_norm": 3.461158514022827,
      "learning_rate": 2.7108707031931603e-05,
      "loss": 2.6836,
      "step": 13220
    },
    {
      "epoch": 1.3813249869587898,
      "grad_norm": 5.156351089477539,
      "learning_rate": 2.7073809108358055e-05,
      "loss": 2.6672,
      "step": 13240
    },
    {
      "epoch": 1.3834115805946792,
      "grad_norm": 4.159293174743652,
      "learning_rate": 2.7038911184784503e-05,
      "loss": 2.7579,
      "step": 13260
    },
    {
      "epoch": 1.3854981742305685,
      "grad_norm": 4.912625312805176,
      "learning_rate": 2.7004013261210958e-05,
      "loss": 2.7032,
      "step": 13280
    },
    {
      "epoch": 1.387584767866458,
      "grad_norm": 4.223891258239746,
      "learning_rate": 2.6969115337637413e-05,
      "loss": 2.6463,
      "step": 13300
    },
    {
      "epoch": 1.3896713615023475,
      "grad_norm": 3.0673933029174805,
      "learning_rate": 2.6934217414063867e-05,
      "loss": 2.6388,
      "step": 13320
    },
    {
      "epoch": 1.3917579551382369,
      "grad_norm": 4.294334888458252,
      "learning_rate": 2.6899319490490315e-05,
      "loss": 2.6646,
      "step": 13340
    },
    {
      "epoch": 1.3938445487741262,
      "grad_norm": 3.5462288856506348,
      "learning_rate": 2.686442156691677e-05,
      "loss": 2.6336,
      "step": 13360
    },
    {
      "epoch": 1.3959311424100156,
      "grad_norm": 4.329077243804932,
      "learning_rate": 2.6829523643343225e-05,
      "loss": 2.6702,
      "step": 13380
    },
    {
      "epoch": 1.398017736045905,
      "grad_norm": 3.7012734413146973,
      "learning_rate": 2.6794625719769673e-05,
      "loss": 2.5665,
      "step": 13400
    },
    {
      "epoch": 1.4001043296817945,
      "grad_norm": 3.5761375427246094,
      "learning_rate": 2.6759727796196128e-05,
      "loss": 2.7868,
      "step": 13420
    },
    {
      "epoch": 1.402190923317684,
      "grad_norm": 4.368717670440674,
      "learning_rate": 2.672482987262258e-05,
      "loss": 2.81,
      "step": 13440
    },
    {
      "epoch": 1.4042775169535733,
      "grad_norm": 3.6652519702911377,
      "learning_rate": 2.6689931949049035e-05,
      "loss": 2.7193,
      "step": 13460
    },
    {
      "epoch": 1.4063641105894626,
      "grad_norm": 4.440767288208008,
      "learning_rate": 2.6655034025475483e-05,
      "loss": 2.6868,
      "step": 13480
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 4.322360038757324,
      "learning_rate": 2.6620136101901937e-05,
      "loss": 2.6876,
      "step": 13500
    },
    {
      "epoch": 1.4105372978612416,
      "grad_norm": 3.7033753395080566,
      "learning_rate": 2.6585238178328392e-05,
      "loss": 2.6397,
      "step": 13520
    },
    {
      "epoch": 1.412623891497131,
      "grad_norm": 3.6956803798675537,
      "learning_rate": 2.655034025475484e-05,
      "loss": 2.7426,
      "step": 13540
    },
    {
      "epoch": 1.4147104851330203,
      "grad_norm": 3.8374807834625244,
      "learning_rate": 2.6515442331181295e-05,
      "loss": 2.7664,
      "step": 13560
    },
    {
      "epoch": 1.4167970787689097,
      "grad_norm": 3.9958064556121826,
      "learning_rate": 2.648054440760775e-05,
      "loss": 2.7954,
      "step": 13580
    },
    {
      "epoch": 1.418883672404799,
      "grad_norm": 4.408527851104736,
      "learning_rate": 2.64456464840342e-05,
      "loss": 2.7153,
      "step": 13600
    },
    {
      "epoch": 1.4209702660406887,
      "grad_norm": 4.170026779174805,
      "learning_rate": 2.6410748560460653e-05,
      "loss": 2.5898,
      "step": 13620
    },
    {
      "epoch": 1.423056859676578,
      "grad_norm": 3.744641065597534,
      "learning_rate": 2.6375850636887105e-05,
      "loss": 2.7035,
      "step": 13640
    },
    {
      "epoch": 1.4251434533124674,
      "grad_norm": 4.476052284240723,
      "learning_rate": 2.634095271331356e-05,
      "loss": 2.6566,
      "step": 13660
    },
    {
      "epoch": 1.4272300469483568,
      "grad_norm": 4.136791229248047,
      "learning_rate": 2.6306054789740014e-05,
      "loss": 2.6617,
      "step": 13680
    },
    {
      "epoch": 1.4293166405842461,
      "grad_norm": 3.34651780128479,
      "learning_rate": 2.6271156866166462e-05,
      "loss": 2.6633,
      "step": 13700
    },
    {
      "epoch": 1.4314032342201357,
      "grad_norm": 4.177919864654541,
      "learning_rate": 2.6236258942592917e-05,
      "loss": 2.6634,
      "step": 13720
    },
    {
      "epoch": 1.433489827856025,
      "grad_norm": 4.080606460571289,
      "learning_rate": 2.6201361019019372e-05,
      "loss": 2.6862,
      "step": 13740
    },
    {
      "epoch": 1.4355764214919144,
      "grad_norm": 3.7031195163726807,
      "learning_rate": 2.616646309544582e-05,
      "loss": 2.5528,
      "step": 13760
    },
    {
      "epoch": 1.4376630151278038,
      "grad_norm": 4.3461408615112305,
      "learning_rate": 2.6131565171872275e-05,
      "loss": 2.6648,
      "step": 13780
    },
    {
      "epoch": 1.4397496087636932,
      "grad_norm": 5.33255672454834,
      "learning_rate": 2.609666724829873e-05,
      "loss": 2.6745,
      "step": 13800
    },
    {
      "epoch": 1.4418362023995828,
      "grad_norm": 4.0995588302612305,
      "learning_rate": 2.606176932472518e-05,
      "loss": 2.6522,
      "step": 13820
    },
    {
      "epoch": 1.4439227960354721,
      "grad_norm": 3.944342613220215,
      "learning_rate": 2.602687140115163e-05,
      "loss": 2.7206,
      "step": 13840
    },
    {
      "epoch": 1.4460093896713615,
      "grad_norm": 3.565667152404785,
      "learning_rate": 2.5991973477578084e-05,
      "loss": 2.6534,
      "step": 13860
    },
    {
      "epoch": 1.4480959833072509,
      "grad_norm": 4.808553695678711,
      "learning_rate": 2.595707555400454e-05,
      "loss": 2.7453,
      "step": 13880
    },
    {
      "epoch": 1.4501825769431402,
      "grad_norm": 3.432868003845215,
      "learning_rate": 2.5922177630430987e-05,
      "loss": 2.7356,
      "step": 13900
    },
    {
      "epoch": 1.4522691705790298,
      "grad_norm": 4.497560501098633,
      "learning_rate": 2.5887279706857442e-05,
      "loss": 2.7268,
      "step": 13920
    },
    {
      "epoch": 1.4543557642149192,
      "grad_norm": 3.7028186321258545,
      "learning_rate": 2.5852381783283897e-05,
      "loss": 2.6705,
      "step": 13940
    },
    {
      "epoch": 1.4564423578508086,
      "grad_norm": 3.706210136413574,
      "learning_rate": 2.5817483859710352e-05,
      "loss": 2.7877,
      "step": 13960
    },
    {
      "epoch": 1.458528951486698,
      "grad_norm": 3.9594244956970215,
      "learning_rate": 2.57825859361368e-05,
      "loss": 2.816,
      "step": 13980
    },
    {
      "epoch": 1.4606155451225873,
      "grad_norm": 4.4320573806762695,
      "learning_rate": 2.5747688012563255e-05,
      "loss": 2.7772,
      "step": 14000
    },
    {
      "epoch": 1.4627021387584769,
      "grad_norm": 4.9586286544799805,
      "learning_rate": 2.5712790088989706e-05,
      "loss": 2.7629,
      "step": 14020
    },
    {
      "epoch": 1.4647887323943662,
      "grad_norm": 3.9225642681121826,
      "learning_rate": 2.5677892165416158e-05,
      "loss": 2.6868,
      "step": 14040
    },
    {
      "epoch": 1.4668753260302556,
      "grad_norm": 4.180988311767578,
      "learning_rate": 2.564299424184261e-05,
      "loss": 2.6419,
      "step": 14060
    },
    {
      "epoch": 1.468961919666145,
      "grad_norm": 3.967170476913452,
      "learning_rate": 2.5608096318269064e-05,
      "loss": 2.6813,
      "step": 14080
    },
    {
      "epoch": 1.4710485133020343,
      "grad_norm": 3.639671564102173,
      "learning_rate": 2.557319839469552e-05,
      "loss": 2.7256,
      "step": 14100
    },
    {
      "epoch": 1.473135106937924,
      "grad_norm": 4.094947338104248,
      "learning_rate": 2.5538300471121967e-05,
      "loss": 2.7043,
      "step": 14120
    },
    {
      "epoch": 1.4752217005738133,
      "grad_norm": 4.370308876037598,
      "learning_rate": 2.5503402547548422e-05,
      "loss": 2.6229,
      "step": 14140
    },
    {
      "epoch": 1.4773082942097027,
      "grad_norm": 3.751702308654785,
      "learning_rate": 2.5468504623974877e-05,
      "loss": 2.7159,
      "step": 14160
    },
    {
      "epoch": 1.479394887845592,
      "grad_norm": 4.324955463409424,
      "learning_rate": 2.543360670040133e-05,
      "loss": 2.7053,
      "step": 14180
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 4.183027744293213,
      "learning_rate": 2.539870877682778e-05,
      "loss": 2.7491,
      "step": 14200
    },
    {
      "epoch": 1.483568075117371,
      "grad_norm": 5.638180255889893,
      "learning_rate": 2.536381085325423e-05,
      "loss": 2.7739,
      "step": 14220
    },
    {
      "epoch": 1.4856546687532604,
      "grad_norm": 3.8143138885498047,
      "learning_rate": 2.5328912929680686e-05,
      "loss": 2.7108,
      "step": 14240
    },
    {
      "epoch": 1.4877412623891497,
      "grad_norm": 5.983775615692139,
      "learning_rate": 2.5294015006107134e-05,
      "loss": 2.6174,
      "step": 14260
    },
    {
      "epoch": 1.489827856025039,
      "grad_norm": 4.902860641479492,
      "learning_rate": 2.525911708253359e-05,
      "loss": 2.7631,
      "step": 14280
    },
    {
      "epoch": 1.4919144496609285,
      "grad_norm": 3.8378584384918213,
      "learning_rate": 2.5224219158960044e-05,
      "loss": 2.731,
      "step": 14300
    },
    {
      "epoch": 1.494001043296818,
      "grad_norm": 4.020913124084473,
      "learning_rate": 2.51893212353865e-05,
      "loss": 2.6814,
      "step": 14320
    },
    {
      "epoch": 1.4960876369327074,
      "grad_norm": 4.0221638679504395,
      "learning_rate": 2.5154423311812947e-05,
      "loss": 2.7837,
      "step": 14340
    },
    {
      "epoch": 1.4981742305685968,
      "grad_norm": 3.886237859725952,
      "learning_rate": 2.5119525388239402e-05,
      "loss": 2.7225,
      "step": 14360
    },
    {
      "epoch": 1.5002608242044861,
      "grad_norm": 4.3350629806518555,
      "learning_rate": 2.5084627464665857e-05,
      "loss": 2.6862,
      "step": 14380
    },
    {
      "epoch": 1.5023474178403755,
      "grad_norm": 4.824154376983643,
      "learning_rate": 2.5049729541092305e-05,
      "loss": 2.7273,
      "step": 14400
    },
    {
      "epoch": 1.504434011476265,
      "grad_norm": 3.094050168991089,
      "learning_rate": 2.5014831617518756e-05,
      "loss": 2.6563,
      "step": 14420
    },
    {
      "epoch": 1.5065206051121542,
      "grad_norm": 3.9597957134246826,
      "learning_rate": 2.497993369394521e-05,
      "loss": 2.6306,
      "step": 14440
    },
    {
      "epoch": 1.5086071987480438,
      "grad_norm": 4.16016960144043,
      "learning_rate": 2.4945035770371663e-05,
      "loss": 2.642,
      "step": 14460
    },
    {
      "epoch": 1.5106937923839332,
      "grad_norm": 4.394265651702881,
      "learning_rate": 2.4910137846798118e-05,
      "loss": 2.6974,
      "step": 14480
    },
    {
      "epoch": 1.5127803860198226,
      "grad_norm": 3.7469046115875244,
      "learning_rate": 2.487523992322457e-05,
      "loss": 2.7118,
      "step": 14500
    },
    {
      "epoch": 1.5148669796557122,
      "grad_norm": 4.030013561248779,
      "learning_rate": 2.484034199965102e-05,
      "loss": 2.7388,
      "step": 14520
    },
    {
      "epoch": 1.5169535732916013,
      "grad_norm": 4.606289386749268,
      "learning_rate": 2.4805444076077475e-05,
      "loss": 2.7267,
      "step": 14540
    },
    {
      "epoch": 1.5190401669274909,
      "grad_norm": 4.682961463928223,
      "learning_rate": 2.4770546152503927e-05,
      "loss": 2.7962,
      "step": 14560
    },
    {
      "epoch": 1.5211267605633803,
      "grad_norm": 4.538148880004883,
      "learning_rate": 2.4735648228930382e-05,
      "loss": 2.7454,
      "step": 14580
    },
    {
      "epoch": 1.5232133541992696,
      "grad_norm": 3.5258114337921143,
      "learning_rate": 2.4700750305356833e-05,
      "loss": 2.6776,
      "step": 14600
    },
    {
      "epoch": 1.5252999478351592,
      "grad_norm": 4.008053779602051,
      "learning_rate": 2.4665852381783285e-05,
      "loss": 2.7501,
      "step": 14620
    },
    {
      "epoch": 1.5273865414710484,
      "grad_norm": 3.5196948051452637,
      "learning_rate": 2.4630954458209736e-05,
      "loss": 2.711,
      "step": 14640
    },
    {
      "epoch": 1.529473135106938,
      "grad_norm": 4.065423488616943,
      "learning_rate": 2.459605653463619e-05,
      "loss": 2.7394,
      "step": 14660
    },
    {
      "epoch": 1.5315597287428273,
      "grad_norm": 3.774212121963501,
      "learning_rate": 2.4561158611062643e-05,
      "loss": 2.725,
      "step": 14680
    },
    {
      "epoch": 1.5336463223787167,
      "grad_norm": 3.4935121536254883,
      "learning_rate": 2.4526260687489094e-05,
      "loss": 2.7472,
      "step": 14700
    },
    {
      "epoch": 1.5357329160146063,
      "grad_norm": 3.3918323516845703,
      "learning_rate": 2.449136276391555e-05,
      "loss": 2.6673,
      "step": 14720
    },
    {
      "epoch": 1.5378195096504954,
      "grad_norm": 3.5958828926086426,
      "learning_rate": 2.4456464840342e-05,
      "loss": 2.7875,
      "step": 14740
    },
    {
      "epoch": 1.539906103286385,
      "grad_norm": 4.027980327606201,
      "learning_rate": 2.4421566916768455e-05,
      "loss": 2.6458,
      "step": 14760
    },
    {
      "epoch": 1.5419926969222744,
      "grad_norm": 3.9080841541290283,
      "learning_rate": 2.4386668993194907e-05,
      "loss": 2.6737,
      "step": 14780
    },
    {
      "epoch": 1.5440792905581637,
      "grad_norm": 3.797276020050049,
      "learning_rate": 2.4351771069621358e-05,
      "loss": 2.7257,
      "step": 14800
    },
    {
      "epoch": 1.5461658841940533,
      "grad_norm": 3.7031428813934326,
      "learning_rate": 2.431687314604781e-05,
      "loss": 2.7605,
      "step": 14820
    },
    {
      "epoch": 1.5482524778299425,
      "grad_norm": 3.5521814823150635,
      "learning_rate": 2.4281975222474265e-05,
      "loss": 2.6913,
      "step": 14840
    },
    {
      "epoch": 1.550339071465832,
      "grad_norm": 3.4017395973205566,
      "learning_rate": 2.4247077298900716e-05,
      "loss": 2.6585,
      "step": 14860
    },
    {
      "epoch": 1.5524256651017214,
      "grad_norm": 3.30102801322937,
      "learning_rate": 2.4212179375327168e-05,
      "loss": 2.6887,
      "step": 14880
    },
    {
      "epoch": 1.5545122587376108,
      "grad_norm": 3.49033522605896,
      "learning_rate": 2.4177281451753622e-05,
      "loss": 2.7227,
      "step": 14900
    },
    {
      "epoch": 1.5565988523735004,
      "grad_norm": 4.300232410430908,
      "learning_rate": 2.4142383528180074e-05,
      "loss": 2.6589,
      "step": 14920
    },
    {
      "epoch": 1.5586854460093895,
      "grad_norm": 4.158451557159424,
      "learning_rate": 2.410748560460653e-05,
      "loss": 2.6658,
      "step": 14940
    },
    {
      "epoch": 1.560772039645279,
      "grad_norm": 4.76943826675415,
      "learning_rate": 2.407258768103298e-05,
      "loss": 2.7745,
      "step": 14960
    },
    {
      "epoch": 1.5628586332811685,
      "grad_norm": 3.099896192550659,
      "learning_rate": 2.4037689757459432e-05,
      "loss": 2.6879,
      "step": 14980
    },
    {
      "epoch": 1.5649452269170578,
      "grad_norm": 3.935579299926758,
      "learning_rate": 2.4002791833885883e-05,
      "loss": 2.7303,
      "step": 15000
    },
    {
      "epoch": 1.5670318205529474,
      "grad_norm": 4.862922668457031,
      "learning_rate": 2.3967893910312335e-05,
      "loss": 2.7688,
      "step": 15020
    },
    {
      "epoch": 1.5691184141888366,
      "grad_norm": 3.4467060565948486,
      "learning_rate": 2.393299598673879e-05,
      "loss": 2.6617,
      "step": 15040
    },
    {
      "epoch": 1.5712050078247262,
      "grad_norm": 3.3369078636169434,
      "learning_rate": 2.389809806316524e-05,
      "loss": 2.8003,
      "step": 15060
    },
    {
      "epoch": 1.5732916014606155,
      "grad_norm": 3.252234697341919,
      "learning_rate": 2.3863200139591696e-05,
      "loss": 2.6283,
      "step": 15080
    },
    {
      "epoch": 1.575378195096505,
      "grad_norm": 3.6452693939208984,
      "learning_rate": 2.3828302216018147e-05,
      "loss": 2.7251,
      "step": 15100
    },
    {
      "epoch": 1.5774647887323945,
      "grad_norm": 3.097381114959717,
      "learning_rate": 2.3793404292444602e-05,
      "loss": 2.6735,
      "step": 15120
    },
    {
      "epoch": 1.5795513823682836,
      "grad_norm": 5.096693992614746,
      "learning_rate": 2.3758506368871054e-05,
      "loss": 2.7609,
      "step": 15140
    },
    {
      "epoch": 1.5816379760041732,
      "grad_norm": 3.7311394214630127,
      "learning_rate": 2.372360844529751e-05,
      "loss": 2.8517,
      "step": 15160
    },
    {
      "epoch": 1.5837245696400626,
      "grad_norm": 3.411975622177124,
      "learning_rate": 2.368871052172396e-05,
      "loss": 2.6953,
      "step": 15180
    },
    {
      "epoch": 1.585811163275952,
      "grad_norm": 3.6866414546966553,
      "learning_rate": 2.365381259815041e-05,
      "loss": 2.7117,
      "step": 15200
    },
    {
      "epoch": 1.5878977569118415,
      "grad_norm": 4.671419143676758,
      "learning_rate": 2.3618914674576863e-05,
      "loss": 2.6725,
      "step": 15220
    },
    {
      "epoch": 1.5899843505477307,
      "grad_norm": 4.003323078155518,
      "learning_rate": 2.3584016751003315e-05,
      "loss": 2.8128,
      "step": 15240
    },
    {
      "epoch": 1.5920709441836203,
      "grad_norm": 4.55520486831665,
      "learning_rate": 2.354911882742977e-05,
      "loss": 2.7167,
      "step": 15260
    },
    {
      "epoch": 1.5941575378195096,
      "grad_norm": 4.835864067077637,
      "learning_rate": 2.351422090385622e-05,
      "loss": 2.7417,
      "step": 15280
    },
    {
      "epoch": 1.596244131455399,
      "grad_norm": 4.94669246673584,
      "learning_rate": 2.3479322980282676e-05,
      "loss": 2.757,
      "step": 15300
    },
    {
      "epoch": 1.5983307250912886,
      "grad_norm": 3.4943599700927734,
      "learning_rate": 2.3444425056709127e-05,
      "loss": 2.6934,
      "step": 15320
    },
    {
      "epoch": 1.6004173187271777,
      "grad_norm": 4.091975212097168,
      "learning_rate": 2.3409527133135582e-05,
      "loss": 2.671,
      "step": 15340
    },
    {
      "epoch": 1.6025039123630673,
      "grad_norm": 4.164900302886963,
      "learning_rate": 2.3374629209562034e-05,
      "loss": 2.6841,
      "step": 15360
    },
    {
      "epoch": 1.6045905059989567,
      "grad_norm": 3.482560873031616,
      "learning_rate": 2.3339731285988485e-05,
      "loss": 2.6941,
      "step": 15380
    },
    {
      "epoch": 1.606677099634846,
      "grad_norm": 3.621534824371338,
      "learning_rate": 2.3304833362414937e-05,
      "loss": 2.6755,
      "step": 15400
    },
    {
      "epoch": 1.6087636932707357,
      "grad_norm": 4.377970218658447,
      "learning_rate": 2.3269935438841388e-05,
      "loss": 2.7286,
      "step": 15420
    },
    {
      "epoch": 1.6108502869066248,
      "grad_norm": 4.1236724853515625,
      "learning_rate": 2.3235037515267843e-05,
      "loss": 2.6453,
      "step": 15440
    },
    {
      "epoch": 1.6129368805425144,
      "grad_norm": 3.5418217182159424,
      "learning_rate": 2.3200139591694294e-05,
      "loss": 2.741,
      "step": 15460
    },
    {
      "epoch": 1.6150234741784038,
      "grad_norm": 3.4074528217315674,
      "learning_rate": 2.316524166812075e-05,
      "loss": 2.6961,
      "step": 15480
    },
    {
      "epoch": 1.6171100678142931,
      "grad_norm": 4.134047031402588,
      "learning_rate": 2.31303437445472e-05,
      "loss": 2.695,
      "step": 15500
    },
    {
      "epoch": 1.6191966614501827,
      "grad_norm": 4.72158670425415,
      "learning_rate": 2.3095445820973652e-05,
      "loss": 2.7045,
      "step": 15520
    },
    {
      "epoch": 1.6212832550860719,
      "grad_norm": 4.512462615966797,
      "learning_rate": 2.3060547897400107e-05,
      "loss": 2.6528,
      "step": 15540
    },
    {
      "epoch": 1.6233698487219614,
      "grad_norm": 3.738273859024048,
      "learning_rate": 2.302564997382656e-05,
      "loss": 2.7075,
      "step": 15560
    },
    {
      "epoch": 1.6254564423578508,
      "grad_norm": 4.436382293701172,
      "learning_rate": 2.299075205025301e-05,
      "loss": 2.7191,
      "step": 15580
    },
    {
      "epoch": 1.6275430359937402,
      "grad_norm": 3.4200170040130615,
      "learning_rate": 2.295585412667946e-05,
      "loss": 2.7232,
      "step": 15600
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 3.3826565742492676,
      "learning_rate": 2.2920956203105916e-05,
      "loss": 2.7613,
      "step": 15620
    },
    {
      "epoch": 1.631716223265519,
      "grad_norm": 3.2402360439300537,
      "learning_rate": 2.2886058279532368e-05,
      "loss": 2.6771,
      "step": 15640
    },
    {
      "epoch": 1.6338028169014085,
      "grad_norm": 3.7348365783691406,
      "learning_rate": 2.2851160355958823e-05,
      "loss": 2.6386,
      "step": 15660
    },
    {
      "epoch": 1.6358894105372979,
      "grad_norm": 3.088359832763672,
      "learning_rate": 2.2816262432385274e-05,
      "loss": 2.6073,
      "step": 15680
    },
    {
      "epoch": 1.6379760041731872,
      "grad_norm": 4.2214155197143555,
      "learning_rate": 2.2781364508811726e-05,
      "loss": 2.7065,
      "step": 15700
    },
    {
      "epoch": 1.6400625978090768,
      "grad_norm": 3.3570618629455566,
      "learning_rate": 2.274646658523818e-05,
      "loss": 2.695,
      "step": 15720
    },
    {
      "epoch": 1.642149191444966,
      "grad_norm": 3.4968373775482178,
      "learning_rate": 2.2711568661664632e-05,
      "loss": 2.7853,
      "step": 15740
    },
    {
      "epoch": 1.6442357850808555,
      "grad_norm": 3.7724709510803223,
      "learning_rate": 2.2676670738091087e-05,
      "loss": 2.6231,
      "step": 15760
    },
    {
      "epoch": 1.646322378716745,
      "grad_norm": 4.114312171936035,
      "learning_rate": 2.2641772814517535e-05,
      "loss": 2.6693,
      "step": 15780
    },
    {
      "epoch": 1.6484089723526343,
      "grad_norm": 4.161987781524658,
      "learning_rate": 2.260687489094399e-05,
      "loss": 2.68,
      "step": 15800
    },
    {
      "epoch": 1.6504955659885239,
      "grad_norm": 3.593043088912964,
      "learning_rate": 2.257197696737044e-05,
      "loss": 2.6545,
      "step": 15820
    },
    {
      "epoch": 1.652582159624413,
      "grad_norm": 3.919732093811035,
      "learning_rate": 2.2537079043796896e-05,
      "loss": 2.6781,
      "step": 15840
    },
    {
      "epoch": 1.6546687532603026,
      "grad_norm": 3.1021335124969482,
      "learning_rate": 2.2502181120223348e-05,
      "loss": 2.6664,
      "step": 15860
    },
    {
      "epoch": 1.656755346896192,
      "grad_norm": 4.276905059814453,
      "learning_rate": 2.24672831966498e-05,
      "loss": 2.7668,
      "step": 15880
    },
    {
      "epoch": 1.6588419405320813,
      "grad_norm": 3.8691771030426025,
      "learning_rate": 2.2432385273076254e-05,
      "loss": 2.6344,
      "step": 15900
    },
    {
      "epoch": 1.660928534167971,
      "grad_norm": 3.679814577102661,
      "learning_rate": 2.2397487349502706e-05,
      "loss": 2.7185,
      "step": 15920
    },
    {
      "epoch": 1.66301512780386,
      "grad_norm": 3.8414034843444824,
      "learning_rate": 2.236258942592916e-05,
      "loss": 2.688,
      "step": 15940
    },
    {
      "epoch": 1.6651017214397497,
      "grad_norm": 4.197015285491943,
      "learning_rate": 2.2327691502355612e-05,
      "loss": 2.6666,
      "step": 15960
    },
    {
      "epoch": 1.667188315075639,
      "grad_norm": 3.7017178535461426,
      "learning_rate": 2.2292793578782063e-05,
      "loss": 2.7083,
      "step": 15980
    },
    {
      "epoch": 1.6692749087115284,
      "grad_norm": 3.7960660457611084,
      "learning_rate": 2.2257895655208515e-05,
      "loss": 2.7126,
      "step": 16000
    },
    {
      "epoch": 1.671361502347418,
      "grad_norm": 4.453550815582275,
      "learning_rate": 2.2222997731634966e-05,
      "loss": 2.6781,
      "step": 16020
    },
    {
      "epoch": 1.6734480959833071,
      "grad_norm": 3.500513792037964,
      "learning_rate": 2.218809980806142e-05,
      "loss": 2.7514,
      "step": 16040
    },
    {
      "epoch": 1.6755346896191967,
      "grad_norm": 3.2877838611602783,
      "learning_rate": 2.2153201884487873e-05,
      "loss": 2.7291,
      "step": 16060
    },
    {
      "epoch": 1.677621283255086,
      "grad_norm": 4.951162338256836,
      "learning_rate": 2.2118303960914328e-05,
      "loss": 2.7208,
      "step": 16080
    },
    {
      "epoch": 1.6797078768909754,
      "grad_norm": 3.670713424682617,
      "learning_rate": 2.208340603734078e-05,
      "loss": 2.689,
      "step": 16100
    },
    {
      "epoch": 1.681794470526865,
      "grad_norm": 3.8640966415405273,
      "learning_rate": 2.2048508113767234e-05,
      "loss": 2.6986,
      "step": 16120
    },
    {
      "epoch": 1.6838810641627542,
      "grad_norm": 3.444828987121582,
      "learning_rate": 2.2013610190193685e-05,
      "loss": 2.7097,
      "step": 16140
    },
    {
      "epoch": 1.6859676577986438,
      "grad_norm": 4.227009296417236,
      "learning_rate": 2.1978712266620137e-05,
      "loss": 2.7189,
      "step": 16160
    },
    {
      "epoch": 1.6880542514345331,
      "grad_norm": 3.492708206176758,
      "learning_rate": 2.194381434304659e-05,
      "loss": 2.7375,
      "step": 16180
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 4.315305709838867,
      "learning_rate": 2.190891641947304e-05,
      "loss": 2.7038,
      "step": 16200
    },
    {
      "epoch": 1.692227438706312,
      "grad_norm": 3.51482892036438,
      "learning_rate": 2.1874018495899495e-05,
      "loss": 2.7215,
      "step": 16220
    },
    {
      "epoch": 1.6943140323422012,
      "grad_norm": 3.8832027912139893,
      "learning_rate": 2.1839120572325946e-05,
      "loss": 2.8054,
      "step": 16240
    },
    {
      "epoch": 1.6964006259780908,
      "grad_norm": 4.144138336181641,
      "learning_rate": 2.18042226487524e-05,
      "loss": 2.6967,
      "step": 16260
    },
    {
      "epoch": 1.6984872196139802,
      "grad_norm": 3.2647812366485596,
      "learning_rate": 2.1769324725178853e-05,
      "loss": 2.6817,
      "step": 16280
    },
    {
      "epoch": 1.7005738132498696,
      "grad_norm": 3.9582440853118896,
      "learning_rate": 2.1734426801605307e-05,
      "loss": 2.6782,
      "step": 16300
    },
    {
      "epoch": 1.7026604068857591,
      "grad_norm": 3.4446208477020264,
      "learning_rate": 2.169952887803176e-05,
      "loss": 2.7363,
      "step": 16320
    },
    {
      "epoch": 1.7047470005216483,
      "grad_norm": 3.4011223316192627,
      "learning_rate": 2.166463095445821e-05,
      "loss": 2.6168,
      "step": 16340
    },
    {
      "epoch": 1.7068335941575379,
      "grad_norm": 4.535950183868408,
      "learning_rate": 2.1629733030884662e-05,
      "loss": 2.7487,
      "step": 16360
    },
    {
      "epoch": 1.7089201877934272,
      "grad_norm": 4.716155052185059,
      "learning_rate": 2.1594835107311113e-05,
      "loss": 2.8437,
      "step": 16380
    },
    {
      "epoch": 1.7110067814293166,
      "grad_norm": 4.194097518920898,
      "learning_rate": 2.1559937183737568e-05,
      "loss": 2.6781,
      "step": 16400
    },
    {
      "epoch": 1.7130933750652062,
      "grad_norm": 4.9637017250061035,
      "learning_rate": 2.152503926016402e-05,
      "loss": 2.7061,
      "step": 16420
    },
    {
      "epoch": 1.7151799687010953,
      "grad_norm": 3.5717380046844482,
      "learning_rate": 2.1490141336590475e-05,
      "loss": 2.684,
      "step": 16440
    },
    {
      "epoch": 1.717266562336985,
      "grad_norm": 3.3810102939605713,
      "learning_rate": 2.1455243413016926e-05,
      "loss": 2.6885,
      "step": 16460
    },
    {
      "epoch": 1.7193531559728743,
      "grad_norm": 3.7843899726867676,
      "learning_rate": 2.142034548944338e-05,
      "loss": 2.8052,
      "step": 16480
    },
    {
      "epoch": 1.7214397496087637,
      "grad_norm": 3.8759896755218506,
      "learning_rate": 2.1385447565869832e-05,
      "loss": 2.7182,
      "step": 16500
    },
    {
      "epoch": 1.7235263432446533,
      "grad_norm": 3.851048231124878,
      "learning_rate": 2.1350549642296284e-05,
      "loss": 2.7211,
      "step": 16520
    },
    {
      "epoch": 1.7256129368805424,
      "grad_norm": 4.01200532913208,
      "learning_rate": 2.131565171872274e-05,
      "loss": 2.6357,
      "step": 16540
    },
    {
      "epoch": 1.727699530516432,
      "grad_norm": 3.654707431793213,
      "learning_rate": 2.128075379514919e-05,
      "loss": 2.7396,
      "step": 16560
    },
    {
      "epoch": 1.7297861241523214,
      "grad_norm": 3.897984027862549,
      "learning_rate": 2.1245855871575642e-05,
      "loss": 2.6185,
      "step": 16580
    },
    {
      "epoch": 1.7318727177882107,
      "grad_norm": 3.864755630493164,
      "learning_rate": 2.1210957948002093e-05,
      "loss": 2.7523,
      "step": 16600
    },
    {
      "epoch": 1.7339593114241003,
      "grad_norm": 4.405309677124023,
      "learning_rate": 2.1176060024428548e-05,
      "loss": 2.6897,
      "step": 16620
    },
    {
      "epoch": 1.7360459050599895,
      "grad_norm": 4.0891499519348145,
      "learning_rate": 2.1141162100855e-05,
      "loss": 2.7312,
      "step": 16640
    },
    {
      "epoch": 1.738132498695879,
      "grad_norm": 3.9323391914367676,
      "learning_rate": 2.1106264177281454e-05,
      "loss": 2.761,
      "step": 16660
    },
    {
      "epoch": 1.7402190923317684,
      "grad_norm": 3.937269687652588,
      "learning_rate": 2.1071366253707906e-05,
      "loss": 2.7724,
      "step": 16680
    },
    {
      "epoch": 1.7423056859676578,
      "grad_norm": 4.389322280883789,
      "learning_rate": 2.1036468330134357e-05,
      "loss": 2.6986,
      "step": 16700
    },
    {
      "epoch": 1.7443922796035474,
      "grad_norm": 4.174658298492432,
      "learning_rate": 2.1001570406560812e-05,
      "loss": 2.7032,
      "step": 16720
    },
    {
      "epoch": 1.7464788732394365,
      "grad_norm": 3.686293125152588,
      "learning_rate": 2.0966672482987264e-05,
      "loss": 2.7248,
      "step": 16740
    },
    {
      "epoch": 1.748565466875326,
      "grad_norm": 3.4830849170684814,
      "learning_rate": 2.0931774559413715e-05,
      "loss": 2.8103,
      "step": 16760
    },
    {
      "epoch": 1.7506520605112155,
      "grad_norm": 3.7433815002441406,
      "learning_rate": 2.0896876635840167e-05,
      "loss": 2.6488,
      "step": 16780
    },
    {
      "epoch": 1.7527386541471048,
      "grad_norm": 3.870828628540039,
      "learning_rate": 2.086197871226662e-05,
      "loss": 2.7701,
      "step": 16800
    },
    {
      "epoch": 1.7548252477829944,
      "grad_norm": 3.7296435832977295,
      "learning_rate": 2.0827080788693073e-05,
      "loss": 2.7231,
      "step": 16820
    },
    {
      "epoch": 1.7569118414188836,
      "grad_norm": 3.8046960830688477,
      "learning_rate": 2.0792182865119528e-05,
      "loss": 2.6365,
      "step": 16840
    },
    {
      "epoch": 1.7589984350547732,
      "grad_norm": 3.5676510334014893,
      "learning_rate": 2.075728494154598e-05,
      "loss": 2.7782,
      "step": 16860
    },
    {
      "epoch": 1.7610850286906625,
      "grad_norm": 4.4636759757995605,
      "learning_rate": 2.072238701797243e-05,
      "loss": 2.713,
      "step": 16880
    },
    {
      "epoch": 1.763171622326552,
      "grad_norm": 4.070155143737793,
      "learning_rate": 2.0687489094398886e-05,
      "loss": 2.746,
      "step": 16900
    },
    {
      "epoch": 1.7652582159624415,
      "grad_norm": 4.254730224609375,
      "learning_rate": 2.0652591170825337e-05,
      "loss": 2.7864,
      "step": 16920
    },
    {
      "epoch": 1.7673448095983306,
      "grad_norm": 3.5642263889312744,
      "learning_rate": 2.061769324725179e-05,
      "loss": 2.6273,
      "step": 16940
    },
    {
      "epoch": 1.7694314032342202,
      "grad_norm": 4.044175624847412,
      "learning_rate": 2.058279532367824e-05,
      "loss": 2.6865,
      "step": 16960
    },
    {
      "epoch": 1.7715179968701096,
      "grad_norm": 3.616225004196167,
      "learning_rate": 2.0547897400104695e-05,
      "loss": 2.7417,
      "step": 16980
    },
    {
      "epoch": 1.773604590505999,
      "grad_norm": 3.77203106880188,
      "learning_rate": 2.0512999476531147e-05,
      "loss": 2.6475,
      "step": 17000
    },
    {
      "epoch": 1.7756911841418883,
      "grad_norm": 3.4155049324035645,
      "learning_rate": 2.0478101552957598e-05,
      "loss": 2.6999,
      "step": 17020
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 4.123930931091309,
      "learning_rate": 2.0443203629384053e-05,
      "loss": 2.7452,
      "step": 17040
    },
    {
      "epoch": 1.7798643714136673,
      "grad_norm": 3.7550411224365234,
      "learning_rate": 2.0408305705810504e-05,
      "loss": 2.6071,
      "step": 17060
    },
    {
      "epoch": 1.7819509650495566,
      "grad_norm": 4.166569709777832,
      "learning_rate": 2.037340778223696e-05,
      "loss": 2.6576,
      "step": 17080
    },
    {
      "epoch": 1.784037558685446,
      "grad_norm": 3.7951786518096924,
      "learning_rate": 2.033850985866341e-05,
      "loss": 2.7842,
      "step": 17100
    },
    {
      "epoch": 1.7861241523213354,
      "grad_norm": 3.148240089416504,
      "learning_rate": 2.0303611935089866e-05,
      "loss": 2.8159,
      "step": 17120
    },
    {
      "epoch": 1.7882107459572247,
      "grad_norm": 3.790785551071167,
      "learning_rate": 2.0268714011516317e-05,
      "loss": 2.5124,
      "step": 17140
    },
    {
      "epoch": 1.7902973395931143,
      "grad_norm": 3.939913749694824,
      "learning_rate": 2.023381608794277e-05,
      "loss": 2.6764,
      "step": 17160
    },
    {
      "epoch": 1.7923839332290037,
      "grad_norm": 3.613281011581421,
      "learning_rate": 2.019891816436922e-05,
      "loss": 2.6585,
      "step": 17180
    },
    {
      "epoch": 1.794470526864893,
      "grad_norm": 3.964282274246216,
      "learning_rate": 2.016402024079567e-05,
      "loss": 2.7767,
      "step": 17200
    },
    {
      "epoch": 1.7965571205007824,
      "grad_norm": 3.7655608654022217,
      "learning_rate": 2.0129122317222126e-05,
      "loss": 2.6299,
      "step": 17220
    },
    {
      "epoch": 1.7986437141366718,
      "grad_norm": 4.4782304763793945,
      "learning_rate": 2.0094224393648578e-05,
      "loss": 2.7136,
      "step": 17240
    },
    {
      "epoch": 1.8007303077725614,
      "grad_norm": 3.6836814880371094,
      "learning_rate": 2.0059326470075033e-05,
      "loss": 2.7109,
      "step": 17260
    },
    {
      "epoch": 1.8028169014084507,
      "grad_norm": 3.614584445953369,
      "learning_rate": 2.0024428546501484e-05,
      "loss": 2.6608,
      "step": 17280
    },
    {
      "epoch": 1.8049034950443401,
      "grad_norm": 4.121825695037842,
      "learning_rate": 1.998953062292794e-05,
      "loss": 2.7083,
      "step": 17300
    },
    {
      "epoch": 1.8069900886802295,
      "grad_norm": 4.409968376159668,
      "learning_rate": 1.995463269935439e-05,
      "loss": 2.7869,
      "step": 17320
    },
    {
      "epoch": 1.8090766823161188,
      "grad_norm": 3.584740161895752,
      "learning_rate": 1.9919734775780842e-05,
      "loss": 2.6603,
      "step": 17340
    },
    {
      "epoch": 1.8111632759520084,
      "grad_norm": 3.565507650375366,
      "learning_rate": 1.9884836852207294e-05,
      "loss": 2.6976,
      "step": 17360
    },
    {
      "epoch": 1.8132498695878978,
      "grad_norm": 3.8915786743164062,
      "learning_rate": 1.9849938928633745e-05,
      "loss": 2.6764,
      "step": 17380
    },
    {
      "epoch": 1.8153364632237872,
      "grad_norm": 3.528573989868164,
      "learning_rate": 1.98150410050602e-05,
      "loss": 2.667,
      "step": 17400
    },
    {
      "epoch": 1.8174230568596765,
      "grad_norm": 3.999694585800171,
      "learning_rate": 1.978014308148665e-05,
      "loss": 2.8156,
      "step": 17420
    },
    {
      "epoch": 1.819509650495566,
      "grad_norm": 5.5181403160095215,
      "learning_rate": 1.9745245157913106e-05,
      "loss": 2.6612,
      "step": 17440
    },
    {
      "epoch": 1.8215962441314555,
      "grad_norm": 3.4497146606445312,
      "learning_rate": 1.9710347234339558e-05,
      "loss": 2.7908,
      "step": 17460
    },
    {
      "epoch": 1.8236828377673449,
      "grad_norm": 3.423795461654663,
      "learning_rate": 1.9675449310766013e-05,
      "loss": 2.6759,
      "step": 17480
    },
    {
      "epoch": 1.8257694314032342,
      "grad_norm": 4.522756099700928,
      "learning_rate": 1.9640551387192464e-05,
      "loss": 2.7258,
      "step": 17500
    },
    {
      "epoch": 1.8278560250391236,
      "grad_norm": 5.387201309204102,
      "learning_rate": 1.9605653463618916e-05,
      "loss": 2.6269,
      "step": 17520
    },
    {
      "epoch": 1.829942618675013,
      "grad_norm": 3.6808042526245117,
      "learning_rate": 1.9570755540045367e-05,
      "loss": 2.7384,
      "step": 17540
    },
    {
      "epoch": 1.8320292123109025,
      "grad_norm": 4.099954605102539,
      "learning_rate": 1.953585761647182e-05,
      "loss": 2.7378,
      "step": 17560
    },
    {
      "epoch": 1.834115805946792,
      "grad_norm": 3.6755549907684326,
      "learning_rate": 1.9500959692898273e-05,
      "loss": 2.7455,
      "step": 17580
    },
    {
      "epoch": 1.8362023995826813,
      "grad_norm": 3.7047500610351562,
      "learning_rate": 1.9466061769324725e-05,
      "loss": 2.7488,
      "step": 17600
    },
    {
      "epoch": 1.8382889932185706,
      "grad_norm": 3.789778470993042,
      "learning_rate": 1.943116384575118e-05,
      "loss": 2.6567,
      "step": 17620
    },
    {
      "epoch": 1.84037558685446,
      "grad_norm": 4.277783393859863,
      "learning_rate": 1.939626592217763e-05,
      "loss": 2.7331,
      "step": 17640
    },
    {
      "epoch": 1.8424621804903496,
      "grad_norm": 4.205923080444336,
      "learning_rate": 1.9361367998604086e-05,
      "loss": 2.726,
      "step": 17660
    },
    {
      "epoch": 1.844548774126239,
      "grad_norm": 3.7212822437286377,
      "learning_rate": 1.9326470075030538e-05,
      "loss": 2.7137,
      "step": 17680
    },
    {
      "epoch": 1.8466353677621283,
      "grad_norm": 3.9738540649414062,
      "learning_rate": 1.929157215145699e-05,
      "loss": 2.7138,
      "step": 17700
    },
    {
      "epoch": 1.8487219613980177,
      "grad_norm": 4.966969966888428,
      "learning_rate": 1.925667422788344e-05,
      "loss": 2.7502,
      "step": 17720
    },
    {
      "epoch": 1.850808555033907,
      "grad_norm": 3.6816458702087402,
      "learning_rate": 1.9221776304309892e-05,
      "loss": 2.7339,
      "step": 17740
    },
    {
      "epoch": 1.8528951486697967,
      "grad_norm": 3.7049458026885986,
      "learning_rate": 1.9186878380736347e-05,
      "loss": 2.7126,
      "step": 17760
    },
    {
      "epoch": 1.854981742305686,
      "grad_norm": 3.5238285064697266,
      "learning_rate": 1.91519804571628e-05,
      "loss": 2.6668,
      "step": 17780
    },
    {
      "epoch": 1.8570683359415754,
      "grad_norm": 4.0513739585876465,
      "learning_rate": 1.9117082533589253e-05,
      "loss": 2.7504,
      "step": 17800
    },
    {
      "epoch": 1.8591549295774648,
      "grad_norm": 4.854464530944824,
      "learning_rate": 1.9082184610015705e-05,
      "loss": 2.716,
      "step": 17820
    },
    {
      "epoch": 1.8612415232133541,
      "grad_norm": 4.879127502441406,
      "learning_rate": 1.904728668644216e-05,
      "loss": 2.5869,
      "step": 17840
    },
    {
      "epoch": 1.8633281168492437,
      "grad_norm": 3.6870980262756348,
      "learning_rate": 1.901238876286861e-05,
      "loss": 2.6203,
      "step": 17860
    },
    {
      "epoch": 1.8654147104851329,
      "grad_norm": 3.8444747924804688,
      "learning_rate": 1.8977490839295063e-05,
      "loss": 2.6485,
      "step": 17880
    },
    {
      "epoch": 1.8675013041210224,
      "grad_norm": 4.006354808807373,
      "learning_rate": 1.8942592915721517e-05,
      "loss": 2.7328,
      "step": 17900
    },
    {
      "epoch": 1.8695878977569118,
      "grad_norm": 4.502716064453125,
      "learning_rate": 1.890769499214797e-05,
      "loss": 2.6615,
      "step": 17920
    },
    {
      "epoch": 1.8716744913928012,
      "grad_norm": 3.7983481884002686,
      "learning_rate": 1.887279706857442e-05,
      "loss": 2.6527,
      "step": 17940
    },
    {
      "epoch": 1.8737610850286908,
      "grad_norm": 4.430079936981201,
      "learning_rate": 1.8837899145000872e-05,
      "loss": 2.8345,
      "step": 17960
    },
    {
      "epoch": 1.87584767866458,
      "grad_norm": 4.037495136260986,
      "learning_rate": 1.8803001221427327e-05,
      "loss": 2.6627,
      "step": 17980
    },
    {
      "epoch": 1.8779342723004695,
      "grad_norm": 3.738022804260254,
      "learning_rate": 1.8768103297853778e-05,
      "loss": 2.6624,
      "step": 18000
    },
    {
      "epoch": 1.8800208659363589,
      "grad_norm": 3.928182601928711,
      "learning_rate": 1.873320537428023e-05,
      "loss": 2.7707,
      "step": 18020
    },
    {
      "epoch": 1.8821074595722482,
      "grad_norm": 3.5252254009246826,
      "learning_rate": 1.8698307450706685e-05,
      "loss": 2.6026,
      "step": 18040
    },
    {
      "epoch": 1.8841940532081378,
      "grad_norm": 4.505651473999023,
      "learning_rate": 1.8663409527133136e-05,
      "loss": 2.6247,
      "step": 18060
    },
    {
      "epoch": 1.886280646844027,
      "grad_norm": 3.892444133758545,
      "learning_rate": 1.862851160355959e-05,
      "loss": 2.6463,
      "step": 18080
    },
    {
      "epoch": 1.8883672404799166,
      "grad_norm": 3.8354289531707764,
      "learning_rate": 1.8593613679986042e-05,
      "loss": 2.7139,
      "step": 18100
    },
    {
      "epoch": 1.890453834115806,
      "grad_norm": 3.8326363563537598,
      "learning_rate": 1.8558715756412494e-05,
      "loss": 2.7041,
      "step": 18120
    },
    {
      "epoch": 1.8925404277516953,
      "grad_norm": 4.4171552658081055,
      "learning_rate": 1.8523817832838945e-05,
      "loss": 2.7706,
      "step": 18140
    },
    {
      "epoch": 1.8946270213875849,
      "grad_norm": 4.410014629364014,
      "learning_rate": 1.84889199092654e-05,
      "loss": 2.6723,
      "step": 18160
    },
    {
      "epoch": 1.896713615023474,
      "grad_norm": 3.5127604007720947,
      "learning_rate": 1.8454021985691852e-05,
      "loss": 2.6496,
      "step": 18180
    },
    {
      "epoch": 1.8988002086593636,
      "grad_norm": 5.388107776641846,
      "learning_rate": 1.8419124062118303e-05,
      "loss": 2.7097,
      "step": 18200
    },
    {
      "epoch": 1.900886802295253,
      "grad_norm": 4.054825782775879,
      "learning_rate": 1.8384226138544758e-05,
      "loss": 2.7012,
      "step": 18220
    },
    {
      "epoch": 1.9029733959311423,
      "grad_norm": 3.5508787631988525,
      "learning_rate": 1.834932821497121e-05,
      "loss": 2.6769,
      "step": 18240
    },
    {
      "epoch": 1.905059989567032,
      "grad_norm": 3.0046846866607666,
      "learning_rate": 1.8314430291397664e-05,
      "loss": 2.7111,
      "step": 18260
    },
    {
      "epoch": 1.907146583202921,
      "grad_norm": 3.496819019317627,
      "learning_rate": 1.8279532367824116e-05,
      "loss": 2.677,
      "step": 18280
    },
    {
      "epoch": 1.9092331768388107,
      "grad_norm": 3.7419960498809814,
      "learning_rate": 1.8244634444250567e-05,
      "loss": 2.7476,
      "step": 18300
    },
    {
      "epoch": 1.9113197704747,
      "grad_norm": 4.648678779602051,
      "learning_rate": 1.820973652067702e-05,
      "loss": 2.8103,
      "step": 18320
    },
    {
      "epoch": 1.9134063641105894,
      "grad_norm": 4.117313385009766,
      "learning_rate": 1.8174838597103474e-05,
      "loss": 2.6606,
      "step": 18340
    },
    {
      "epoch": 1.915492957746479,
      "grad_norm": 3.7074615955352783,
      "learning_rate": 1.8139940673529925e-05,
      "loss": 2.745,
      "step": 18360
    },
    {
      "epoch": 1.9175795513823681,
      "grad_norm": 3.7437918186187744,
      "learning_rate": 1.8105042749956377e-05,
      "loss": 2.7318,
      "step": 18380
    },
    {
      "epoch": 1.9196661450182577,
      "grad_norm": 3.841876745223999,
      "learning_rate": 1.807014482638283e-05,
      "loss": 2.6281,
      "step": 18400
    },
    {
      "epoch": 1.921752738654147,
      "grad_norm": 3.296434164047241,
      "learning_rate": 1.8035246902809283e-05,
      "loss": 2.6775,
      "step": 18420
    },
    {
      "epoch": 1.9238393322900365,
      "grad_norm": 4.468271732330322,
      "learning_rate": 1.8000348979235738e-05,
      "loss": 2.6992,
      "step": 18440
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 3.253541946411133,
      "learning_rate": 1.796545105566219e-05,
      "loss": 2.7253,
      "step": 18460
    },
    {
      "epoch": 1.9280125195618152,
      "grad_norm": 4.5665154457092285,
      "learning_rate": 1.7930553132088644e-05,
      "loss": 2.6466,
      "step": 18480
    },
    {
      "epoch": 1.9300991131977048,
      "grad_norm": 4.061086654663086,
      "learning_rate": 1.7895655208515096e-05,
      "loss": 2.6844,
      "step": 18500
    },
    {
      "epoch": 1.9321857068335941,
      "grad_norm": 3.5721869468688965,
      "learning_rate": 1.7860757284941547e-05,
      "loss": 2.6831,
      "step": 18520
    },
    {
      "epoch": 1.9342723004694835,
      "grad_norm": 4.039945602416992,
      "learning_rate": 1.7825859361368e-05,
      "loss": 2.699,
      "step": 18540
    },
    {
      "epoch": 1.936358894105373,
      "grad_norm": 4.483156681060791,
      "learning_rate": 1.779096143779445e-05,
      "loss": 2.6978,
      "step": 18560
    },
    {
      "epoch": 1.9384454877412622,
      "grad_norm": 3.1790454387664795,
      "learning_rate": 1.7756063514220905e-05,
      "loss": 2.6302,
      "step": 18580
    },
    {
      "epoch": 1.9405320813771518,
      "grad_norm": 4.137099742889404,
      "learning_rate": 1.7721165590647357e-05,
      "loss": 2.567,
      "step": 18600
    },
    {
      "epoch": 1.9426186750130412,
      "grad_norm": 4.108007907867432,
      "learning_rate": 1.768626766707381e-05,
      "loss": 2.6717,
      "step": 18620
    },
    {
      "epoch": 1.9447052686489306,
      "grad_norm": 4.090353488922119,
      "learning_rate": 1.7651369743500263e-05,
      "loss": 2.6511,
      "step": 18640
    },
    {
      "epoch": 1.9467918622848202,
      "grad_norm": 3.340315341949463,
      "learning_rate": 1.7616471819926718e-05,
      "loss": 2.6811,
      "step": 18660
    },
    {
      "epoch": 1.9488784559207093,
      "grad_norm": 3.8429558277130127,
      "learning_rate": 1.758157389635317e-05,
      "loss": 2.73,
      "step": 18680
    },
    {
      "epoch": 1.9509650495565989,
      "grad_norm": 3.7868945598602295,
      "learning_rate": 1.754667597277962e-05,
      "loss": 2.7141,
      "step": 18700
    },
    {
      "epoch": 1.9530516431924883,
      "grad_norm": 5.099379539489746,
      "learning_rate": 1.7511778049206072e-05,
      "loss": 2.7133,
      "step": 18720
    },
    {
      "epoch": 1.9551382368283776,
      "grad_norm": 4.639618873596191,
      "learning_rate": 1.7476880125632524e-05,
      "loss": 2.8833,
      "step": 18740
    },
    {
      "epoch": 1.9572248304642672,
      "grad_norm": 3.537332773208618,
      "learning_rate": 1.744198220205898e-05,
      "loss": 2.7061,
      "step": 18760
    },
    {
      "epoch": 1.9593114241001564,
      "grad_norm": 3.661484479904175,
      "learning_rate": 1.740708427848543e-05,
      "loss": 2.6435,
      "step": 18780
    },
    {
      "epoch": 1.961398017736046,
      "grad_norm": 3.570408821105957,
      "learning_rate": 1.7372186354911885e-05,
      "loss": 2.581,
      "step": 18800
    },
    {
      "epoch": 1.9634846113719353,
      "grad_norm": 4.447778701782227,
      "learning_rate": 1.7337288431338336e-05,
      "loss": 2.6982,
      "step": 18820
    },
    {
      "epoch": 1.9655712050078247,
      "grad_norm": 4.930878639221191,
      "learning_rate": 1.730239050776479e-05,
      "loss": 2.7783,
      "step": 18840
    },
    {
      "epoch": 1.9676577986437143,
      "grad_norm": 3.5762250423431396,
      "learning_rate": 1.7267492584191243e-05,
      "loss": 2.7703,
      "step": 18860
    },
    {
      "epoch": 1.9697443922796034,
      "grad_norm": 3.3746590614318848,
      "learning_rate": 1.7232594660617694e-05,
      "loss": 2.7241,
      "step": 18880
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 4.673812389373779,
      "learning_rate": 1.7197696737044146e-05,
      "loss": 2.6409,
      "step": 18900
    },
    {
      "epoch": 1.9739175795513824,
      "grad_norm": 3.6656620502471924,
      "learning_rate": 1.7162798813470597e-05,
      "loss": 2.6859,
      "step": 18920
    },
    {
      "epoch": 1.9760041731872717,
      "grad_norm": 3.7366857528686523,
      "learning_rate": 1.7127900889897052e-05,
      "loss": 2.6727,
      "step": 18940
    },
    {
      "epoch": 1.9780907668231613,
      "grad_norm": 3.2133562564849854,
      "learning_rate": 1.7093002966323504e-05,
      "loss": 2.7397,
      "step": 18960
    },
    {
      "epoch": 1.9801773604590505,
      "grad_norm": 3.1637120246887207,
      "learning_rate": 1.705810504274996e-05,
      "loss": 2.6665,
      "step": 18980
    },
    {
      "epoch": 1.98226395409494,
      "grad_norm": 4.2900776863098145,
      "learning_rate": 1.702320711917641e-05,
      "loss": 2.6931,
      "step": 19000
    },
    {
      "epoch": 1.9843505477308294,
      "grad_norm": 3.3485279083251953,
      "learning_rate": 1.698830919560286e-05,
      "loss": 2.7808,
      "step": 19020
    },
    {
      "epoch": 1.9864371413667188,
      "grad_norm": 3.6667115688323975,
      "learning_rate": 1.6953411272029316e-05,
      "loss": 2.6992,
      "step": 19040
    },
    {
      "epoch": 1.9885237350026084,
      "grad_norm": 3.77217173576355,
      "learning_rate": 1.6918513348455768e-05,
      "loss": 2.5724,
      "step": 19060
    },
    {
      "epoch": 1.9906103286384975,
      "grad_norm": 4.156249046325684,
      "learning_rate": 1.6883615424882223e-05,
      "loss": 2.6883,
      "step": 19080
    },
    {
      "epoch": 1.992696922274387,
      "grad_norm": 4.145449161529541,
      "learning_rate": 1.684871750130867e-05,
      "loss": 2.7708,
      "step": 19100
    },
    {
      "epoch": 1.9947835159102765,
      "grad_norm": 3.7093613147735596,
      "learning_rate": 1.6813819577735126e-05,
      "loss": 2.7038,
      "step": 19120
    },
    {
      "epoch": 1.9968701095461658,
      "grad_norm": 3.725473642349243,
      "learning_rate": 1.6778921654161577e-05,
      "loss": 2.6991,
      "step": 19140
    },
    {
      "epoch": 1.9989567031820554,
      "grad_norm": 3.1136372089385986,
      "learning_rate": 1.6744023730588032e-05,
      "loss": 2.7914,
      "step": 19160
    },
    {
      "epoch": 2.0010432968179446,
      "grad_norm": 4.056649684906006,
      "learning_rate": 1.6709125807014483e-05,
      "loss": 2.7437,
      "step": 19180
    },
    {
      "epoch": 2.003129890453834,
      "grad_norm": 3.9367241859436035,
      "learning_rate": 1.6674227883440935e-05,
      "loss": 2.5825,
      "step": 19200
    },
    {
      "epoch": 2.0052164840897233,
      "grad_norm": 3.5383875370025635,
      "learning_rate": 1.663932995986739e-05,
      "loss": 2.5007,
      "step": 19220
    },
    {
      "epoch": 2.007303077725613,
      "grad_norm": 3.694690227508545,
      "learning_rate": 1.660443203629384e-05,
      "loss": 2.5714,
      "step": 19240
    },
    {
      "epoch": 2.0093896713615025,
      "grad_norm": 3.666653633117676,
      "learning_rate": 1.6569534112720296e-05,
      "loss": 2.7023,
      "step": 19260
    },
    {
      "epoch": 2.0114762649973916,
      "grad_norm": 3.5538182258605957,
      "learning_rate": 1.6534636189146748e-05,
      "loss": 2.5423,
      "step": 19280
    },
    {
      "epoch": 2.013562858633281,
      "grad_norm": 6.292622089385986,
      "learning_rate": 1.64997382655732e-05,
      "loss": 2.5461,
      "step": 19300
    },
    {
      "epoch": 2.0156494522691704,
      "grad_norm": 4.514675140380859,
      "learning_rate": 1.646484034199965e-05,
      "loss": 2.597,
      "step": 19320
    },
    {
      "epoch": 2.01773604590506,
      "grad_norm": 3.9320011138916016,
      "learning_rate": 1.6429942418426105e-05,
      "loss": 2.6056,
      "step": 19340
    },
    {
      "epoch": 2.0198226395409495,
      "grad_norm": 4.347386837005615,
      "learning_rate": 1.6395044494852557e-05,
      "loss": 2.4792,
      "step": 19360
    },
    {
      "epoch": 2.0219092331768387,
      "grad_norm": 4.063209056854248,
      "learning_rate": 1.636014657127901e-05,
      "loss": 2.5272,
      "step": 19380
    },
    {
      "epoch": 2.0239958268127283,
      "grad_norm": 4.002066612243652,
      "learning_rate": 1.6325248647705463e-05,
      "loss": 2.4994,
      "step": 19400
    },
    {
      "epoch": 2.0260824204486174,
      "grad_norm": 4.143770217895508,
      "learning_rate": 1.6290350724131915e-05,
      "loss": 2.5856,
      "step": 19420
    },
    {
      "epoch": 2.028169014084507,
      "grad_norm": 3.772077798843384,
      "learning_rate": 1.625545280055837e-05,
      "loss": 2.4873,
      "step": 19440
    },
    {
      "epoch": 2.0302556077203966,
      "grad_norm": 3.301798105239868,
      "learning_rate": 1.622055487698482e-05,
      "loss": 2.481,
      "step": 19460
    },
    {
      "epoch": 2.0323422013562857,
      "grad_norm": 3.1812705993652344,
      "learning_rate": 1.6185656953411273e-05,
      "loss": 2.4776,
      "step": 19480
    },
    {
      "epoch": 2.0344287949921753,
      "grad_norm": 3.8758797645568848,
      "learning_rate": 1.6150759029837724e-05,
      "loss": 2.5145,
      "step": 19500
    },
    {
      "epoch": 2.0365153886280645,
      "grad_norm": 3.7349367141723633,
      "learning_rate": 1.6115861106264175e-05,
      "loss": 2.5191,
      "step": 19520
    },
    {
      "epoch": 2.038601982263954,
      "grad_norm": 3.772280693054199,
      "learning_rate": 1.608096318269063e-05,
      "loss": 2.4734,
      "step": 19540
    },
    {
      "epoch": 2.0406885758998436,
      "grad_norm": 4.710294246673584,
      "learning_rate": 1.6046065259117082e-05,
      "loss": 2.5679,
      "step": 19560
    },
    {
      "epoch": 2.042775169535733,
      "grad_norm": 6.265488147735596,
      "learning_rate": 1.6011167335543537e-05,
      "loss": 2.5802,
      "step": 19580
    },
    {
      "epoch": 2.0448617631716224,
      "grad_norm": 4.090734958648682,
      "learning_rate": 1.5976269411969988e-05,
      "loss": 2.4925,
      "step": 19600
    },
    {
      "epoch": 2.0469483568075115,
      "grad_norm": 3.65594744682312,
      "learning_rate": 1.5941371488396443e-05,
      "loss": 2.5403,
      "step": 19620
    },
    {
      "epoch": 2.049034950443401,
      "grad_norm": 3.6477272510528564,
      "learning_rate": 1.5906473564822895e-05,
      "loss": 2.5037,
      "step": 19640
    },
    {
      "epoch": 2.0511215440792907,
      "grad_norm": 4.443326950073242,
      "learning_rate": 1.5871575641249346e-05,
      "loss": 2.5221,
      "step": 19660
    },
    {
      "epoch": 2.05320813771518,
      "grad_norm": 3.9308359622955322,
      "learning_rate": 1.5836677717675797e-05,
      "loss": 2.4558,
      "step": 19680
    },
    {
      "epoch": 2.0552947313510694,
      "grad_norm": 4.028308868408203,
      "learning_rate": 1.580177979410225e-05,
      "loss": 2.5478,
      "step": 19700
    },
    {
      "epoch": 2.0573813249869586,
      "grad_norm": 4.265520095825195,
      "learning_rate": 1.5766881870528704e-05,
      "loss": 2.6047,
      "step": 19720
    },
    {
      "epoch": 2.059467918622848,
      "grad_norm": 3.8805124759674072,
      "learning_rate": 1.5731983946955155e-05,
      "loss": 2.522,
      "step": 19740
    },
    {
      "epoch": 2.0615545122587378,
      "grad_norm": 3.5739874839782715,
      "learning_rate": 1.569708602338161e-05,
      "loss": 2.4566,
      "step": 19760
    },
    {
      "epoch": 2.063641105894627,
      "grad_norm": 4.2285614013671875,
      "learning_rate": 1.566218809980806e-05,
      "loss": 2.647,
      "step": 19780
    },
    {
      "epoch": 2.0657276995305165,
      "grad_norm": 4.11470365524292,
      "learning_rate": 1.5627290176234517e-05,
      "loss": 2.5936,
      "step": 19800
    },
    {
      "epoch": 2.0678142931664056,
      "grad_norm": 3.5145087242126465,
      "learning_rate": 1.5592392252660968e-05,
      "loss": 2.5658,
      "step": 19820
    },
    {
      "epoch": 2.0699008868022952,
      "grad_norm": 4.043701171875,
      "learning_rate": 1.555749432908742e-05,
      "loss": 2.4889,
      "step": 19840
    },
    {
      "epoch": 2.071987480438185,
      "grad_norm": 4.122781276702881,
      "learning_rate": 1.5522596405513874e-05,
      "loss": 2.5056,
      "step": 19860
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 4.69913911819458,
      "learning_rate": 1.5487698481940326e-05,
      "loss": 2.5223,
      "step": 19880
    },
    {
      "epoch": 2.0761606677099635,
      "grad_norm": 4.201854705810547,
      "learning_rate": 1.5452800558366777e-05,
      "loss": 2.5092,
      "step": 19900
    },
    {
      "epoch": 2.0782472613458527,
      "grad_norm": 4.558180332183838,
      "learning_rate": 1.541790263479323e-05,
      "loss": 2.6188,
      "step": 19920
    },
    {
      "epoch": 2.0803338549817423,
      "grad_norm": 3.791048526763916,
      "learning_rate": 1.5383004711219684e-05,
      "loss": 2.4999,
      "step": 19940
    },
    {
      "epoch": 2.082420448617632,
      "grad_norm": 3.6147282123565674,
      "learning_rate": 1.5348106787646135e-05,
      "loss": 2.6004,
      "step": 19960
    },
    {
      "epoch": 2.084507042253521,
      "grad_norm": 4.000856876373291,
      "learning_rate": 1.531320886407259e-05,
      "loss": 2.5533,
      "step": 19980
    },
    {
      "epoch": 2.0865936358894106,
      "grad_norm": 4.822998046875,
      "learning_rate": 1.527831094049904e-05,
      "loss": 2.5041,
      "step": 20000
    },
    {
      "epoch": 2.0886802295252997,
      "grad_norm": 4.985659122467041,
      "learning_rate": 1.5243413016925493e-05,
      "loss": 2.5477,
      "step": 20020
    },
    {
      "epoch": 2.0907668231611893,
      "grad_norm": 3.9807920455932617,
      "learning_rate": 1.5208515093351946e-05,
      "loss": 2.6129,
      "step": 20040
    },
    {
      "epoch": 2.092853416797079,
      "grad_norm": 4.440083026885986,
      "learning_rate": 1.5173617169778398e-05,
      "loss": 2.4824,
      "step": 20060
    },
    {
      "epoch": 2.094940010432968,
      "grad_norm": 3.76346755027771,
      "learning_rate": 1.5138719246204853e-05,
      "loss": 2.5089,
      "step": 20080
    },
    {
      "epoch": 2.0970266040688577,
      "grad_norm": 5.504209518432617,
      "learning_rate": 1.5103821322631304e-05,
      "loss": 2.6604,
      "step": 20100
    },
    {
      "epoch": 2.099113197704747,
      "grad_norm": 4.881521701812744,
      "learning_rate": 1.5068923399057757e-05,
      "loss": 2.5054,
      "step": 20120
    },
    {
      "epoch": 2.1011997913406364,
      "grad_norm": 4.983552932739258,
      "learning_rate": 1.5034025475484209e-05,
      "loss": 2.5649,
      "step": 20140
    },
    {
      "epoch": 2.103286384976526,
      "grad_norm": 4.121748447418213,
      "learning_rate": 1.4999127551910664e-05,
      "loss": 2.5211,
      "step": 20160
    },
    {
      "epoch": 2.105372978612415,
      "grad_norm": 4.15126371383667,
      "learning_rate": 1.4964229628337115e-05,
      "loss": 2.6191,
      "step": 20180
    },
    {
      "epoch": 2.1074595722483047,
      "grad_norm": 3.9817025661468506,
      "learning_rate": 1.4929331704763566e-05,
      "loss": 2.5559,
      "step": 20200
    },
    {
      "epoch": 2.109546165884194,
      "grad_norm": 4.961371421813965,
      "learning_rate": 1.489443378119002e-05,
      "loss": 2.6042,
      "step": 20220
    },
    {
      "epoch": 2.1116327595200834,
      "grad_norm": 4.985083103179932,
      "learning_rate": 1.4859535857616471e-05,
      "loss": 2.531,
      "step": 20240
    },
    {
      "epoch": 2.113719353155973,
      "grad_norm": 4.227967262268066,
      "learning_rate": 1.4824637934042926e-05,
      "loss": 2.4924,
      "step": 20260
    },
    {
      "epoch": 2.115805946791862,
      "grad_norm": 4.593385219573975,
      "learning_rate": 1.4789740010469377e-05,
      "loss": 2.4754,
      "step": 20280
    },
    {
      "epoch": 2.1178925404277518,
      "grad_norm": 3.363424777984619,
      "learning_rate": 1.475484208689583e-05,
      "loss": 2.5434,
      "step": 20300
    },
    {
      "epoch": 2.119979134063641,
      "grad_norm": 4.618465423583984,
      "learning_rate": 1.4719944163322282e-05,
      "loss": 2.5857,
      "step": 20320
    },
    {
      "epoch": 2.1220657276995305,
      "grad_norm": 3.7277207374572754,
      "learning_rate": 1.4685046239748734e-05,
      "loss": 2.559,
      "step": 20340
    },
    {
      "epoch": 2.12415232133542,
      "grad_norm": 5.489431858062744,
      "learning_rate": 1.4650148316175189e-05,
      "loss": 2.5829,
      "step": 20360
    },
    {
      "epoch": 2.1262389149713092,
      "grad_norm": 4.493313789367676,
      "learning_rate": 1.461525039260164e-05,
      "loss": 2.6345,
      "step": 20380
    },
    {
      "epoch": 2.128325508607199,
      "grad_norm": 3.3998019695281982,
      "learning_rate": 1.4580352469028095e-05,
      "loss": 2.5059,
      "step": 20400
    },
    {
      "epoch": 2.130412102243088,
      "grad_norm": 4.940708160400391,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 2.5787,
      "step": 20420
    },
    {
      "epoch": 2.1324986958789776,
      "grad_norm": 3.659799575805664,
      "learning_rate": 1.4510556621881e-05,
      "loss": 2.5725,
      "step": 20440
    },
    {
      "epoch": 2.134585289514867,
      "grad_norm": 4.327925205230713,
      "learning_rate": 1.4475658698307451e-05,
      "loss": 2.427,
      "step": 20460
    },
    {
      "epoch": 2.1366718831507563,
      "grad_norm": 3.7159955501556396,
      "learning_rate": 1.4440760774733906e-05,
      "loss": 2.5731,
      "step": 20480
    },
    {
      "epoch": 2.138758476786646,
      "grad_norm": 4.321867942810059,
      "learning_rate": 1.4405862851160357e-05,
      "loss": 2.5335,
      "step": 20500
    },
    {
      "epoch": 2.140845070422535,
      "grad_norm": 4.2409868240356445,
      "learning_rate": 1.4370964927586809e-05,
      "loss": 2.5525,
      "step": 20520
    },
    {
      "epoch": 2.1429316640584246,
      "grad_norm": 4.12758207321167,
      "learning_rate": 1.4336067004013262e-05,
      "loss": 2.5802,
      "step": 20540
    },
    {
      "epoch": 2.145018257694314,
      "grad_norm": 3.74623441696167,
      "learning_rate": 1.4301169080439713e-05,
      "loss": 2.5329,
      "step": 20560
    },
    {
      "epoch": 2.1471048513302033,
      "grad_norm": 4.060650825500488,
      "learning_rate": 1.4266271156866168e-05,
      "loss": 2.5905,
      "step": 20580
    },
    {
      "epoch": 2.149191444966093,
      "grad_norm": 4.260141372680664,
      "learning_rate": 1.423137323329262e-05,
      "loss": 2.659,
      "step": 20600
    },
    {
      "epoch": 2.151278038601982,
      "grad_norm": 4.273487091064453,
      "learning_rate": 1.4196475309719073e-05,
      "loss": 2.6414,
      "step": 20620
    },
    {
      "epoch": 2.1533646322378717,
      "grad_norm": 3.751404047012329,
      "learning_rate": 1.4161577386145524e-05,
      "loss": 2.5556,
      "step": 20640
    },
    {
      "epoch": 2.1554512258737613,
      "grad_norm": 4.438665866851807,
      "learning_rate": 1.412667946257198e-05,
      "loss": 2.5312,
      "step": 20660
    },
    {
      "epoch": 2.1575378195096504,
      "grad_norm": 3.58754563331604,
      "learning_rate": 1.409178153899843e-05,
      "loss": 2.6322,
      "step": 20680
    },
    {
      "epoch": 2.15962441314554,
      "grad_norm": 4.537868976593018,
      "learning_rate": 1.4056883615424882e-05,
      "loss": 2.5494,
      "step": 20700
    },
    {
      "epoch": 2.161711006781429,
      "grad_norm": 3.9832353591918945,
      "learning_rate": 1.4021985691851335e-05,
      "loss": 2.4532,
      "step": 20720
    },
    {
      "epoch": 2.1637976004173187,
      "grad_norm": 4.488011360168457,
      "learning_rate": 1.3987087768277787e-05,
      "loss": 2.5648,
      "step": 20740
    },
    {
      "epoch": 2.1658841940532083,
      "grad_norm": 3.4699761867523193,
      "learning_rate": 1.3952189844704242e-05,
      "loss": 2.5903,
      "step": 20760
    },
    {
      "epoch": 2.1679707876890975,
      "grad_norm": 3.677583694458008,
      "learning_rate": 1.3917291921130693e-05,
      "loss": 2.4664,
      "step": 20780
    },
    {
      "epoch": 2.170057381324987,
      "grad_norm": 3.78527569770813,
      "learning_rate": 1.3882393997557146e-05,
      "loss": 2.5393,
      "step": 20800
    },
    {
      "epoch": 2.172143974960876,
      "grad_norm": 6.010016441345215,
      "learning_rate": 1.3847496073983598e-05,
      "loss": 2.5148,
      "step": 20820
    },
    {
      "epoch": 2.174230568596766,
      "grad_norm": 4.2545318603515625,
      "learning_rate": 1.381259815041005e-05,
      "loss": 2.558,
      "step": 20840
    },
    {
      "epoch": 2.1763171622326554,
      "grad_norm": 3.7350282669067383,
      "learning_rate": 1.3777700226836504e-05,
      "loss": 2.5384,
      "step": 20860
    },
    {
      "epoch": 2.1784037558685445,
      "grad_norm": 3.744590997695923,
      "learning_rate": 1.3742802303262956e-05,
      "loss": 2.5224,
      "step": 20880
    },
    {
      "epoch": 2.180490349504434,
      "grad_norm": 3.8821001052856445,
      "learning_rate": 1.3707904379689409e-05,
      "loss": 2.4652,
      "step": 20900
    },
    {
      "epoch": 2.1825769431403232,
      "grad_norm": 4.4145731925964355,
      "learning_rate": 1.367300645611586e-05,
      "loss": 2.5307,
      "step": 20920
    },
    {
      "epoch": 2.184663536776213,
      "grad_norm": 3.3738510608673096,
      "learning_rate": 1.3638108532542315e-05,
      "loss": 2.5557,
      "step": 20940
    },
    {
      "epoch": 2.1867501304121024,
      "grad_norm": 4.331995487213135,
      "learning_rate": 1.3603210608968767e-05,
      "loss": 2.4974,
      "step": 20960
    },
    {
      "epoch": 2.1888367240479916,
      "grad_norm": 3.9416894912719727,
      "learning_rate": 1.356831268539522e-05,
      "loss": 2.4574,
      "step": 20980
    },
    {
      "epoch": 2.190923317683881,
      "grad_norm": 4.250840663909912,
      "learning_rate": 1.3533414761821671e-05,
      "loss": 2.561,
      "step": 21000
    },
    {
      "epoch": 2.1930099113197703,
      "grad_norm": 4.327929496765137,
      "learning_rate": 1.3498516838248123e-05,
      "loss": 2.4811,
      "step": 21020
    },
    {
      "epoch": 2.19509650495566,
      "grad_norm": 4.189966201782227,
      "learning_rate": 1.3463618914674578e-05,
      "loss": 2.5013,
      "step": 21040
    },
    {
      "epoch": 2.1971830985915495,
      "grad_norm": 4.59001350402832,
      "learning_rate": 1.342872099110103e-05,
      "loss": 2.5188,
      "step": 21060
    },
    {
      "epoch": 2.1992696922274386,
      "grad_norm": 3.261721134185791,
      "learning_rate": 1.3393823067527484e-05,
      "loss": 2.4683,
      "step": 21080
    },
    {
      "epoch": 2.201356285863328,
      "grad_norm": 5.6058759689331055,
      "learning_rate": 1.3358925143953934e-05,
      "loss": 2.5083,
      "step": 21100
    },
    {
      "epoch": 2.2034428794992174,
      "grad_norm": 4.250865936279297,
      "learning_rate": 1.3324027220380389e-05,
      "loss": 2.572,
      "step": 21120
    },
    {
      "epoch": 2.205529473135107,
      "grad_norm": 3.772814989089966,
      "learning_rate": 1.328912929680684e-05,
      "loss": 2.4653,
      "step": 21140
    },
    {
      "epoch": 2.2076160667709965,
      "grad_norm": 3.4500699043273926,
      "learning_rate": 1.3254231373233295e-05,
      "loss": 2.5843,
      "step": 21160
    },
    {
      "epoch": 2.2097026604068857,
      "grad_norm": 3.5749011039733887,
      "learning_rate": 1.3219333449659747e-05,
      "loss": 2.4839,
      "step": 21180
    },
    {
      "epoch": 2.2117892540427753,
      "grad_norm": 4.324774265289307,
      "learning_rate": 1.3184435526086198e-05,
      "loss": 2.5813,
      "step": 21200
    },
    {
      "epoch": 2.2138758476786644,
      "grad_norm": 3.96677565574646,
      "learning_rate": 1.3149537602512651e-05,
      "loss": 2.5685,
      "step": 21220
    },
    {
      "epoch": 2.215962441314554,
      "grad_norm": 4.210107326507568,
      "learning_rate": 1.3114639678939103e-05,
      "loss": 2.512,
      "step": 21240
    },
    {
      "epoch": 2.2180490349504436,
      "grad_norm": 4.054569244384766,
      "learning_rate": 1.3079741755365558e-05,
      "loss": 2.6567,
      "step": 21260
    },
    {
      "epoch": 2.2201356285863327,
      "grad_norm": 3.937621831893921,
      "learning_rate": 1.3044843831792009e-05,
      "loss": 2.6403,
      "step": 21280
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 3.8539721965789795,
      "learning_rate": 1.3009945908218462e-05,
      "loss": 2.5184,
      "step": 21300
    },
    {
      "epoch": 2.2243088158581115,
      "grad_norm": 4.232360363006592,
      "learning_rate": 1.2975047984644914e-05,
      "loss": 2.6099,
      "step": 21320
    },
    {
      "epoch": 2.226395409494001,
      "grad_norm": 3.996065139770508,
      "learning_rate": 1.2940150061071365e-05,
      "loss": 2.5597,
      "step": 21340
    },
    {
      "epoch": 2.2284820031298906,
      "grad_norm": 3.7878670692443848,
      "learning_rate": 1.290525213749782e-05,
      "loss": 2.5017,
      "step": 21360
    },
    {
      "epoch": 2.23056859676578,
      "grad_norm": 3.8590776920318604,
      "learning_rate": 1.2870354213924272e-05,
      "loss": 2.5036,
      "step": 21380
    },
    {
      "epoch": 2.2326551904016694,
      "grad_norm": 4.521339416503906,
      "learning_rate": 1.2835456290350725e-05,
      "loss": 2.6077,
      "step": 21400
    },
    {
      "epoch": 2.2347417840375585,
      "grad_norm": 4.5102667808532715,
      "learning_rate": 1.2800558366777176e-05,
      "loss": 2.6042,
      "step": 21420
    },
    {
      "epoch": 2.236828377673448,
      "grad_norm": 4.104746341705322,
      "learning_rate": 1.2765660443203631e-05,
      "loss": 2.534,
      "step": 21440
    },
    {
      "epoch": 2.2389149713093377,
      "grad_norm": 3.6141715049743652,
      "learning_rate": 1.2730762519630083e-05,
      "loss": 2.5923,
      "step": 21460
    },
    {
      "epoch": 2.241001564945227,
      "grad_norm": 3.996860980987549,
      "learning_rate": 1.2695864596056536e-05,
      "loss": 2.6795,
      "step": 21480
    },
    {
      "epoch": 2.2430881585811164,
      "grad_norm": 3.7950680255889893,
      "learning_rate": 1.2660966672482987e-05,
      "loss": 2.5086,
      "step": 21500
    },
    {
      "epoch": 2.2451747522170056,
      "grad_norm": 4.693943977355957,
      "learning_rate": 1.2626068748909439e-05,
      "loss": 2.6303,
      "step": 21520
    },
    {
      "epoch": 2.247261345852895,
      "grad_norm": 4.897477149963379,
      "learning_rate": 1.2591170825335894e-05,
      "loss": 2.5587,
      "step": 21540
    },
    {
      "epoch": 2.2493479394887848,
      "grad_norm": 3.925151824951172,
      "learning_rate": 1.2556272901762345e-05,
      "loss": 2.5235,
      "step": 21560
    },
    {
      "epoch": 2.251434533124674,
      "grad_norm": 4.6982741355896,
      "learning_rate": 1.2521374978188798e-05,
      "loss": 2.4493,
      "step": 21580
    },
    {
      "epoch": 2.2535211267605635,
      "grad_norm": 4.369913101196289,
      "learning_rate": 1.248647705461525e-05,
      "loss": 2.6291,
      "step": 21600
    },
    {
      "epoch": 2.2556077203964526,
      "grad_norm": 4.0758185386657715,
      "learning_rate": 1.2451579131041703e-05,
      "loss": 2.6493,
      "step": 21620
    },
    {
      "epoch": 2.257694314032342,
      "grad_norm": 5.072996616363525,
      "learning_rate": 1.2416681207468156e-05,
      "loss": 2.549,
      "step": 21640
    },
    {
      "epoch": 2.2597809076682314,
      "grad_norm": 3.5931334495544434,
      "learning_rate": 1.238178328389461e-05,
      "loss": 2.6122,
      "step": 21660
    },
    {
      "epoch": 2.261867501304121,
      "grad_norm": 4.072222709655762,
      "learning_rate": 1.234688536032106e-05,
      "loss": 2.5228,
      "step": 21680
    },
    {
      "epoch": 2.2639540949400105,
      "grad_norm": 4.509871959686279,
      "learning_rate": 1.2311987436747514e-05,
      "loss": 2.6006,
      "step": 21700
    },
    {
      "epoch": 2.2660406885758997,
      "grad_norm": 3.9246201515197754,
      "learning_rate": 1.2277089513173967e-05,
      "loss": 2.5267,
      "step": 21720
    },
    {
      "epoch": 2.2681272822117893,
      "grad_norm": 5.203069686889648,
      "learning_rate": 1.224219158960042e-05,
      "loss": 2.4822,
      "step": 21740
    },
    {
      "epoch": 2.270213875847679,
      "grad_norm": 3.7168078422546387,
      "learning_rate": 1.2207293666026872e-05,
      "loss": 2.5476,
      "step": 21760
    },
    {
      "epoch": 2.272300469483568,
      "grad_norm": 3.497467517852783,
      "learning_rate": 1.2172395742453325e-05,
      "loss": 2.521,
      "step": 21780
    },
    {
      "epoch": 2.2743870631194576,
      "grad_norm": 4.4908928871154785,
      "learning_rate": 1.2137497818879776e-05,
      "loss": 2.5632,
      "step": 21800
    },
    {
      "epoch": 2.2764736567553467,
      "grad_norm": 4.130624294281006,
      "learning_rate": 1.210259989530623e-05,
      "loss": 2.5271,
      "step": 21820
    },
    {
      "epoch": 2.2785602503912363,
      "grad_norm": 4.386298179626465,
      "learning_rate": 1.2067701971732683e-05,
      "loss": 2.4532,
      "step": 21840
    },
    {
      "epoch": 2.2806468440271255,
      "grad_norm": 4.023198127746582,
      "learning_rate": 1.2032804048159136e-05,
      "loss": 2.6055,
      "step": 21860
    },
    {
      "epoch": 2.282733437663015,
      "grad_norm": 3.940556049346924,
      "learning_rate": 1.1997906124585587e-05,
      "loss": 2.626,
      "step": 21880
    },
    {
      "epoch": 2.2848200312989047,
      "grad_norm": 4.189990043640137,
      "learning_rate": 1.196300820101204e-05,
      "loss": 2.5276,
      "step": 21900
    },
    {
      "epoch": 2.286906624934794,
      "grad_norm": 4.694396495819092,
      "learning_rate": 1.1928110277438492e-05,
      "loss": 2.5613,
      "step": 21920
    },
    {
      "epoch": 2.2889932185706834,
      "grad_norm": 4.654135227203369,
      "learning_rate": 1.1893212353864945e-05,
      "loss": 2.4233,
      "step": 21940
    },
    {
      "epoch": 2.291079812206573,
      "grad_norm": 4.136350631713867,
      "learning_rate": 1.1858314430291398e-05,
      "loss": 2.5318,
      "step": 21960
    },
    {
      "epoch": 2.293166405842462,
      "grad_norm": 3.7633931636810303,
      "learning_rate": 1.182341650671785e-05,
      "loss": 2.5999,
      "step": 21980
    },
    {
      "epoch": 2.2952529994783517,
      "grad_norm": 3.8602230548858643,
      "learning_rate": 1.1788518583144303e-05,
      "loss": 2.4458,
      "step": 22000
    },
    {
      "epoch": 2.297339593114241,
      "grad_norm": 4.011709213256836,
      "learning_rate": 1.1753620659570756e-05,
      "loss": 2.5465,
      "step": 22020
    },
    {
      "epoch": 2.2994261867501304,
      "grad_norm": 4.365503787994385,
      "learning_rate": 1.171872273599721e-05,
      "loss": 2.5625,
      "step": 22040
    },
    {
      "epoch": 2.3015127803860196,
      "grad_norm": 3.5832505226135254,
      "learning_rate": 1.1683824812423663e-05,
      "loss": 2.5604,
      "step": 22060
    },
    {
      "epoch": 2.303599374021909,
      "grad_norm": 4.980339050292969,
      "learning_rate": 1.1648926888850114e-05,
      "loss": 2.5954,
      "step": 22080
    },
    {
      "epoch": 2.3056859676577988,
      "grad_norm": 4.2293572425842285,
      "learning_rate": 1.1614028965276566e-05,
      "loss": 2.5338,
      "step": 22100
    },
    {
      "epoch": 2.307772561293688,
      "grad_norm": 3.874427556991577,
      "learning_rate": 1.1579131041703019e-05,
      "loss": 2.5169,
      "step": 22120
    },
    {
      "epoch": 2.3098591549295775,
      "grad_norm": 3.780090570449829,
      "learning_rate": 1.1544233118129472e-05,
      "loss": 2.5007,
      "step": 22140
    },
    {
      "epoch": 2.311945748565467,
      "grad_norm": 4.180276870727539,
      "learning_rate": 1.1509335194555925e-05,
      "loss": 2.5573,
      "step": 22160
    },
    {
      "epoch": 2.3140323422013562,
      "grad_norm": 3.9466986656188965,
      "learning_rate": 1.1474437270982377e-05,
      "loss": 2.5459,
      "step": 22180
    },
    {
      "epoch": 2.316118935837246,
      "grad_norm": 5.1548919677734375,
      "learning_rate": 1.143953934740883e-05,
      "loss": 2.6145,
      "step": 22200
    },
    {
      "epoch": 2.318205529473135,
      "grad_norm": 3.5161964893341064,
      "learning_rate": 1.1404641423835283e-05,
      "loss": 2.6049,
      "step": 22220
    },
    {
      "epoch": 2.3202921231090246,
      "grad_norm": 4.351737976074219,
      "learning_rate": 1.1369743500261736e-05,
      "loss": 2.6569,
      "step": 22240
    },
    {
      "epoch": 2.3223787167449137,
      "grad_norm": 4.443028926849365,
      "learning_rate": 1.1334845576688188e-05,
      "loss": 2.5102,
      "step": 22260
    },
    {
      "epoch": 2.3244653103808033,
      "grad_norm": 4.150625228881836,
      "learning_rate": 1.1299947653114639e-05,
      "loss": 2.5585,
      "step": 22280
    },
    {
      "epoch": 2.326551904016693,
      "grad_norm": 4.606986999511719,
      "learning_rate": 1.1265049729541092e-05,
      "loss": 2.5954,
      "step": 22300
    },
    {
      "epoch": 2.328638497652582,
      "grad_norm": 4.9204535484313965,
      "learning_rate": 1.1230151805967545e-05,
      "loss": 2.5659,
      "step": 22320
    },
    {
      "epoch": 2.3307250912884716,
      "grad_norm": 4.0807342529296875,
      "learning_rate": 1.1195253882393999e-05,
      "loss": 2.6178,
      "step": 22340
    },
    {
      "epoch": 2.332811684924361,
      "grad_norm": 4.230457782745361,
      "learning_rate": 1.116035595882045e-05,
      "loss": 2.5203,
      "step": 22360
    },
    {
      "epoch": 2.3348982785602503,
      "grad_norm": 4.018174171447754,
      "learning_rate": 1.1125458035246903e-05,
      "loss": 2.5308,
      "step": 22380
    },
    {
      "epoch": 2.33698487219614,
      "grad_norm": 4.312243461608887,
      "learning_rate": 1.1090560111673356e-05,
      "loss": 2.5684,
      "step": 22400
    },
    {
      "epoch": 2.339071465832029,
      "grad_norm": 3.629882574081421,
      "learning_rate": 1.1055662188099808e-05,
      "loss": 2.5395,
      "step": 22420
    },
    {
      "epoch": 2.3411580594679187,
      "grad_norm": 4.601108074188232,
      "learning_rate": 1.1020764264526261e-05,
      "loss": 2.5456,
      "step": 22440
    },
    {
      "epoch": 2.343244653103808,
      "grad_norm": 3.9702069759368896,
      "learning_rate": 1.0985866340952714e-05,
      "loss": 2.5564,
      "step": 22460
    },
    {
      "epoch": 2.3453312467396974,
      "grad_norm": 4.565975189208984,
      "learning_rate": 1.0950968417379166e-05,
      "loss": 2.5338,
      "step": 22480
    },
    {
      "epoch": 2.347417840375587,
      "grad_norm": 4.037290573120117,
      "learning_rate": 1.0916070493805619e-05,
      "loss": 2.5296,
      "step": 22500
    },
    {
      "epoch": 2.349504434011476,
      "grad_norm": 3.748384714126587,
      "learning_rate": 1.0881172570232072e-05,
      "loss": 2.4963,
      "step": 22520
    },
    {
      "epoch": 2.3515910276473657,
      "grad_norm": 4.069034099578857,
      "learning_rate": 1.0846274646658525e-05,
      "loss": 2.6535,
      "step": 22540
    },
    {
      "epoch": 2.3536776212832553,
      "grad_norm": 4.383304595947266,
      "learning_rate": 1.0811376723084977e-05,
      "loss": 2.4604,
      "step": 22560
    },
    {
      "epoch": 2.3557642149191445,
      "grad_norm": 3.874375581741333,
      "learning_rate": 1.077647879951143e-05,
      "loss": 2.587,
      "step": 22580
    },
    {
      "epoch": 2.357850808555034,
      "grad_norm": 4.9295878410339355,
      "learning_rate": 1.0741580875937881e-05,
      "loss": 2.6941,
      "step": 22600
    },
    {
      "epoch": 2.359937402190923,
      "grad_norm": 5.051862716674805,
      "learning_rate": 1.0706682952364335e-05,
      "loss": 2.5568,
      "step": 22620
    },
    {
      "epoch": 2.3620239958268128,
      "grad_norm": 4.311367511749268,
      "learning_rate": 1.0671785028790788e-05,
      "loss": 2.6454,
      "step": 22640
    },
    {
      "epoch": 2.364110589462702,
      "grad_norm": 3.668412446975708,
      "learning_rate": 1.063688710521724e-05,
      "loss": 2.6251,
      "step": 22660
    },
    {
      "epoch": 2.3661971830985915,
      "grad_norm": 4.0638909339904785,
      "learning_rate": 1.0601989181643692e-05,
      "loss": 2.5287,
      "step": 22680
    },
    {
      "epoch": 2.368283776734481,
      "grad_norm": 4.307244300842285,
      "learning_rate": 1.0567091258070146e-05,
      "loss": 2.5124,
      "step": 22700
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 4.010771751403809,
      "learning_rate": 1.0532193334496599e-05,
      "loss": 2.5336,
      "step": 22720
    },
    {
      "epoch": 2.37245696400626,
      "grad_norm": 4.534318923950195,
      "learning_rate": 1.0497295410923052e-05,
      "loss": 2.5207,
      "step": 22740
    },
    {
      "epoch": 2.3745435576421494,
      "grad_norm": 3.541860342025757,
      "learning_rate": 1.0462397487349502e-05,
      "loss": 2.4727,
      "step": 22760
    },
    {
      "epoch": 2.3766301512780386,
      "grad_norm": 3.773244857788086,
      "learning_rate": 1.0427499563775955e-05,
      "loss": 2.4999,
      "step": 22780
    },
    {
      "epoch": 2.378716744913928,
      "grad_norm": 3.409550189971924,
      "learning_rate": 1.0392601640202408e-05,
      "loss": 2.507,
      "step": 22800
    },
    {
      "epoch": 2.3808033385498173,
      "grad_norm": 3.9289989471435547,
      "learning_rate": 1.0357703716628861e-05,
      "loss": 2.5856,
      "step": 22820
    },
    {
      "epoch": 2.382889932185707,
      "grad_norm": 4.536839962005615,
      "learning_rate": 1.0322805793055314e-05,
      "loss": 2.4984,
      "step": 22840
    },
    {
      "epoch": 2.384976525821596,
      "grad_norm": 3.8284752368927,
      "learning_rate": 1.0287907869481766e-05,
      "loss": 2.5885,
      "step": 22860
    },
    {
      "epoch": 2.3870631194574856,
      "grad_norm": 3.369544506072998,
      "learning_rate": 1.0253009945908219e-05,
      "loss": 2.5583,
      "step": 22880
    },
    {
      "epoch": 2.389149713093375,
      "grad_norm": 4.202390670776367,
      "learning_rate": 1.0218112022334672e-05,
      "loss": 2.4543,
      "step": 22900
    },
    {
      "epoch": 2.3912363067292643,
      "grad_norm": 3.596743583679199,
      "learning_rate": 1.0183214098761124e-05,
      "loss": 2.4422,
      "step": 22920
    },
    {
      "epoch": 2.393322900365154,
      "grad_norm": 4.132856369018555,
      "learning_rate": 1.0148316175187577e-05,
      "loss": 2.4797,
      "step": 22940
    },
    {
      "epoch": 2.3954094940010435,
      "grad_norm": 3.726489782333374,
      "learning_rate": 1.0113418251614028e-05,
      "loss": 2.6649,
      "step": 22960
    },
    {
      "epoch": 2.3974960876369327,
      "grad_norm": 4.190402984619141,
      "learning_rate": 1.0078520328040482e-05,
      "loss": 2.5836,
      "step": 22980
    },
    {
      "epoch": 2.3995826812728223,
      "grad_norm": 3.960270881652832,
      "learning_rate": 1.0043622404466935e-05,
      "loss": 2.4726,
      "step": 23000
    },
    {
      "epoch": 2.4016692749087114,
      "grad_norm": 4.141024589538574,
      "learning_rate": 1.0008724480893388e-05,
      "loss": 2.5634,
      "step": 23020
    },
    {
      "epoch": 2.403755868544601,
      "grad_norm": 4.820784568786621,
      "learning_rate": 9.973826557319841e-06,
      "loss": 2.6214,
      "step": 23040
    },
    {
      "epoch": 2.40584246218049,
      "grad_norm": 4.043966293334961,
      "learning_rate": 9.938928633746293e-06,
      "loss": 2.5754,
      "step": 23060
    },
    {
      "epoch": 2.4079290558163797,
      "grad_norm": 3.9703421592712402,
      "learning_rate": 9.904030710172746e-06,
      "loss": 2.5743,
      "step": 23080
    },
    {
      "epoch": 2.4100156494522693,
      "grad_norm": 4.040903568267822,
      "learning_rate": 9.869132786599197e-06,
      "loss": 2.5422,
      "step": 23100
    },
    {
      "epoch": 2.4121022430881585,
      "grad_norm": 4.143942356109619,
      "learning_rate": 9.83423486302565e-06,
      "loss": 2.6084,
      "step": 23120
    },
    {
      "epoch": 2.414188836724048,
      "grad_norm": 3.4746944904327393,
      "learning_rate": 9.799336939452104e-06,
      "loss": 2.6167,
      "step": 23140
    },
    {
      "epoch": 2.4162754303599376,
      "grad_norm": 4.630892753601074,
      "learning_rate": 9.764439015878555e-06,
      "loss": 2.5583,
      "step": 23160
    },
    {
      "epoch": 2.418362023995827,
      "grad_norm": 4.205109596252441,
      "learning_rate": 9.729541092305008e-06,
      "loss": 2.5349,
      "step": 23180
    },
    {
      "epoch": 2.4204486176317164,
      "grad_norm": 4.575005054473877,
      "learning_rate": 9.694643168731461e-06,
      "loss": 2.5125,
      "step": 23200
    },
    {
      "epoch": 2.4225352112676055,
      "grad_norm": 4.701319694519043,
      "learning_rate": 9.659745245157915e-06,
      "loss": 2.5182,
      "step": 23220
    },
    {
      "epoch": 2.424621804903495,
      "grad_norm": 3.8066582679748535,
      "learning_rate": 9.624847321584366e-06,
      "loss": 2.5209,
      "step": 23240
    },
    {
      "epoch": 2.4267083985393842,
      "grad_norm": 4.181232452392578,
      "learning_rate": 9.589949398010818e-06,
      "loss": 2.5087,
      "step": 23260
    },
    {
      "epoch": 2.428794992175274,
      "grad_norm": 4.498927593231201,
      "learning_rate": 9.55505147443727e-06,
      "loss": 2.6002,
      "step": 23280
    },
    {
      "epoch": 2.4308815858111634,
      "grad_norm": 3.6623246669769287,
      "learning_rate": 9.520153550863724e-06,
      "loss": 2.5819,
      "step": 23300
    },
    {
      "epoch": 2.4329681794470526,
      "grad_norm": 3.8173272609710693,
      "learning_rate": 9.485255627290177e-06,
      "loss": 2.5678,
      "step": 23320
    },
    {
      "epoch": 2.435054773082942,
      "grad_norm": 3.791278839111328,
      "learning_rate": 9.450357703716629e-06,
      "loss": 2.6269,
      "step": 23340
    },
    {
      "epoch": 2.4371413667188317,
      "grad_norm": 3.8043041229248047,
      "learning_rate": 9.415459780143082e-06,
      "loss": 2.5354,
      "step": 23360
    },
    {
      "epoch": 2.439227960354721,
      "grad_norm": 4.743374824523926,
      "learning_rate": 9.380561856569535e-06,
      "loss": 2.5101,
      "step": 23380
    },
    {
      "epoch": 2.4413145539906105,
      "grad_norm": 5.112143039703369,
      "learning_rate": 9.345663932995988e-06,
      "loss": 2.5931,
      "step": 23400
    },
    {
      "epoch": 2.4434011476264996,
      "grad_norm": 4.065608978271484,
      "learning_rate": 9.31076600942244e-06,
      "loss": 2.4935,
      "step": 23420
    },
    {
      "epoch": 2.445487741262389,
      "grad_norm": 5.1252827644348145,
      "learning_rate": 9.275868085848893e-06,
      "loss": 2.5714,
      "step": 23440
    },
    {
      "epoch": 2.4475743348982784,
      "grad_norm": 5.63790225982666,
      "learning_rate": 9.240970162275344e-06,
      "loss": 2.4401,
      "step": 23460
    },
    {
      "epoch": 2.449660928534168,
      "grad_norm": 4.3535966873168945,
      "learning_rate": 9.206072238701797e-06,
      "loss": 2.5828,
      "step": 23480
    },
    {
      "epoch": 2.4517475221700575,
      "grad_norm": 4.331467628479004,
      "learning_rate": 9.17117431512825e-06,
      "loss": 2.4458,
      "step": 23500
    },
    {
      "epoch": 2.4538341158059467,
      "grad_norm": 4.872981071472168,
      "learning_rate": 9.136276391554704e-06,
      "loss": 2.6062,
      "step": 23520
    },
    {
      "epoch": 2.4559207094418363,
      "grad_norm": 3.840763568878174,
      "learning_rate": 9.101378467981155e-06,
      "loss": 2.5212,
      "step": 23540
    },
    {
      "epoch": 2.458007303077726,
      "grad_norm": 4.311958312988281,
      "learning_rate": 9.066480544407608e-06,
      "loss": 2.5897,
      "step": 23560
    },
    {
      "epoch": 2.460093896713615,
      "grad_norm": 4.905377388000488,
      "learning_rate": 9.031582620834062e-06,
      "loss": 2.567,
      "step": 23580
    },
    {
      "epoch": 2.4621804903495046,
      "grad_norm": 4.697936534881592,
      "learning_rate": 8.996684697260513e-06,
      "loss": 2.5444,
      "step": 23600
    },
    {
      "epoch": 2.4642670839853937,
      "grad_norm": 4.105935573577881,
      "learning_rate": 8.961786773686966e-06,
      "loss": 2.5437,
      "step": 23620
    },
    {
      "epoch": 2.4663536776212833,
      "grad_norm": 5.549560070037842,
      "learning_rate": 8.926888850113418e-06,
      "loss": 2.5483,
      "step": 23640
    },
    {
      "epoch": 2.4684402712571725,
      "grad_norm": 5.194908618927002,
      "learning_rate": 8.891990926539871e-06,
      "loss": 2.4919,
      "step": 23660
    },
    {
      "epoch": 2.470526864893062,
      "grad_norm": 4.208420753479004,
      "learning_rate": 8.857093002966324e-06,
      "loss": 2.6562,
      "step": 23680
    },
    {
      "epoch": 2.4726134585289516,
      "grad_norm": 4.924654960632324,
      "learning_rate": 8.822195079392777e-06,
      "loss": 2.6195,
      "step": 23700
    },
    {
      "epoch": 2.474700052164841,
      "grad_norm": 3.5425596237182617,
      "learning_rate": 8.78729715581923e-06,
      "loss": 2.5349,
      "step": 23720
    },
    {
      "epoch": 2.4767866458007304,
      "grad_norm": 4.144535541534424,
      "learning_rate": 8.752399232245682e-06,
      "loss": 2.6389,
      "step": 23740
    },
    {
      "epoch": 2.4788732394366195,
      "grad_norm": 5.354536056518555,
      "learning_rate": 8.717501308672133e-06,
      "loss": 2.4893,
      "step": 23760
    },
    {
      "epoch": 2.480959833072509,
      "grad_norm": 4.520798683166504,
      "learning_rate": 8.682603385098587e-06,
      "loss": 2.4799,
      "step": 23780
    },
    {
      "epoch": 2.4830464267083987,
      "grad_norm": 3.824387550354004,
      "learning_rate": 8.64770546152504e-06,
      "loss": 2.5635,
      "step": 23800
    },
    {
      "epoch": 2.485133020344288,
      "grad_norm": 4.2494378089904785,
      "learning_rate": 8.612807537951493e-06,
      "loss": 2.517,
      "step": 23820
    },
    {
      "epoch": 2.4872196139801774,
      "grad_norm": 3.7951135635375977,
      "learning_rate": 8.577909614377944e-06,
      "loss": 2.5641,
      "step": 23840
    },
    {
      "epoch": 2.4893062076160666,
      "grad_norm": 4.248316764831543,
      "learning_rate": 8.543011690804398e-06,
      "loss": 2.555,
      "step": 23860
    },
    {
      "epoch": 2.491392801251956,
      "grad_norm": 5.471393585205078,
      "learning_rate": 8.50811376723085e-06,
      "loss": 2.5563,
      "step": 23880
    },
    {
      "epoch": 2.4934793948878458,
      "grad_norm": 3.9071202278137207,
      "learning_rate": 8.473215843657304e-06,
      "loss": 2.593,
      "step": 23900
    },
    {
      "epoch": 2.495565988523735,
      "grad_norm": 4.224759578704834,
      "learning_rate": 8.438317920083755e-06,
      "loss": 2.6507,
      "step": 23920
    },
    {
      "epoch": 2.4976525821596245,
      "grad_norm": 4.922282695770264,
      "learning_rate": 8.403419996510207e-06,
      "loss": 2.622,
      "step": 23940
    },
    {
      "epoch": 2.4997391757955136,
      "grad_norm": 3.3208882808685303,
      "learning_rate": 8.36852207293666e-06,
      "loss": 2.5409,
      "step": 23960
    },
    {
      "epoch": 2.5018257694314032,
      "grad_norm": 4.376823425292969,
      "learning_rate": 8.333624149363113e-06,
      "loss": 2.6046,
      "step": 23980
    },
    {
      "epoch": 2.5039123630672924,
      "grad_norm": 3.4410197734832764,
      "learning_rate": 8.298726225789566e-06,
      "loss": 2.5181,
      "step": 24000
    },
    {
      "epoch": 2.505998956703182,
      "grad_norm": 4.856719970703125,
      "learning_rate": 8.263828302216018e-06,
      "loss": 2.5988,
      "step": 24020
    },
    {
      "epoch": 2.5080855503390715,
      "grad_norm": 4.144157886505127,
      "learning_rate": 8.228930378642471e-06,
      "loss": 2.598,
      "step": 24040
    },
    {
      "epoch": 2.5101721439749607,
      "grad_norm": 3.6038875579833984,
      "learning_rate": 8.194032455068924e-06,
      "loss": 2.5249,
      "step": 24060
    },
    {
      "epoch": 2.5122587376108503,
      "grad_norm": 4.495243072509766,
      "learning_rate": 8.159134531495377e-06,
      "loss": 2.5456,
      "step": 24080
    },
    {
      "epoch": 2.51434533124674,
      "grad_norm": 4.449002265930176,
      "learning_rate": 8.124236607921829e-06,
      "loss": 2.5929,
      "step": 24100
    },
    {
      "epoch": 2.516431924882629,
      "grad_norm": 4.506199836730957,
      "learning_rate": 8.089338684348282e-06,
      "loss": 2.6255,
      "step": 24120
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 4.0176615715026855,
      "learning_rate": 8.054440760774734e-06,
      "loss": 2.6984,
      "step": 24140
    },
    {
      "epoch": 2.520605112154408,
      "grad_norm": 4.2436017990112305,
      "learning_rate": 8.019542837201187e-06,
      "loss": 2.5181,
      "step": 24160
    },
    {
      "epoch": 2.5226917057902973,
      "grad_norm": 3.8336873054504395,
      "learning_rate": 7.98464491362764e-06,
      "loss": 2.5762,
      "step": 24180
    },
    {
      "epoch": 2.5247782994261865,
      "grad_norm": 3.6272408962249756,
      "learning_rate": 7.949746990054093e-06,
      "loss": 2.4828,
      "step": 24200
    },
    {
      "epoch": 2.526864893062076,
      "grad_norm": 4.785990238189697,
      "learning_rate": 7.914849066480545e-06,
      "loss": 2.5364,
      "step": 24220
    },
    {
      "epoch": 2.5289514866979657,
      "grad_norm": 4.447965145111084,
      "learning_rate": 7.879951142906998e-06,
      "loss": 2.6151,
      "step": 24240
    },
    {
      "epoch": 2.531038080333855,
      "grad_norm": 4.848264217376709,
      "learning_rate": 7.84505321933345e-06,
      "loss": 2.5403,
      "step": 24260
    },
    {
      "epoch": 2.5331246739697444,
      "grad_norm": 3.802832841873169,
      "learning_rate": 7.810155295759902e-06,
      "loss": 2.4467,
      "step": 24280
    },
    {
      "epoch": 2.535211267605634,
      "grad_norm": 3.964871883392334,
      "learning_rate": 7.775257372186356e-06,
      "loss": 2.5512,
      "step": 24300
    },
    {
      "epoch": 2.537297861241523,
      "grad_norm": 4.017346382141113,
      "learning_rate": 7.740359448612807e-06,
      "loss": 2.5215,
      "step": 24320
    },
    {
      "epoch": 2.5393844548774127,
      "grad_norm": 4.62153434753418,
      "learning_rate": 7.70546152503926e-06,
      "loss": 2.4606,
      "step": 24340
    },
    {
      "epoch": 2.5414710485133023,
      "grad_norm": 4.702916145324707,
      "learning_rate": 7.670563601465713e-06,
      "loss": 2.4237,
      "step": 24360
    },
    {
      "epoch": 2.5435576421491914,
      "grad_norm": 4.2271270751953125,
      "learning_rate": 7.635665677892167e-06,
      "loss": 2.4926,
      "step": 24380
    },
    {
      "epoch": 2.5456442357850806,
      "grad_norm": 3.8040506839752197,
      "learning_rate": 7.600767754318619e-06,
      "loss": 2.6019,
      "step": 24400
    },
    {
      "epoch": 2.54773082942097,
      "grad_norm": 5.245677471160889,
      "learning_rate": 7.56586983074507e-06,
      "loss": 2.5048,
      "step": 24420
    },
    {
      "epoch": 2.5498174230568598,
      "grad_norm": 4.474211692810059,
      "learning_rate": 7.5309719071715236e-06,
      "loss": 2.5215,
      "step": 24440
    },
    {
      "epoch": 2.551904016692749,
      "grad_norm": 4.572597026824951,
      "learning_rate": 7.496073983597976e-06,
      "loss": 2.5213,
      "step": 24460
    },
    {
      "epoch": 2.5539906103286385,
      "grad_norm": 3.560106039047241,
      "learning_rate": 7.461176060024429e-06,
      "loss": 2.5352,
      "step": 24480
    },
    {
      "epoch": 2.556077203964528,
      "grad_norm": 3.9186301231384277,
      "learning_rate": 7.426278136450881e-06,
      "loss": 2.4614,
      "step": 24500
    },
    {
      "epoch": 2.5581637976004172,
      "grad_norm": 4.096772193908691,
      "learning_rate": 7.3913802128773346e-06,
      "loss": 2.6545,
      "step": 24520
    },
    {
      "epoch": 2.560250391236307,
      "grad_norm": 3.5955705642700195,
      "learning_rate": 7.356482289303787e-06,
      "loss": 2.6033,
      "step": 24540
    },
    {
      "epoch": 2.5623369848721964,
      "grad_norm": 4.25027322769165,
      "learning_rate": 7.32158436573024e-06,
      "loss": 2.5981,
      "step": 24560
    },
    {
      "epoch": 2.5644235785080856,
      "grad_norm": 3.5499515533447266,
      "learning_rate": 7.286686442156692e-06,
      "loss": 2.5709,
      "step": 24580
    },
    {
      "epoch": 2.5665101721439747,
      "grad_norm": 3.43392014503479,
      "learning_rate": 7.251788518583144e-06,
      "loss": 2.5352,
      "step": 24600
    },
    {
      "epoch": 2.5685967657798643,
      "grad_norm": 4.540950775146484,
      "learning_rate": 7.216890595009597e-06,
      "loss": 2.5952,
      "step": 24620
    },
    {
      "epoch": 2.570683359415754,
      "grad_norm": 3.9833524227142334,
      "learning_rate": 7.181992671436049e-06,
      "loss": 2.4912,
      "step": 24640
    },
    {
      "epoch": 2.572769953051643,
      "grad_norm": 4.212133407592773,
      "learning_rate": 7.1470947478625026e-06,
      "loss": 2.5385,
      "step": 24660
    },
    {
      "epoch": 2.5748565466875326,
      "grad_norm": 3.726086139678955,
      "learning_rate": 7.112196824288955e-06,
      "loss": 2.6184,
      "step": 24680
    },
    {
      "epoch": 2.576943140323422,
      "grad_norm": 4.571599006652832,
      "learning_rate": 7.077298900715408e-06,
      "loss": 2.4609,
      "step": 24700
    },
    {
      "epoch": 2.5790297339593113,
      "grad_norm": 3.9382872581481934,
      "learning_rate": 7.042400977141861e-06,
      "loss": 2.5083,
      "step": 24720
    },
    {
      "epoch": 2.581116327595201,
      "grad_norm": 5.660054683685303,
      "learning_rate": 7.0075030535683136e-06,
      "loss": 2.5793,
      "step": 24740
    },
    {
      "epoch": 2.5832029212310905,
      "grad_norm": 3.7461979389190674,
      "learning_rate": 6.972605129994765e-06,
      "loss": 2.5313,
      "step": 24760
    },
    {
      "epoch": 2.5852895148669797,
      "grad_norm": 3.951392650604248,
      "learning_rate": 6.937707206421218e-06,
      "loss": 2.6289,
      "step": 24780
    },
    {
      "epoch": 2.587376108502869,
      "grad_norm": 5.004421710968018,
      "learning_rate": 6.9028092828476706e-06,
      "loss": 2.4867,
      "step": 24800
    },
    {
      "epoch": 2.5894627021387584,
      "grad_norm": 3.8379154205322266,
      "learning_rate": 6.867911359274124e-06,
      "loss": 2.6586,
      "step": 24820
    },
    {
      "epoch": 2.591549295774648,
      "grad_norm": 3.8543081283569336,
      "learning_rate": 6.833013435700576e-06,
      "loss": 2.5719,
      "step": 24840
    },
    {
      "epoch": 2.593635889410537,
      "grad_norm": 4.070662498474121,
      "learning_rate": 6.798115512127029e-06,
      "loss": 2.5618,
      "step": 24860
    },
    {
      "epoch": 2.5957224830464267,
      "grad_norm": 3.757103681564331,
      "learning_rate": 6.7632175885534816e-06,
      "loss": 2.5755,
      "step": 24880
    },
    {
      "epoch": 2.5978090766823163,
      "grad_norm": 4.297637939453125,
      "learning_rate": 6.728319664979935e-06,
      "loss": 2.4587,
      "step": 24900
    },
    {
      "epoch": 2.5998956703182055,
      "grad_norm": 6.169604778289795,
      "learning_rate": 6.693421741406386e-06,
      "loss": 2.5388,
      "step": 24920
    },
    {
      "epoch": 2.601982263954095,
      "grad_norm": 3.8016107082366943,
      "learning_rate": 6.6585238178328385e-06,
      "loss": 2.5721,
      "step": 24940
    },
    {
      "epoch": 2.604068857589984,
      "grad_norm": 5.091359615325928,
      "learning_rate": 6.623625894259292e-06,
      "loss": 2.5622,
      "step": 24960
    },
    {
      "epoch": 2.6061554512258738,
      "grad_norm": 4.127537727355957,
      "learning_rate": 6.588727970685744e-06,
      "loss": 2.5397,
      "step": 24980
    },
    {
      "epoch": 2.608242044861763,
      "grad_norm": 3.9347119331359863,
      "learning_rate": 6.553830047112197e-06,
      "loss": 2.647,
      "step": 25000
    },
    {
      "epoch": 2.6103286384976525,
      "grad_norm": 4.266389846801758,
      "learning_rate": 6.5189321235386496e-06,
      "loss": 2.5268,
      "step": 25020
    },
    {
      "epoch": 2.612415232133542,
      "grad_norm": 3.93501877784729,
      "learning_rate": 6.484034199965103e-06,
      "loss": 2.5528,
      "step": 25040
    },
    {
      "epoch": 2.6145018257694312,
      "grad_norm": 4.336010456085205,
      "learning_rate": 6.449136276391556e-06,
      "loss": 2.5589,
      "step": 25060
    },
    {
      "epoch": 2.616588419405321,
      "grad_norm": 3.618414878845215,
      "learning_rate": 6.4142383528180065e-06,
      "loss": 2.6351,
      "step": 25080
    },
    {
      "epoch": 2.6186750130412104,
      "grad_norm": 3.7442209720611572,
      "learning_rate": 6.37934042924446e-06,
      "loss": 2.6084,
      "step": 25100
    },
    {
      "epoch": 2.6207616066770996,
      "grad_norm": 3.5486204624176025,
      "learning_rate": 6.344442505670913e-06,
      "loss": 2.5608,
      "step": 25120
    },
    {
      "epoch": 2.622848200312989,
      "grad_norm": 4.971822261810303,
      "learning_rate": 6.309544582097365e-06,
      "loss": 2.5743,
      "step": 25140
    },
    {
      "epoch": 2.6249347939488783,
      "grad_norm": 4.169508457183838,
      "learning_rate": 6.274646658523818e-06,
      "loss": 2.5478,
      "step": 25160
    },
    {
      "epoch": 2.627021387584768,
      "grad_norm": 3.9123997688293457,
      "learning_rate": 6.239748734950271e-06,
      "loss": 2.5873,
      "step": 25180
    },
    {
      "epoch": 2.629107981220657,
      "grad_norm": 3.8565075397491455,
      "learning_rate": 6.204850811376724e-06,
      "loss": 2.5277,
      "step": 25200
    },
    {
      "epoch": 2.6311945748565466,
      "grad_norm": 4.645976543426514,
      "learning_rate": 6.169952887803175e-06,
      "loss": 2.5943,
      "step": 25220
    },
    {
      "epoch": 2.633281168492436,
      "grad_norm": 4.8118367195129395,
      "learning_rate": 6.1350549642296285e-06,
      "loss": 2.6004,
      "step": 25240
    },
    {
      "epoch": 2.6353677621283254,
      "grad_norm": 3.5474865436553955,
      "learning_rate": 6.100157040656082e-06,
      "loss": 2.4463,
      "step": 25260
    },
    {
      "epoch": 2.637454355764215,
      "grad_norm": 4.386117935180664,
      "learning_rate": 6.065259117082534e-06,
      "loss": 2.6063,
      "step": 25280
    },
    {
      "epoch": 2.6395409494001045,
      "grad_norm": 3.6498193740844727,
      "learning_rate": 6.030361193508986e-06,
      "loss": 2.5848,
      "step": 25300
    },
    {
      "epoch": 2.6416275430359937,
      "grad_norm": 4.641049385070801,
      "learning_rate": 5.995463269935439e-06,
      "loss": 2.588,
      "step": 25320
    },
    {
      "epoch": 2.6437141366718833,
      "grad_norm": 5.11944580078125,
      "learning_rate": 5.960565346361892e-06,
      "loss": 2.581,
      "step": 25340
    },
    {
      "epoch": 2.6458007303077724,
      "grad_norm": 4.181540489196777,
      "learning_rate": 5.925667422788345e-06,
      "loss": 2.5421,
      "step": 25360
    },
    {
      "epoch": 2.647887323943662,
      "grad_norm": 4.518252372741699,
      "learning_rate": 5.8907694992147965e-06,
      "loss": 2.6119,
      "step": 25380
    },
    {
      "epoch": 2.649973917579551,
      "grad_norm": 4.279024124145508,
      "learning_rate": 5.85587157564125e-06,
      "loss": 2.5096,
      "step": 25400
    },
    {
      "epoch": 2.6520605112154407,
      "grad_norm": 3.865201950073242,
      "learning_rate": 5.820973652067702e-06,
      "loss": 2.6102,
      "step": 25420
    },
    {
      "epoch": 2.6541471048513303,
      "grad_norm": 3.971653699874878,
      "learning_rate": 5.786075728494155e-06,
      "loss": 2.5395,
      "step": 25440
    },
    {
      "epoch": 2.6562336984872195,
      "grad_norm": 4.451091289520264,
      "learning_rate": 5.7511778049206075e-06,
      "loss": 2.5708,
      "step": 25460
    },
    {
      "epoch": 2.658320292123109,
      "grad_norm": 3.8971149921417236,
      "learning_rate": 5.71627988134706e-06,
      "loss": 2.6095,
      "step": 25480
    },
    {
      "epoch": 2.6604068857589986,
      "grad_norm": 3.4914326667785645,
      "learning_rate": 5.681381957773513e-06,
      "loss": 2.6299,
      "step": 25500
    },
    {
      "epoch": 2.662493479394888,
      "grad_norm": 4.091533660888672,
      "learning_rate": 5.646484034199965e-06,
      "loss": 2.5197,
      "step": 25520
    },
    {
      "epoch": 2.6645800730307774,
      "grad_norm": 4.721798896789551,
      "learning_rate": 5.611586110626418e-06,
      "loss": 2.5529,
      "step": 25540
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 3.5023694038391113,
      "learning_rate": 5.576688187052871e-06,
      "loss": 2.594,
      "step": 25560
    },
    {
      "epoch": 2.668753260302556,
      "grad_norm": 4.072299480438232,
      "learning_rate": 5.541790263479323e-06,
      "loss": 2.5032,
      "step": 25580
    },
    {
      "epoch": 2.6708398539384453,
      "grad_norm": 4.506401538848877,
      "learning_rate": 5.506892339905776e-06,
      "loss": 2.626,
      "step": 25600
    },
    {
      "epoch": 2.672926447574335,
      "grad_norm": 4.204372406005859,
      "learning_rate": 5.471994416332229e-06,
      "loss": 2.5373,
      "step": 25620
    },
    {
      "epoch": 2.6750130412102244,
      "grad_norm": 4.88733434677124,
      "learning_rate": 5.437096492758681e-06,
      "loss": 2.5933,
      "step": 25640
    },
    {
      "epoch": 2.6770996348461136,
      "grad_norm": 4.014341354370117,
      "learning_rate": 5.402198569185133e-06,
      "loss": 2.506,
      "step": 25660
    },
    {
      "epoch": 2.679186228482003,
      "grad_norm": 4.04020881652832,
      "learning_rate": 5.3673006456115865e-06,
      "loss": 2.5127,
      "step": 25680
    },
    {
      "epoch": 2.6812728221178928,
      "grad_norm": 4.2371826171875,
      "learning_rate": 5.33240272203804e-06,
      "loss": 2.5718,
      "step": 25700
    },
    {
      "epoch": 2.683359415753782,
      "grad_norm": 6.131139278411865,
      "learning_rate": 5.297504798464491e-06,
      "loss": 2.5307,
      "step": 25720
    },
    {
      "epoch": 2.6854460093896715,
      "grad_norm": 4.02719783782959,
      "learning_rate": 5.262606874890944e-06,
      "loss": 2.6561,
      "step": 25740
    },
    {
      "epoch": 2.6875326030255606,
      "grad_norm": 3.5099897384643555,
      "learning_rate": 5.227708951317397e-06,
      "loss": 2.5616,
      "step": 25760
    },
    {
      "epoch": 2.68961919666145,
      "grad_norm": 3.625973701477051,
      "learning_rate": 5.19281102774385e-06,
      "loss": 2.5604,
      "step": 25780
    },
    {
      "epoch": 2.6917057902973394,
      "grad_norm": 5.221146106719971,
      "learning_rate": 5.157913104170302e-06,
      "loss": 2.5666,
      "step": 25800
    },
    {
      "epoch": 2.693792383933229,
      "grad_norm": 3.7837326526641846,
      "learning_rate": 5.1230151805967545e-06,
      "loss": 2.5715,
      "step": 25820
    },
    {
      "epoch": 2.6958789775691185,
      "grad_norm": 4.246307373046875,
      "learning_rate": 5.088117257023208e-06,
      "loss": 2.562,
      "step": 25840
    },
    {
      "epoch": 2.6979655712050077,
      "grad_norm": 3.8435065746307373,
      "learning_rate": 5.05321933344966e-06,
      "loss": 2.5881,
      "step": 25860
    },
    {
      "epoch": 2.7000521648408973,
      "grad_norm": 3.8915133476257324,
      "learning_rate": 5.018321409876112e-06,
      "loss": 2.503,
      "step": 25880
    },
    {
      "epoch": 2.702138758476787,
      "grad_norm": 4.247517108917236,
      "learning_rate": 4.9834234863025655e-06,
      "loss": 2.5831,
      "step": 25900
    },
    {
      "epoch": 2.704225352112676,
      "grad_norm": 3.6344242095947266,
      "learning_rate": 4.948525562729018e-06,
      "loss": 2.5491,
      "step": 25920
    },
    {
      "epoch": 2.7063119457485656,
      "grad_norm": 3.807398557662964,
      "learning_rate": 4.913627639155471e-06,
      "loss": 2.5701,
      "step": 25940
    },
    {
      "epoch": 2.7083985393844547,
      "grad_norm": 5.025998115539551,
      "learning_rate": 4.8787297155819225e-06,
      "loss": 2.5717,
      "step": 25960
    },
    {
      "epoch": 2.7104851330203443,
      "grad_norm": 4.244622230529785,
      "learning_rate": 4.843831792008376e-06,
      "loss": 2.5644,
      "step": 25980
    },
    {
      "epoch": 2.7125717266562335,
      "grad_norm": 4.796576976776123,
      "learning_rate": 4.808933868434828e-06,
      "loss": 2.4422,
      "step": 26000
    },
    {
      "epoch": 2.714658320292123,
      "grad_norm": 4.127851486206055,
      "learning_rate": 4.774035944861281e-06,
      "loss": 2.553,
      "step": 26020
    },
    {
      "epoch": 2.7167449139280127,
      "grad_norm": 3.343036413192749,
      "learning_rate": 4.7391380212877335e-06,
      "loss": 2.5999,
      "step": 26040
    },
    {
      "epoch": 2.718831507563902,
      "grad_norm": 4.756318092346191,
      "learning_rate": 4.704240097714186e-06,
      "loss": 2.4666,
      "step": 26060
    },
    {
      "epoch": 2.7209181011997914,
      "grad_norm": 3.6283912658691406,
      "learning_rate": 4.669342174140639e-06,
      "loss": 2.5657,
      "step": 26080
    },
    {
      "epoch": 2.723004694835681,
      "grad_norm": 4.224010944366455,
      "learning_rate": 4.634444250567091e-06,
      "loss": 2.5037,
      "step": 26100
    },
    {
      "epoch": 2.72509128847157,
      "grad_norm": 4.111839294433594,
      "learning_rate": 4.599546326993544e-06,
      "loss": 2.5817,
      "step": 26120
    },
    {
      "epoch": 2.7271778821074597,
      "grad_norm": 4.24668025970459,
      "learning_rate": 4.564648403419997e-06,
      "loss": 2.5606,
      "step": 26140
    },
    {
      "epoch": 2.729264475743349,
      "grad_norm": 4.558852195739746,
      "learning_rate": 4.529750479846449e-06,
      "loss": 2.6448,
      "step": 26160
    },
    {
      "epoch": 2.7313510693792384,
      "grad_norm": 4.193173885345459,
      "learning_rate": 4.494852556272902e-06,
      "loss": 2.6046,
      "step": 26180
    },
    {
      "epoch": 2.7334376630151276,
      "grad_norm": 4.303905487060547,
      "learning_rate": 4.459954632699355e-06,
      "loss": 2.4646,
      "step": 26200
    },
    {
      "epoch": 2.735524256651017,
      "grad_norm": 3.979672431945801,
      "learning_rate": 4.425056709125807e-06,
      "loss": 2.4876,
      "step": 26220
    },
    {
      "epoch": 2.7376108502869068,
      "grad_norm": 4.652512550354004,
      "learning_rate": 4.39015878555226e-06,
      "loss": 2.5194,
      "step": 26240
    },
    {
      "epoch": 2.739697443922796,
      "grad_norm": 4.850394248962402,
      "learning_rate": 4.3552608619787125e-06,
      "loss": 2.5304,
      "step": 26260
    },
    {
      "epoch": 2.7417840375586855,
      "grad_norm": 4.606376647949219,
      "learning_rate": 4.320362938405166e-06,
      "loss": 2.6043,
      "step": 26280
    },
    {
      "epoch": 2.743870631194575,
      "grad_norm": 4.298604488372803,
      "learning_rate": 4.285465014831617e-06,
      "loss": 2.4749,
      "step": 26300
    },
    {
      "epoch": 2.7459572248304642,
      "grad_norm": 4.662076950073242,
      "learning_rate": 4.25056709125807e-06,
      "loss": 2.586,
      "step": 26320
    },
    {
      "epoch": 2.748043818466354,
      "grad_norm": 4.526334762573242,
      "learning_rate": 4.215669167684523e-06,
      "loss": 2.5371,
      "step": 26340
    },
    {
      "epoch": 2.750130412102243,
      "grad_norm": 4.277784824371338,
      "learning_rate": 4.180771244110976e-06,
      "loss": 2.5137,
      "step": 26360
    },
    {
      "epoch": 2.7522170057381325,
      "grad_norm": 3.971590042114258,
      "learning_rate": 4.145873320537428e-06,
      "loss": 2.6217,
      "step": 26380
    },
    {
      "epoch": 2.7543035993740217,
      "grad_norm": 3.79693341255188,
      "learning_rate": 4.1109753969638805e-06,
      "loss": 2.5437,
      "step": 26400
    },
    {
      "epoch": 2.7563901930099113,
      "grad_norm": 4.544597148895264,
      "learning_rate": 4.076077473390334e-06,
      "loss": 2.5105,
      "step": 26420
    },
    {
      "epoch": 2.758476786645801,
      "grad_norm": 4.435028553009033,
      "learning_rate": 4.041179549816786e-06,
      "loss": 2.5991,
      "step": 26440
    },
    {
      "epoch": 2.76056338028169,
      "grad_norm": 3.7876639366149902,
      "learning_rate": 4.006281626243238e-06,
      "loss": 2.5547,
      "step": 26460
    },
    {
      "epoch": 2.7626499739175796,
      "grad_norm": 3.8753106594085693,
      "learning_rate": 3.9713837026696915e-06,
      "loss": 2.5387,
      "step": 26480
    },
    {
      "epoch": 2.764736567553469,
      "grad_norm": 4.549182415008545,
      "learning_rate": 3.936485779096144e-06,
      "loss": 2.4656,
      "step": 26500
    },
    {
      "epoch": 2.7668231611893583,
      "grad_norm": 4.258023262023926,
      "learning_rate": 3.901587855522597e-06,
      "loss": 2.5532,
      "step": 26520
    },
    {
      "epoch": 2.768909754825248,
      "grad_norm": 4.365508079528809,
      "learning_rate": 3.8666899319490485e-06,
      "loss": 2.5822,
      "step": 26540
    },
    {
      "epoch": 2.770996348461137,
      "grad_norm": 4.693016052246094,
      "learning_rate": 3.831792008375502e-06,
      "loss": 2.6355,
      "step": 26560
    },
    {
      "epoch": 2.7730829420970267,
      "grad_norm": 4.3166913986206055,
      "learning_rate": 3.7968940848019544e-06,
      "loss": 2.6042,
      "step": 26580
    },
    {
      "epoch": 2.775169535732916,
      "grad_norm": 4.71570348739624,
      "learning_rate": 3.761996161228407e-06,
      "loss": 2.438,
      "step": 26600
    },
    {
      "epoch": 2.7772561293688054,
      "grad_norm": 4.567866325378418,
      "learning_rate": 3.7270982376548595e-06,
      "loss": 2.5999,
      "step": 26620
    },
    {
      "epoch": 2.779342723004695,
      "grad_norm": 3.872851610183716,
      "learning_rate": 3.6922003140813123e-06,
      "loss": 2.6213,
      "step": 26640
    },
    {
      "epoch": 2.781429316640584,
      "grad_norm": 4.234388828277588,
      "learning_rate": 3.657302390507765e-06,
      "loss": 2.6296,
      "step": 26660
    },
    {
      "epoch": 2.7835159102764737,
      "grad_norm": 4.618773937225342,
      "learning_rate": 3.6224044669342178e-06,
      "loss": 2.5669,
      "step": 26680
    },
    {
      "epoch": 2.7856025039123633,
      "grad_norm": 3.958649158477783,
      "learning_rate": 3.5875065433606705e-06,
      "loss": 2.5178,
      "step": 26700
    },
    {
      "epoch": 2.7876890975482524,
      "grad_norm": 4.369450569152832,
      "learning_rate": 3.552608619787123e-06,
      "loss": 2.5148,
      "step": 26720
    },
    {
      "epoch": 2.789775691184142,
      "grad_norm": 3.8764724731445312,
      "learning_rate": 3.5177106962135756e-06,
      "loss": 2.54,
      "step": 26740
    },
    {
      "epoch": 2.791862284820031,
      "grad_norm": 4.30078649520874,
      "learning_rate": 3.4828127726400284e-06,
      "loss": 2.5535,
      "step": 26760
    },
    {
      "epoch": 2.7939488784559208,
      "grad_norm": 4.330747604370117,
      "learning_rate": 3.447914849066481e-06,
      "loss": 2.5816,
      "step": 26780
    },
    {
      "epoch": 2.79603547209181,
      "grad_norm": 5.0949907302856445,
      "learning_rate": 3.413016925492933e-06,
      "loss": 2.4994,
      "step": 26800
    },
    {
      "epoch": 2.7981220657276995,
      "grad_norm": 4.264200687408447,
      "learning_rate": 3.3781190019193858e-06,
      "loss": 2.6506,
      "step": 26820
    },
    {
      "epoch": 2.800208659363589,
      "grad_norm": 3.631063461303711,
      "learning_rate": 3.3432210783458385e-06,
      "loss": 2.5157,
      "step": 26840
    },
    {
      "epoch": 2.8022952529994782,
      "grad_norm": 5.246498107910156,
      "learning_rate": 3.3083231547722917e-06,
      "loss": 2.6366,
      "step": 26860
    },
    {
      "epoch": 2.804381846635368,
      "grad_norm": 5.246118545532227,
      "learning_rate": 3.2734252311987436e-06,
      "loss": 2.5337,
      "step": 26880
    },
    {
      "epoch": 2.8064684402712574,
      "grad_norm": 5.004763603210449,
      "learning_rate": 3.2385273076251963e-06,
      "loss": 2.6727,
      "step": 26900
    },
    {
      "epoch": 2.8085550339071466,
      "grad_norm": 3.861112117767334,
      "learning_rate": 3.203629384051649e-06,
      "loss": 2.5233,
      "step": 26920
    },
    {
      "epoch": 2.810641627543036,
      "grad_norm": 3.993084192276001,
      "learning_rate": 3.168731460478102e-06,
      "loss": 2.4722,
      "step": 26940
    },
    {
      "epoch": 2.8127282211789253,
      "grad_norm": 4.1531548500061035,
      "learning_rate": 3.133833536904554e-06,
      "loss": 2.5723,
      "step": 26960
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 3.990025520324707,
      "learning_rate": 3.098935613331007e-06,
      "loss": 2.6056,
      "step": 26980
    },
    {
      "epoch": 2.816901408450704,
      "grad_norm": 3.892401933670044,
      "learning_rate": 3.0640376897574597e-06,
      "loss": 2.5477,
      "step": 27000
    },
    {
      "epoch": 2.8189880020865936,
      "grad_norm": 4.61973237991333,
      "learning_rate": 3.029139766183912e-06,
      "loss": 2.5878,
      "step": 27020
    },
    {
      "epoch": 2.821074595722483,
      "grad_norm": 7.3623223304748535,
      "learning_rate": 2.9942418426103648e-06,
      "loss": 2.5303,
      "step": 27040
    },
    {
      "epoch": 2.8231611893583723,
      "grad_norm": 4.375003814697266,
      "learning_rate": 2.9593439190368175e-06,
      "loss": 2.519,
      "step": 27060
    },
    {
      "epoch": 2.825247782994262,
      "grad_norm": 4.436962127685547,
      "learning_rate": 2.9244459954632703e-06,
      "loss": 2.5968,
      "step": 27080
    },
    {
      "epoch": 2.8273343766301515,
      "grad_norm": 3.858829975128174,
      "learning_rate": 2.8895480718897226e-06,
      "loss": 2.6652,
      "step": 27100
    },
    {
      "epoch": 2.8294209702660407,
      "grad_norm": 5.025664329528809,
      "learning_rate": 2.8546501483161753e-06,
      "loss": 2.6556,
      "step": 27120
    },
    {
      "epoch": 2.8315075639019303,
      "grad_norm": 4.087061882019043,
      "learning_rate": 2.8197522247426277e-06,
      "loss": 2.4932,
      "step": 27140
    },
    {
      "epoch": 2.8335941575378194,
      "grad_norm": 4.372162818908691,
      "learning_rate": 2.7848543011690804e-06,
      "loss": 2.5039,
      "step": 27160
    },
    {
      "epoch": 2.835680751173709,
      "grad_norm": 4.314291000366211,
      "learning_rate": 2.749956377595533e-06,
      "loss": 2.5216,
      "step": 27180
    },
    {
      "epoch": 2.837767344809598,
      "grad_norm": 4.471795082092285,
      "learning_rate": 2.715058454021986e-06,
      "loss": 2.4806,
      "step": 27200
    },
    {
      "epoch": 2.8398539384454877,
      "grad_norm": 4.747176170349121,
      "learning_rate": 2.6801605304484387e-06,
      "loss": 2.555,
      "step": 27220
    },
    {
      "epoch": 2.8419405320813773,
      "grad_norm": 4.2624831199646,
      "learning_rate": 2.645262606874891e-06,
      "loss": 2.535,
      "step": 27240
    },
    {
      "epoch": 2.8440271257172665,
      "grad_norm": 4.484983444213867,
      "learning_rate": 2.6103646833013438e-06,
      "loss": 2.6013,
      "step": 27260
    },
    {
      "epoch": 2.846113719353156,
      "grad_norm": 3.7477452754974365,
      "learning_rate": 2.575466759727796e-06,
      "loss": 2.5386,
      "step": 27280
    },
    {
      "epoch": 2.8482003129890456,
      "grad_norm": 4.148338317871094,
      "learning_rate": 2.5405688361542493e-06,
      "loss": 2.5184,
      "step": 27300
    },
    {
      "epoch": 2.850286906624935,
      "grad_norm": 4.016167640686035,
      "learning_rate": 2.5056709125807016e-06,
      "loss": 2.5074,
      "step": 27320
    },
    {
      "epoch": 2.8523735002608244,
      "grad_norm": 3.9604995250701904,
      "learning_rate": 2.4707729890071543e-06,
      "loss": 2.6511,
      "step": 27340
    },
    {
      "epoch": 2.8544600938967135,
      "grad_norm": 3.6637632846832275,
      "learning_rate": 2.4358750654336067e-06,
      "loss": 2.5624,
      "step": 27360
    },
    {
      "epoch": 2.856546687532603,
      "grad_norm": 4.322919845581055,
      "learning_rate": 2.4009771418600594e-06,
      "loss": 2.5811,
      "step": 27380
    },
    {
      "epoch": 2.8586332811684922,
      "grad_norm": 4.250860214233398,
      "learning_rate": 2.366079218286512e-06,
      "loss": 2.5043,
      "step": 27400
    },
    {
      "epoch": 2.860719874804382,
      "grad_norm": 4.13468074798584,
      "learning_rate": 2.331181294712965e-06,
      "loss": 2.497,
      "step": 27420
    },
    {
      "epoch": 2.8628064684402714,
      "grad_norm": 4.0247273445129395,
      "learning_rate": 2.2962833711394172e-06,
      "loss": 2.6143,
      "step": 27440
    },
    {
      "epoch": 2.8648930620761606,
      "grad_norm": 3.8948447704315186,
      "learning_rate": 2.26138544756587e-06,
      "loss": 2.5346,
      "step": 27460
    },
    {
      "epoch": 2.86697965571205,
      "grad_norm": 3.6239964962005615,
      "learning_rate": 2.2264875239923223e-06,
      "loss": 2.5485,
      "step": 27480
    },
    {
      "epoch": 2.8690662493479397,
      "grad_norm": 4.053345680236816,
      "learning_rate": 2.191589600418775e-06,
      "loss": 2.5196,
      "step": 27500
    },
    {
      "epoch": 2.871152842983829,
      "grad_norm": 4.030655860900879,
      "learning_rate": 2.156691676845228e-06,
      "loss": 2.5573,
      "step": 27520
    },
    {
      "epoch": 2.873239436619718,
      "grad_norm": 4.268490314483643,
      "learning_rate": 2.1217937532716806e-06,
      "loss": 2.5322,
      "step": 27540
    },
    {
      "epoch": 2.8753260302556076,
      "grad_norm": 4.449936389923096,
      "learning_rate": 2.086895829698133e-06,
      "loss": 2.4687,
      "step": 27560
    },
    {
      "epoch": 2.877412623891497,
      "grad_norm": 4.108904838562012,
      "learning_rate": 2.0519979061245857e-06,
      "loss": 2.6379,
      "step": 27580
    },
    {
      "epoch": 2.8794992175273864,
      "grad_norm": 4.947356224060059,
      "learning_rate": 2.017099982551038e-06,
      "loss": 2.5501,
      "step": 27600
    },
    {
      "epoch": 2.881585811163276,
      "grad_norm": 4.659063339233398,
      "learning_rate": 1.982202058977491e-06,
      "loss": 2.5285,
      "step": 27620
    },
    {
      "epoch": 2.8836724047991655,
      "grad_norm": 3.778475284576416,
      "learning_rate": 1.9473041354039435e-06,
      "loss": 2.5583,
      "step": 27640
    },
    {
      "epoch": 2.8857589984350547,
      "grad_norm": 4.0334086418151855,
      "learning_rate": 1.9124062118303962e-06,
      "loss": 2.603,
      "step": 27660
    },
    {
      "epoch": 2.8878455920709443,
      "grad_norm": 4.444037914276123,
      "learning_rate": 1.8775082882568486e-06,
      "loss": 2.5477,
      "step": 27680
    },
    {
      "epoch": 2.889932185706834,
      "grad_norm": 4.366006374359131,
      "learning_rate": 1.8426103646833015e-06,
      "loss": 2.5761,
      "step": 27700
    },
    {
      "epoch": 2.892018779342723,
      "grad_norm": 4.776525020599365,
      "learning_rate": 1.8077124411097543e-06,
      "loss": 2.6501,
      "step": 27720
    },
    {
      "epoch": 2.894105372978612,
      "grad_norm": 4.249317169189453,
      "learning_rate": 1.7728145175362066e-06,
      "loss": 2.4609,
      "step": 27740
    },
    {
      "epoch": 2.8961919666145017,
      "grad_norm": 4.671166896820068,
      "learning_rate": 1.7379165939626594e-06,
      "loss": 2.5388,
      "step": 27760
    },
    {
      "epoch": 2.8982785602503913,
      "grad_norm": 3.9619176387786865,
      "learning_rate": 1.703018670389112e-06,
      "loss": 2.5818,
      "step": 27780
    },
    {
      "epoch": 2.9003651538862805,
      "grad_norm": 3.737910270690918,
      "learning_rate": 1.6681207468155647e-06,
      "loss": 2.5176,
      "step": 27800
    },
    {
      "epoch": 2.90245174752217,
      "grad_norm": 4.199671268463135,
      "learning_rate": 1.6332228232420172e-06,
      "loss": 2.4525,
      "step": 27820
    },
    {
      "epoch": 2.9045383411580596,
      "grad_norm": 4.145812034606934,
      "learning_rate": 1.59832489966847e-06,
      "loss": 2.6116,
      "step": 27840
    },
    {
      "epoch": 2.906624934793949,
      "grad_norm": 3.879124641418457,
      "learning_rate": 1.5634269760949223e-06,
      "loss": 2.5992,
      "step": 27860
    },
    {
      "epoch": 2.9087115284298384,
      "grad_norm": 3.8903753757476807,
      "learning_rate": 1.528529052521375e-06,
      "loss": 2.5466,
      "step": 27880
    },
    {
      "epoch": 2.910798122065728,
      "grad_norm": 3.7833456993103027,
      "learning_rate": 1.4936311289478276e-06,
      "loss": 2.4446,
      "step": 27900
    },
    {
      "epoch": 2.912884715701617,
      "grad_norm": 4.50266695022583,
      "learning_rate": 1.4587332053742803e-06,
      "loss": 2.6334,
      "step": 27920
    },
    {
      "epoch": 2.9149713093375063,
      "grad_norm": 5.162437915802002,
      "learning_rate": 1.4238352818007329e-06,
      "loss": 2.4863,
      "step": 27940
    },
    {
      "epoch": 2.917057902973396,
      "grad_norm": 3.8363254070281982,
      "learning_rate": 1.3889373582271854e-06,
      "loss": 2.483,
      "step": 27960
    },
    {
      "epoch": 2.9191444966092854,
      "grad_norm": 4.515854358673096,
      "learning_rate": 1.3540394346536382e-06,
      "loss": 2.5592,
      "step": 27980
    },
    {
      "epoch": 2.9212310902451746,
      "grad_norm": 4.227335453033447,
      "learning_rate": 1.319141511080091e-06,
      "loss": 2.5205,
      "step": 28000
    },
    {
      "epoch": 2.923317683881064,
      "grad_norm": 4.838918209075928,
      "learning_rate": 1.2842435875065434e-06,
      "loss": 2.6182,
      "step": 28020
    },
    {
      "epoch": 2.9254042775169538,
      "grad_norm": 3.4202821254730225,
      "learning_rate": 1.2493456639329962e-06,
      "loss": 2.601,
      "step": 28040
    },
    {
      "epoch": 2.927490871152843,
      "grad_norm": 4.73744010925293,
      "learning_rate": 1.2144477403594487e-06,
      "loss": 2.5983,
      "step": 28060
    },
    {
      "epoch": 2.9295774647887325,
      "grad_norm": 3.7852165699005127,
      "learning_rate": 1.1795498167859013e-06,
      "loss": 2.3836,
      "step": 28080
    },
    {
      "epoch": 2.931664058424622,
      "grad_norm": 5.530162811279297,
      "learning_rate": 1.144651893212354e-06,
      "loss": 2.6725,
      "step": 28100
    },
    {
      "epoch": 2.933750652060511,
      "grad_norm": 3.8814034461975098,
      "learning_rate": 1.1097539696388066e-06,
      "loss": 2.5205,
      "step": 28120
    },
    {
      "epoch": 2.9358372456964004,
      "grad_norm": 3.7169506549835205,
      "learning_rate": 1.074856046065259e-06,
      "loss": 2.5719,
      "step": 28140
    },
    {
      "epoch": 2.93792383933229,
      "grad_norm": 4.074486255645752,
      "learning_rate": 1.0399581224917119e-06,
      "loss": 2.5664,
      "step": 28160
    },
    {
      "epoch": 2.9400104329681795,
      "grad_norm": 3.8665049076080322,
      "learning_rate": 1.0050601989181644e-06,
      "loss": 2.5748,
      "step": 28180
    },
    {
      "epoch": 2.9420970266040687,
      "grad_norm": 3.657545566558838,
      "learning_rate": 9.701622753446171e-07,
      "loss": 2.6007,
      "step": 28200
    },
    {
      "epoch": 2.9441836202399583,
      "grad_norm": 3.5654568672180176,
      "learning_rate": 9.352643517710697e-07,
      "loss": 2.6162,
      "step": 28220
    },
    {
      "epoch": 2.946270213875848,
      "grad_norm": 4.811371326446533,
      "learning_rate": 9.003664281975223e-07,
      "loss": 2.4135,
      "step": 28240
    },
    {
      "epoch": 2.948356807511737,
      "grad_norm": 4.346680164337158,
      "learning_rate": 8.654685046239749e-07,
      "loss": 2.5934,
      "step": 28260
    },
    {
      "epoch": 2.9504434011476266,
      "grad_norm": 4.859425067901611,
      "learning_rate": 8.305705810504275e-07,
      "loss": 2.5743,
      "step": 28280
    },
    {
      "epoch": 2.952529994783516,
      "grad_norm": 4.361306667327881,
      "learning_rate": 7.956726574768802e-07,
      "loss": 2.6298,
      "step": 28300
    },
    {
      "epoch": 2.9546165884194053,
      "grad_norm": 3.956629753112793,
      "learning_rate": 7.607747339033328e-07,
      "loss": 2.4559,
      "step": 28320
    },
    {
      "epoch": 2.9567031820552945,
      "grad_norm": 4.396886825561523,
      "learning_rate": 7.258768103297854e-07,
      "loss": 2.5981,
      "step": 28340
    },
    {
      "epoch": 2.958789775691184,
      "grad_norm": 3.8900904655456543,
      "learning_rate": 6.90978886756238e-07,
      "loss": 2.5828,
      "step": 28360
    },
    {
      "epoch": 2.9608763693270737,
      "grad_norm": 3.5229382514953613,
      "learning_rate": 6.560809631826907e-07,
      "loss": 2.56,
      "step": 28380
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 4.056211948394775,
      "learning_rate": 6.211830396091433e-07,
      "loss": 2.4291,
      "step": 28400
    },
    {
      "epoch": 2.9650495565988524,
      "grad_norm": 4.6647748947143555,
      "learning_rate": 5.862851160355959e-07,
      "loss": 2.5641,
      "step": 28420
    },
    {
      "epoch": 2.967136150234742,
      "grad_norm": 4.7427849769592285,
      "learning_rate": 5.513871924620486e-07,
      "loss": 2.5284,
      "step": 28440
    },
    {
      "epoch": 2.969222743870631,
      "grad_norm": 5.537717819213867,
      "learning_rate": 5.164892688885011e-07,
      "loss": 2.6305,
      "step": 28460
    },
    {
      "epoch": 2.9713093375065207,
      "grad_norm": 3.798100709915161,
      "learning_rate": 4.815913453149538e-07,
      "loss": 2.5432,
      "step": 28480
    },
    {
      "epoch": 2.97339593114241,
      "grad_norm": 4.336981773376465,
      "learning_rate": 4.466934217414064e-07,
      "loss": 2.604,
      "step": 28500
    },
    {
      "epoch": 2.9754825247782994,
      "grad_norm": 3.955471992492676,
      "learning_rate": 4.11795498167859e-07,
      "loss": 2.6068,
      "step": 28520
    },
    {
      "epoch": 2.9775691184141886,
      "grad_norm": 4.633211612701416,
      "learning_rate": 3.7689757459431165e-07,
      "loss": 2.5251,
      "step": 28540
    },
    {
      "epoch": 2.979655712050078,
      "grad_norm": 3.855515241622925,
      "learning_rate": 3.419996510207643e-07,
      "loss": 2.5787,
      "step": 28560
    },
    {
      "epoch": 2.9817423056859678,
      "grad_norm": 3.954287528991699,
      "learning_rate": 3.0710172744721694e-07,
      "loss": 2.5593,
      "step": 28580
    },
    {
      "epoch": 2.983828899321857,
      "grad_norm": 4.416306018829346,
      "learning_rate": 2.7220380387366953e-07,
      "loss": 2.5196,
      "step": 28600
    },
    {
      "epoch": 2.9859154929577465,
      "grad_norm": 3.8467772006988525,
      "learning_rate": 2.3730588030012215e-07,
      "loss": 2.5739,
      "step": 28620
    },
    {
      "epoch": 2.988002086593636,
      "grad_norm": 3.966747760772705,
      "learning_rate": 2.0240795672657477e-07,
      "loss": 2.5766,
      "step": 28640
    },
    {
      "epoch": 2.9900886802295252,
      "grad_norm": 3.961334228515625,
      "learning_rate": 1.6751003315302742e-07,
      "loss": 2.6049,
      "step": 28660
    },
    {
      "epoch": 2.992175273865415,
      "grad_norm": 4.8509111404418945,
      "learning_rate": 1.3261210957948004e-07,
      "loss": 2.5597,
      "step": 28680
    },
    {
      "epoch": 2.994261867501304,
      "grad_norm": 4.907057285308838,
      "learning_rate": 9.771418600593264e-08,
      "loss": 2.5083,
      "step": 28700
    },
    {
      "epoch": 2.9963484611371936,
      "grad_norm": 3.8551080226898193,
      "learning_rate": 6.281626243238527e-08,
      "loss": 2.5546,
      "step": 28720
    },
    {
      "epoch": 2.9984350547730827,
      "grad_norm": 3.700977325439453,
      "learning_rate": 2.79183388588379e-08,
      "loss": 2.5636,
      "step": 28740
    }
  ],
  "logging_steps": 20,
  "max_steps": 28755,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7512766488576000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
