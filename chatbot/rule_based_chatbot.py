# chatbot/rule_based_chatbot.py

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from utils.sentiment_analysis import analyze_sentiment
import torch
import traceback
import time

print("ğŸ§  [Init] Starting MindMate (GODEL + Sentiment Hybrid)...")

# === Load GODEL model and tokenizer ===
try:
    print("ğŸ“¦ [Step 1] Loading model and tokenizer from HuggingFace...")
    model_name = "microsoft/GODEL-v1_1-base-seq2seq"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    print("âœ… [Step 1 Complete] GODEL model loaded successfully.")
except Exception as e:
    print("âŒ [Error] Failed to load model/tokenizer.")
    traceback.print_exc()
    raise

# === Main bot reply function ===
def generate_bot_reply(user_input):
    print("\n====================")
    print(f"ğŸ’¬ [User Input] {user_input}")
    print("====================")

    try:
        if not user_input.strip():
            print("âš ï¸ [Warning] Empty input received.")
            return "I'm here when you're ready to talk."

        # Step 2: Analyze sentiment
        sentiment = analyze_sentiment(user_input)
        print(f"ğŸ” [Sentiment] Detected: {sentiment}")

        # Step 3: Select instruction based on sentiment
        if sentiment == "Positive":
            instruction = "Instruction: respond joyfully. "
        elif sentiment == "Negative":
            instruction = "Instruction: respond empathetically. "
        else:
            instruction = "Instruction: respond supportively. "

        # Step 4: Format prompt
        full_input = f"{instruction}Context: User: {user_input}"
        print(f"ğŸ§¾ [Formatted Input] {full_input}")

        # Step 5: Tokenize
        inputs = tokenizer(full_input, return_tensors="pt", padding=True)
        print("ğŸ§ª [Tokenization] Done.")

        # Step 6: Generate response
        print("âš™ï¸ [Generation] Generating reply...")
        start = time.time()
        output_ids = model.generate(
            **inputs,
            max_length=60,
            num_beams=4,
            top_p=0.9,
            do_sample=True,
            temperature=0.7,
            repetition_penalty=1.3,
            pad_token_id=tokenizer.eos_token_id
        )
        elapsed = time.time() - start
        print(f"â±ï¸ [Time Taken] {elapsed:.2f} seconds")

        # Step 7: Decode + check reply
        reply = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()
        print(f"ğŸ¤– [Raw Bot Reply] {reply}")

        # Step 8: Fallback for echo or empty
        if (
            reply.lower().startswith("no answer") or
            len(reply.split()) < 2 or
            reply.strip().lower() == user_input.strip().lower()
        ):
            print("âš ï¸ [Fallback Triggered] Low-quality or echo reply detected.")
            if sentiment == "Positive":
                reply = "That's amazing! You should be proud â€” congratulations ğŸ‰"
            elif sentiment == "Negative":
                reply = "I'm here for you. Want to talk more about whatâ€™s bothering you?"
            else:
                reply = "Tell me more â€” Iâ€™m listening."

        return reply

    except Exception as e:
        print("âŒ [Generation Error] Failed during reply generation.")
        traceback.print_exc()
        return "[Bot error: Unable to process right now.]"

# === CLI Debug Mode ===
if __name__ == "__main__":
    print("\nğŸ§  [MindMate Ready] GODEL + VADER chatbot is live.")
    print("ğŸ’¬ Type your message (or 'exit' to quit):\n")

    while True:
        try:
            text = input("You: ")
            if text.lower() in ["exit", "quit"]:
                print("ğŸ‘‹ Goodbye!")
                break
            response = generate_bot_reply(text)
            print(f"Bot: {response}")
        except Exception as e:
            print("âŒ [Fatal CLI Error] Unexpected error in chat loop:")
            traceback.print_exc()
